---
title: 关于本书
date: 2026-02-25
---

#### 本书

- [下载 PDF](https://github.com/little-book-of/algorithms/blob/main/releases/book.pdf) - 适合打印
- [下载 EPUB](https://github.com/little-book-of/algorithms/blob/main/releases/book.epub) - 适合电子阅读器
- [查看 LaTex](https://github.com/little-book-of/algorithms/blob/main/releases/book.tex) - `.tex` 源文件
- [源代码 (Github)](https://github.com/little-book-of/algorithms) - Markdown 源文件
- [在 GitHub Pages 上阅读](https://little-book-of.github.io/algorithms/books/en-US/book.html) - 在线查看

遵循 CC BY-NC-SA 4.0 许可协议。

## 第 1 章 算法基础
### 1. 什么是算法？

让我们从头开始。在代码、数据或性能之前，我们需要清楚地了解算法究竟是什么。

算法是一个清晰、按部就班地解决问题的过程。可以把它想象成一个食谱：你有输入（食材）、一系列步骤（说明）和一个输出（完成的菜肴）。

算法的核心应该是：

- **精确的**：每个步骤都有明确的定义，没有歧义。
- **有限的**：在有限的步骤之后结束。
- **有效的**：每个步骤都足够简单，可以执行。
- **确定的（通常）**：相同的输入产生相同的输出。

当你编写算法时，你是在描述如何从问题得到答案，而不仅仅是答案是什么。

#### 示例：求 1 到 (n) 的和

假设你想求从 1 到 (n) 的数字之和。

**自然语言步骤**

1. 设置 `total = 0`
2. 对于每个从 `1` 到 `n` 的 `i`，将 `i` 加到 `total` 上
3. 返回 `total`

**伪代码**

```text
算法 SumToN(n):
    total ← 0
    for i ← 1 to n:
        total ← total + i
    return total
```

**C 代码**

```c
int sum_to_n(int n) {
    int total = 0;
    for (int i = 1; i <= n; i++) {
        total += i;
    }
    return total;
}
```

#### 小试牛刀

尝试手动快速运行一下 (n = 5) 的情况：

- 起始 `total = 0`
- 加 1 → `total = 1`
- 加 2 → `total = 3`
- 加 3 → `total = 6`
- 加 4 → `total = 10`
- 加 5 → `total = 15`

输出是 `15`。

你很快也会看到这个闭式公式：

$$
1 + 2 + 3 + \dots + n = \frac{n(n+1)}{2}
$$

#### 为什么这很重要

算法是计算的蓝图。从计算器到人工智能模型，每个程序都是由算法构建而成的。计算机在执行指令方面速度很快。算法为这些指令提供了结构和目的。

> 算法是解决问题的语言。

#### 动手试试

1.  编写一个算法，用于找出列表中的最大数字。
2.  编写一个算法，用于反转字符串。
3.  将你的早晨例行公事描述为一个算法：列出输入、步骤和最终输出。

提示：学习的最佳方法是思考清晰的小步骤。将问题分解成可以逐一执行的简单操作。
### 2. 度量时间与空间

既然你已经知道什么是算法，现在是时候问一个更深层次的问题了：

> 我们如何知道一个算法比另一个更好？

算法仅仅正确是不够的，它还应该是高效的。我们通过两个关键方面来衡量效率：时间和空间。

#### 时间复杂度

时间衡量的是算法相对于其输入大小，运行需要多长时间。我们不以秒为单位进行测量，因为硬件速度各不相同。相反，我们计算步骤或操作的数量。

示例：

```c
for (int i = 0; i < n; i++) {
    printf("Hi\n");
}
```

这个循环运行 $n$ 次，因此它的时间复杂度是 $O(n)$。时间随着输入大小线性增长。

另一个例子：

```c
for (int i = 0; i < n; i++)
  for (int j = 0; j < n; j++)
    printf("*");
```

这个循环运行 $n \times n = n^2$ 次，因此它的时间复杂度是 $O(n^2)$。

这些大 O 符号描述了运行时间如何随着输入的增长而增长。

#### 空间复杂度

空间衡量的是算法使用了多少内存。

示例：

```c
int sum = 0;  // O(1) 空间
```

这使用了恒定数量的内存，与输入大小无关。

但如果我们分配一个数组：

```c
int arr[n];   // O(n) 空间
```

这使用了与 $n$ 成比例的空间。

通常，我们会在时间和空间之间进行权衡：

- 使用哈希表可以加快查找速度（更多内存，更少时间）
- 使用流式算法可以节省内存（更少空间，更多时间）

#### 小代码示例

比较两种计算从 1 到 $n$ 的和的方法：

方法 1：循环

```c
int sum_loop(int n) {
    int total = 0;
    for (int i = 1; i <= n; i++) total += i;
    return total;
}
```

时间：$O(n)$
空间：$O(1)$

方法 2：公式

```c
int sum_formula(int n) {
    return n * (n + 1) / 2;
}
```

时间：$O(1)$
空间：$O(1)$

两种方法都是正确的，但其中一种更快。分析时间和空间可以帮助你理解原因。

#### 为什么这很重要

当数据变得非常庞大（数百万或数十亿）时，微小的低效会急剧放大。

一个需要 $O(n^2)$ 时间的算法对于 10 个元素可能感觉良好，但对于 1,000,000 个元素则可能无法实现。

度量时间和空间可以帮助你：

- 预测性能
- 比较不同的解决方案
- 智能地进行优化

它是你驾驭复杂性的指南针。

#### 亲自尝试

1.  写一个简单的算法来查找数组中的最小值。估算其时间和空间复杂度。
2.  比较两个解决同一问题的算法。哪一个的扩展性更好？
3.  想一个感觉像 $O(n)$ 的日常任务。你能想象一个 $O(1)$ 的任务吗？

尽早理解这些度量，将使你未来学习的每一个算法都更有意义。
### 3. 大 O、大 Θ、大 Ω

既然你已经能够度量时间和空间，现在让我们来学习描述这些度量所用的语言。

当我们说一个算法是 $O(n)$ 时，我们使用的是**渐近记号**，这是一种描述算法运行时间或内存消耗如何随着输入规模 $n$ 增加而增长的方式。

它关注的不是确切的步骤数，而是当 $n$ 非常大时，成本如何**按比例变化**。

#### 大 O 记号（上界）

大 O 回答了这个问题：*"最坏能有多糟？"*
它给出了增长的一个**上界**，即最坏情况。

如果一个算法最多需要 $5n + 20$ 步，我们记为 $O(n)$。
我们省略常数项和低阶项，因为在规模很大时它们无关紧要。

常见的大 O 记号：

| 名称         | 记号         | 增长趋势         | 示例                     |
| ------------ | ------------ | ---------------- | ------------------------ |
| 常数阶       | $O(1)$       | 平坦             | 访问数组元素             |
| 对数阶       | $O(\log n)$  | 增长非常缓慢     | 二分查找                 |
| 线性阶       | $O(n)$       | 成比例增长       | 单层循环                 |
| 平方阶       | $O(n^2)$     | 增长迅速         | 双层循环                 |
| 指数阶       | $O(2^n)$     | 爆炸式增长       | 递归生成子集             |

如果你的算法是 $O(n)$，那么输入规模翻倍，运行时间也大致翻倍。
如果是 $O(n^2)$，输入规模翻倍会使运行时间变为原来的大约四倍。

#### 大 Θ 记号（紧确界）

大 Θ 记号 ($\Theta$) 给出了一个**紧确界**，当你同时知道算法增长的上界和下界时使用。

如果运行时间大致是 $3n + 2$，那么 $T(n) = \Theta(n)$。
这意味着它既是 $O(n)$，也是 $\Omega(n)$。

#### 大 Ω 记号（下界）

大 Ω 记号 ($\Omega$) 回答：*"它最快能有多快？"*
它表示**最佳情况**下的增长，即下界。

例如：

- 线性查找：如果元素在开头，则为 $\Omega(1)$
- 如果元素在末尾，最坏情况为 $O(n)$

所以我们可以说：

$$
T(n) = \Omega(1),\quad T(n) = O(n)
$$

#### 小代码示例

让我们看看大 O 在实际中的应用。

```c
int sum_pairs(int n) {
    int total = 0;
    for (int i = 0; i < n; i++)        // O(n)
        for (int j = 0; j < n; j++)    // O(n)
            total += i + j;            // O(1)
    return total;
}
```

总步骤数 ≈ $n \times n = n^2$。
所以 $T(n) = O(n^2)$。

如果我们在循环之前或之后添加一个常数时间的操作，这无关紧要。在渐近记号中，常数被忽略。

#### 为什么这很重要

大 O、大 Θ 和大 Ω 让你能够精确地讨论性能。
它们是效率的语法。

当你能写出：

> 算法 A 在 $O(n \log n)$ 时间内运行，占用 $O(n)$ 空间

你就清晰地抓住了它的本质，并进行了有意义的比较。

它们帮助你：

- 预测大规模下的行为
- 选择更好的数据结构
- 在面试和论文中交流效率

这不是关于精确的计时，而是关于**增长**。

#### 自己动手试试

1.  分析这段代码：

    ```c
    for (int i = 1; i <= n; i *= 2)
        printf("%d", i);
    ```

    它的时间复杂度是多少？

2.  写一个 $O(n \log n)$ 的算法（提示：归并排序）。

3.  找出线性查找和二分查找的最佳、最坏和平均情况复杂度。

学习大 O 就像学习一门新语言，一旦你熟练了，你甚至可以在运行代码之前就看到代码将如何增长。
### 4. 算法范式（贪心、分治、动态规划）

一旦你能衡量性能，就该探索算法是如何设计的了。每一个巧妙的解决方案背后都有一个指导性的范式，一种思考问题的方式。

其中最强大的三种是：

1.  贪心算法
2.  分治法
3.  动态规划

每一种都代表了不同的解决问题的思维方式。

#### 1. 贪心算法

贪心算法在每一步都做出局部最优选择，希望这能导向全局最优解。

可以把它想象成：

> "拿现在看起来最好的，不要担心未来。"

它们快速且简单，但并不总是正确的。只有当贪心选择性质成立时，它们才有效。

**示例：找零问题（贪心版本）**
假设你想用美国硬币（25, 10, 5, 1）凑出 63 美分。
贪心做法是：

-   拿 25 → 还剩 38
-   拿 25 → 还剩 13
-   拿 10 → 还剩 3
-   拿 1 × 3

在这个例子中可行，但并非总是如此（试试用硬币 [1, 3, 4] 凑出金额 6）。
简单，但不能保证最优。

**常见的贪心算法：**

-   Kruskal 最小生成树
-   Prim 最小生成树
-   Dijkstra 最短路径（非负权重）
-   霍夫曼编码

#### 2. 分治法

这是一个经典的范式。你将问题分解成更小的子问题，递归地解决每个子问题，然后合并结果。

这就像把任务分给朋友们做，然后合并他们的答案。

形式化表示为：

$$
T(n) = aT\left(\frac{n}{b}\right) + f(n)
$$

**示例：**

-   归并排序：分割数组，排序两半，合并
-   快速排序：围绕枢轴进行分区
-   二分查找：每一步将范围减半

优雅而强大，但如果结构不佳，递归开销会增加成本。

#### 3. 动态规划

动态规划适用于具有重叠子问题和最优子结构的问题。
你只需解决一次较小的子问题并存储结果，以避免重复计算。

这就像是带记忆功能的分治法。

**示例：斐波那契数列**
朴素的递归是指数级的。
带记忆化的动态规划是线性的。

```c
int fib(int n) {
    if (n <= 1) return n;
    static int memo[1000] = {0};
    if (memo[n]) return memo[n];
    memo[n] = fib(n-1) + fib(n-2);
    return memo[n];
}
```

高效复用，但需要对子问题结构有深入的洞察。

#### 微型代码

使用斐波那契数列进行快速比较：

**朴素（分治法）**

```c
int fib_dc(int n) {
    if (n <= 1) return n;
    return fib_dc(n-1) + fib_dc(n-2);  // 指数级复杂度
}
```

**动态规划（记忆化）**

```c
int fib_dp(int n, int memo[]) {
    if (n <= 1) return n;
    if (memo[n]) return memo[n];
    return memo[n] = fib_dp(n-1, memo) + fib_dp(n-2, memo);
}
```

#### 为何重要

算法范式为你提供了设计模式：

-   **贪心**：当局部选择能导向全局最优时
-   **分治**：当问题可以自然拆分时
-   **动态规划**：当子问题重叠时

一旦你识别出问题的结构，你就能立刻知道哪种思维方式最适合。

把范式看作是推理的模板，不仅仅是技术，更是哲学。

#### 动手试试

1.  写一个贪心算法，用硬币 [1, 3, 4] 凑出金额 6。它能成功吗？
2.  使用分治法实现归并排序。
3.  用两种方法（朴素递归 vs 动态规划）解决斐波那契问题，并比较速度。
4.  想一个你在现实生活中用贪心方式解决的任务。

学习范式就像学习思维方式。一旦你掌握了它们，每个问题看起来都会变得熟悉。
### 5. 递推关系

每次你将一个问题分解为更小的子问题时，你就在创建一个递推关系，这是一种描述总成本如何增长的数学方法。

递推关系是分析递归算法的核心。
它们根据子问题的成本，告诉我们一个算法使用了多少时间或空间。

#### 什么是递推关系？

递推关系用更小实例的代价来表示输入规模为 $n$ 时的总成本 $T(n)$。

示例（归并排序）：

$$
T(n) = 2T(n/2) + O(n)
$$

这意味着：

- 它将问题分成两半（$2T(n/2)$）
- 以 $O(n)$ 的时间合并结果

你经常会看到像这样的递推关系：

- $T(n) = T(n - 1) + O(1)$
- $T(n) = 2T(n/2) + O(n)$
- $T(n) = T(n/2) + O(1)$

每一个都代表了不同的递归结构。

#### 示例 1：简单的线性递推

考虑以下代码：

```c
int count_down(int n) {
    if (n == 0) return 0; // 如果 n 等于 0，返回 0
    return 1 + count_down(n - 1); // 返回 1 加上对 n-1 的递归调用结果
}
```

这段代码为每个更小的输入调用自身一次：

$$
T(n) = T(n - 1) + O(1)
$$

求解：

$$
T(n) = O(n)
$$

因为它在每一层运行一次。

#### 示例 2：二叉递推

对于二叉递归：

```c
int sum_tree(int n) {
    if (n == 1) return 1; // 如果 n 等于 1，返回 1
    return sum_tree(n/2) + sum_tree(n/2) + 1; // 返回两个对 n/2 的递归调用结果之和再加 1
}
```

这里我们对 $n/2$ 进行两次子调用，并做常数量的额外工作：

$$
T(n) = 2T(n/2) + O(1)
$$

求解：$T(n) = O(n)$

为什么？每一层调用次数翻倍但规模减半。
共有 $\log n$ 层，总工作量加起来是 $O(n)$。

#### 求解递推关系

有几种求解方法：

- 代入法
  猜测解，然后用归纳法证明。
- 递归树法
  将递推关系展开成一棵树，并对每层的成本求和。
- 主定理
  当递推关系符合特定形式时使用公式：

  $$
  T(n) = aT(n/b) + f(n)
  $$

#### 主定理（快速总结）

如果 $T(n) = aT(n/b) + f(n)$，那么：

- 如果 $f(n) = O(n^{\log_b a - \epsilon})$，则 $T(n) = \Theta(n^{\log_b a})$
- 如果 $f(n) = \Theta(n^{\log_b a})$，则 $T(n) = \Theta(n^{\log_b a} \log n)$
- 如果 $f(n) = \Omega(n^{\log_b a + \epsilon})$，并且满足正则性条件，则 $T(n) = \Theta(f(n))$

示例（归并排序）：$a = 2$, $b = 2$, $f(n) = O(n)$

$$
T(n) = 2T(n/2) + O(n) = O(n \log n)
$$

#### 小段代码

让我们写一个快速的递归求和：

```c
int sum_array(int arr[], int l, int r) {
    if (l == r) return arr[l]; // 如果左右索引相等，返回该元素
    int mid = (l + r) / 2; // 计算中间索引
    return sum_array(arr, l, mid) + sum_array(arr, mid+1, r); // 递归求和左半部分和右半部分
}
```

递推关系：

$$
T(n) = 2T(n/2) + O(1)
$$

→ $O(n)$

如果你添加了合并操作（像归并排序那样），你会得到 $+O(n)$：

→ $O(n \log n)$

#### 为什么这很重要

递推关系让你能够预测递归解决方案的成本。

没有它们，递归感觉像魔法。有了它们，你可以量化效率。

它们是理解以下内容的关键：

- 分治法
- 动态规划
- 回溯法

一旦你能建立递推关系，求解它就变成了一场代数和逻辑的游戏。

#### 自己动手试试

1.  为二分搜索写出递推关系。求解它。
2.  为归并排序写出递推关系。求解它。
3.  分析这个函数：

    ```c
    void fun(int n) {
        if (n <= 1) return; // 如果 n 小于等于 1，返回
        fun(n/2);
        fun(n/3);
        fun(n/6);
    }
    ```

    递推关系是什么？近似估算其复杂度。
4.  将 $T(n) = T(n-1) + 1$ 展开为显式和。

学习递推关系能帮助你洞察递归的内部。它们将代码转化为方程。
### 6. 搜索基础

在我们进行排序或优化之前，需要一种查找事物的方法。搜索是计算中最基本的操作之一，无论是查找一个名字、寻找一个键值，还是检查某物是否存在。

搜索算法接收一个集合（数组、列表、树等）和一个目标值，并返回目标值是否存在（通常还包括其位置）。

让我们从两种基础技术开始：线性搜索和二分搜索。

#### 1. 线性搜索

线性搜索是最简单的方法：

- 从开头开始
- 依次检查每个元素
- 如果找到目标则停止

它适用于任何列表，无论是否排序，但对于大数据量可能很慢。

```c
int linear_search(int arr[], int n, int key) {
    for (int i = 0; i < n; i++) {
        if (arr[i] == key) return i;
    }
    return -1;
}
```

示例：
如果 `arr = [2, 4, 6, 8, 10]` 且 `key = 6`，它会在索引 2 处找到。

复杂度：

- 时间：$O(n)$
- 空间：$O(1)$

线性搜索简单，并且如果目标存在则保证能找到，但在列表很大时速度很慢。

#### 2. 二分搜索

当列表已排序时，我们可以做得更好。
二分搜索反复将搜索空间对半划分。

步骤：

1.  检查中间元素
2.  如果匹配，则完成
3.  如果 `target < mid`，搜索左半部分
4.  否则，搜索右半部分

```c
int binary_search(int arr[], int n, int key) {
    int low = 0, high = n - 1;
    while (low <= high) {
        int mid = (low + high) / 2;
        if (arr[mid] == key) return mid;
        else if (arr[mid] < key) low = mid + 1;
        else high = mid - 1;
    }
    return -1;
}
```

示例：
`arr = [2, 4, 6, 8, 10]`, `key = 8`

- mid = 6 → key > mid → 搜索右半部分
- mid = 8 → 找到

复杂度：

- 时间：$O(\log n)$
- 空间：$O(1)$

二分搜索是一个巨大的改进；输入规模翻倍仅增加一个额外步骤。

#### 3. 递归二分搜索

二分搜索也可以用递归方式编写：

```c
int binary_search_rec(int arr[], int low, int high, int key) {
    if (low > high) return -1;
    int mid = (low + high) / 2;
    if (arr[mid] == key) return mid;
    else if (arr[mid] > key) return binary_search_rec(arr, low, mid - 1, key);
    else return binary_search_rec(arr, mid + 1, high, key);
}
```

逻辑相同，结构不同。
迭代形式和递归形式效率相同。

#### 4. 在两者之间选择

| 方法         | 适用对象     | 时间复杂度 | 空间复杂度 | 需要排序 |
| ------------ | ------------ | ---------- | ---------- | -------- |
| 线性搜索     | 任何列表     | O(n)       | O(1)       | 否       |
| 二分搜索     | 已排序列表   | O(log n)   | O(1)       | 是       |

如果数据未排序或非常小，线性搜索即可。
如果数据已排序且量大，二分搜索则优越得多。

#### 小代码对比

比较步骤：
对于 $n = 16$：

- 线性搜索 → 最多 16 次比较
- 二分搜索 → $\log_2 16 = 4$ 次比较

这是巨大的差异。

#### 为何重要

搜索是信息检索的核心。每个数据库、编译器和系统都依赖于它。

理解简单的搜索为你学习以下内容做好准备：

- 哈希表（常数时间查找）
- 树搜索（有序结构）
- 图遍历（结构化探索）

这不仅仅是关于查找值；更是关于学习数据结构和算法设计如何协同工作。

#### 亲自尝试

1.  编写一个线性搜索，返回目标值出现的所有索引。
2.  修改二分搜索，使其返回有序数组中目标值的首次出现位置。
3.  比较在大小为 10、100、1000 的数组上的运行时间。
4.  如果在未排序的列表上运行二分搜索会发生什么？

搜索是基础。一旦你掌握了它，你将在各处识别出它的模式。
### 7. 排序基础

排序是计算机科学中被研究最多的问题之一。
为什么？因为顺序很重要。它能让搜索更快、模式更清晰、数据更易于管理。

排序算法将元素按特定顺序（通常是升序或降序）排列。一旦排好序，许多操作（如二分查找、合并或去重）就会变得简单得多。

让我们探索基础的排序方法及其背后的原理。

#### 1. 排序算法的构成要素

一个排序算法应定义：

- 输入：一个元素序列
- 输出：相同的元素，但按排序后的顺序
- 稳定性：保持相等元素的相对顺序（对于多键排序很重要）
- 原地性：仅使用常数量的额外空间

不同的算法在速度、内存占用和简单性之间进行权衡。

#### 2. 冒泡排序

思想：通过反复交换相邻元素对，将最大的元素“冒泡”到末尾。

```c
void bubble_sort(int arr[], int n) {
    for (int i = 0; i < n - 1; i++) {
        for (int j = 0; j < n - i - 1; j++) {
            if (arr[j] > arr[j + 1]) {
                int temp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = temp;
            }
        }
    }
}
```

每一轮都将剩余元素中最大的一个移动到其最终位置。

- 时间复杂度：$O(n^2)$
- 空间复杂度：$O(1)$
- 稳定性：是

简单，但对于大数据集效率低下。

#### 3. 选择排序

思想：反复选择最小的元素并将其放到正确的位置。

```c
void selection_sort(int arr[], int n) {
    for (int i = 0; i < n - 1; i++) {
        int min_idx = i;
        for (int j = i + 1; j < n; j++) {
            if (arr[j] < arr[min_idx]) min_idx = j;
        }
        int temp = arr[i];
        arr[i] = arr[min_idx];
        arr[min_idx] = temp;
    }
}
```

- 时间复杂度：$O(n^2)$
- 空间复杂度：$O(1)$
- 稳定性：否

交换次数较少，但时间复杂度仍然是平方级的。

#### 4. 插入排序

思想：一次一个元素地构建有序列表，将每个新元素插入到正确的位置。

```c
void insertion_sort(int arr[], int n) {
    for (int i = 1; i < n; i++) {
        int key = arr[i];
        int j = i - 1;
        while (j >= 0 && arr[j] > key) {
            arr[j + 1] = arr[j];
            j--;
        }
        arr[j + 1] = key;
    }
}
```

- 时间复杂度：$O(n^2)$（当数组接近有序时，最好情况为 $O(n)$）
- 空间复杂度：$O(1)$
- 稳定性：是

插入排序对于小型或接近有序的数据集非常有效。它常被用作 Timsort 等混合排序算法的基础。

#### 5. 基础排序算法比较

| 算法         | 最好情况   | 平均情况   | 最坏情况   | 稳定性 | 原地性 |
| ------------ | ---------- | ---------- | ---------- | ------ | ------ |
| 冒泡排序     | O(n)       | O(n²)      | O(n²)      | 是     | 是     |
| 选择排序     | O(n²)      | O(n²)      | O(n²)      | 否     | 是     |
| 插入排序     | O(n)       | O(n²)      | O(n²)      | 是     | 是     |

这三种算法的时间复杂度都是平方级的，但插入排序在小型或部分有序的数据上表现最佳。

#### 小示例

用 `arr = [5, 3, 4, 1, 2]` 快速验证：

插入排序（逐步过程）

- 将 3 插入到 5 之前 → [3, 5, 4, 1, 2]
- 插入 4 → [3, 4, 5, 1, 2]
- 插入 1 → [1, 3, 4, 5, 2]
- 插入 2 → [1, 2, 3, 4, 5]

排序完成！

#### 为什么排序很重要

排序是入门算法。它教会你迭代、交换和优化。

高效的排序对于以下方面至关重要：

- 为二分查找预处理数据
- 为分析组织数据
- 构建索引和排名系统

它是通往分治法和混合优化等更深层次概念的第一步。

#### 动手实践

1.  实现所有三种排序：冒泡、选择、插入
2.  在大小为 10、100、1000 的数组上测试它们，并注意时间差异
3.  尝试对一个已经排好序的数组进行排序。哪种算法适应性最好？
4.  修改插入排序，使其按降序排序

排序看似简单，但它是基石。掌握它将塑造你对几乎所有其他算法的直觉。
### 8. 数据结构概述

算法和数据结构是同一枚硬币的两面。
算法是你解决问题的方式。
数据结构是你存储和组织数据的地方，以便你的算法能够高效地工作。

你可以把数据结构想象成容器，每一种都针对特定的访问模式、权衡取舍和性能需求而设计。选择正确的数据结构通常是设计快速算法的关键。

#### 1. 为什么数据结构重要

想象一下你想快速找到一本书。

- 如果所有书都随机堆叠 → 你必须扫描每一本 ($O(n)$)
- 如果它们被排序在书架上 → 你可以使用二分查找 ($O(\log n)$)
- 如果你有一个索引或目录 → 你可以立即找到它 ($O(1)$)

不同的结构能解锁不同的效率。

#### 2. 核心数据结构

让我们浏览一下最基本的数据结构：

| 类型         | 描述                           | 关键操作                                   | 典型用途             |
| ------------ | ------------------------------ | ------------------------------------------ | -------------------- |
| 数组         | 固定大小的连续内存             | 访问 ($O(1)$), 插入/删除 ($O(n)$)          | 快速索引访问         |
| 链表         | 带有指针的节点序列             | 插入/删除 ($O(1)$), 访问 ($O(n)$)          | 动态序列             |
| 栈           | LIFO (后进先出)                | push(), pop() 在 $O(1)$ 时间内             | 撤销、递归           |
| 队列         | FIFO (先进先出)                | enqueue(), dequeue() 在 $O(1)$ 时间内      | 调度、缓冲区         |
| 哈希表       | 通过哈希实现的键值对           | 平均 $O(1)$, 最坏 $O(n)$                   | 查找、缓存           |
| 堆           | 部分有序的树                   | 插入 $O(\log n)$, 提取最小值 $O(\log n)$   | 优先队列             |
| 树           | 层次结构                       | 访问 $O(\log n)$ (平衡时)                  | 有序存储             |
| 图           | 节点 + 边                      | 遍历 $O(V+E)$                              | 网络、路径           |
| 集合 / 映射  | 唯一的键或键值对               | $O(\log n)$ 或 $O(1)$                      | 成员资格测试         |

每一种都有其权衡取舍。数组速度快但僵化，链表灵活但访问慢，哈希表极快但无序。

#### 3. 抽象数据类型 (ADTs)

ADT 定义了你**可以做什么操作**，而不是它们**如何被实现**。
例如，一个栈 ADT 承诺：

- `push(x)`
- `pop()`
- `peek()`

它可以用数组或链表来实现，其行为保持不变。

常见的 ADT：

- 栈
- 队列
- 双端队列
- 优先队列
- 映射 / 字典

这种接口与实现的分离有助于设计灵活的系统。

#### 4. 为工作选择合适的工具

选择正确的数据结构通常决定了你算法的性能：

| 问题                 | 好的选择         | 原因                     |
| -------------------- | ---------------- | ------------------------ |
| 撤销功能             | 栈               | LIFO 符合历史记录        |
| 调度任务             | 队列             | FIFO 顺序                |
| Dijkstra（迪杰斯特拉）算法 | 优先队列         | 提取最小距离             |
| 统计频率             | 哈希映射         | 快速键查找               |
| 动态中位数           | 堆 + 堆          | 平衡两部分               |
| 按前缀搜索           | 字典树           | 快速前缀查找             |

优秀的程序员不只是写代码。他们会选择正确的结构。

#### 小段代码

示例：比较数组与链表

数组：

```c
int arr[5] = {1, 2, 3, 4, 5};
printf("%d", arr[3]); // O(1)
```

链表：

```c
struct Node { int val; struct Node* next; };
```

要获取第 4 个元素，你必须遍历 → $O(n)$

不同的结构，不同的访问成本。

#### 为什么这很重要

每一个高效的算法都依赖于正确的数据结构。

- 搜索、排序和存储都依赖于结构
- 内存布局影响缓存性能
- 错误的选择可以将 $O(1)$ 变成 $O(n^2)$

理解这些结构就像了解车间里的工具。
一旦你认清了它们的形状，你就会本能地知道该用哪一个。

#### 亲自尝试

1.  使用数组实现一个栈。然后使用链表实现它。
2.  使用两个栈实现一个队列。
3.  尝试在哈希表中存储键值对（提示：对表大小取模）。
4.  通过实验比较数组与链表的访问时间。

数据结构不仅仅是存储。它们是你的算法所依赖的骨架。
### 9. 图与树概述

现在你已经了解了数组和链表这样的线性结构，是时候探索非线性结构——图和树了。这些是网络、层次结构和关系背后的形态。

它们无处不在：家谱、文件系统、地图、社交网络和知识图谱都依赖于它们。

#### 1. 树

树是一种没有环的连通结构。它是一种层次结构，每个节点（除了根节点）都有一个父节点。

- 根：顶部的节点
- 子节点：直接连接在下面的节点
- 叶节点：没有子节点的节点
- 高度：从根节点到叶节点的最长路径

二叉树是每个节点最多有两个子节点的树。二叉搜索树（BST）保持元素有序：

- 左子节点 < 父节点 < 右子节点

基本操作：

- 插入
- 搜索
- 删除
- 遍历（前序、中序、后序、层序）

示例：

```c
struct Node {
    int val;
    struct Node *left, *right;
};
```

在 BST 中插入：

```c
struct Node* insert(struct Node* root, int val) {
    if (!root) return newNode(val);
    if (val < root->val) root->left = insert(root->left, val);
    else root->right = insert(root->right, val);
    return root;
}
```

#### 2. 常见树类型

| 类型                 | 描述                                   | 应用场景           |
| -------------------- | -------------------------------------- | ------------------ |
| 二叉树               | 每个节点有 ≤ 2 个子节点                | 通用层次结构       |
| 二叉搜索树           | 左 < 根 < 右                           | 有序数据           |
| AVL / 红黑树         | 自平衡二叉搜索树                       | 快速搜索/插入      |
| 堆                   | 完全二叉树，父节点 ≥ 或 ≤ 子节点       | 优先队列           |
| 字典树               | 字符组成的树                           | 前缀搜索           |
| 线段树               | 基于区间的树                           | 区间查询           |
| 树状数组             | 带有前缀和的树                         | 高效更新           |

平衡树保持高度为 $O(\log n)$，保证了快速的操作。

#### 3. 图

图推广了树的概念。在图中，节点（顶点）可以自由连接。

图是顶点（$V$）和边（$E$）的集合：

$$
G = (V, E)
$$

有向图 vs 无向图：

- 有向图：边有方向（A → B）
- 无向图：边双向连接（A, B）

带权图 vs 无权图：

- 带权图：每条边都有成本
- 无权图：所有边权重相等

表示方法：

1. 邻接矩阵：$n \times n$ 矩阵；如果边存在，则条目 $(i, j) = 1$
2. 邻接表：列表数组；每个顶点存储其邻居

邻接表示例：

```c
vector<int> graph[n];
graph[0].push_back(1);
graph[0].push_back(2);
```

#### 4. 常见图类型

| 图类型               | 描述                   | 示例               |
| -------------------- | ---------------------- | ------------------ |
| 无向图               | 边没有方向             | 社交网络           |
| 有向图               | 箭头指示方向           | 网页链接           |
| 带权图               | 边有成本               | 道路网络           |
| 有环图               | 包含环                 | 任务依赖           |
| 无环图               | 没有环                 | 家谱               |
| DAG（有向无环图）    | 有向，无环             | 调度、编译器       |
| 完全图               | 所有节点对都相连       | 密集网络           |
| 稀疏图               | 边很少                 | 现实世界中的图     |

#### 5. 基本图操作

- 添加顶点 / 边
- 遍历：深度优先搜索（DFS）、广度优先搜索（BFS）
- 路径查找：Dijkstra（迪杰斯特拉）、Bellman-Ford（贝尔曼-福特）
- 连通性：并查集、Tarjan（强连通分量）
- 生成树：Kruskal（克鲁斯卡尔）、Prim（普里姆）

每个图问题都有其独特之处，从寻找最短路径到检测环。

#### 微型代码

广度优先搜索（BFS）：

```c
void bfs(int start, vector<int> graph[], int n) {
    bool visited[n];
    memset(visited, false, sizeof(visited));
    queue<int> q;
    visited[start] = true;
    q.push(start);
    while (!q.empty()) {
        int node = q.front(); q.pop();
        printf("%d ", node);
        for (int neighbor : graph[node]) {
            if (!visited[neighbor]) {
                visited[neighbor] = true;
                q.push(neighbor);
            }
        }
    }
}
```

这逐层进行探索，非常适合无权图中的最短路径问题。

#### 为何重要

树和图不仅建模序列，还建模关系和连接。它们对于以下方面至关重要：

- 搜索引擎（网络图）
- 编译器（语法树、依赖 DAG）
- 人工智能（状态空间、决策树）
- 数据库（索引、连接、关系）

理解它们将开启一个完整的算法世界，从 DFS 和 BFS 到 Dijkstra、Kruskal 等等。

#### 亲自尝试

1.  构建一个简单的二叉搜索树并实现中序遍历。
2.  用邻接表表示一个图并打印所有边。
3.  为一个小图编写 DFS 和 BFS。
4.  画一个包含环的有向图并手动检测它。

图和树让你超越线性思维。它们让你探索*连接*，而不仅仅是集合。
### 10. 算法设计模式

到目前为止，你已经了解了什么是算法以及如何分析它们。你探索了搜索、排序、数据结构和递归。下一步是学习模式，即可复用的策略，它们指导你如何从零开始构建新算法。

就像软件架构中的设计模式一样，算法设计模式为你的思考提供了结构。一旦你认出了它们，许多问题会突然变得熟悉起来。

#### 1. 暴力法

从简单开始。尝试每一种可能性，然后选出最佳结果。
暴力法通常是你的基线，清晰但效率低下。

示例：
通过检查所有子数组来找到最大子数组和。

- 时间复杂度：$O(n^2)$
- 优点：易于推理
- 缺点：对于大规模输入会急剧膨胀

有时，暴力法能帮助你看到需要更好方法的结构。

#### 2. 分治法

将问题分解成更小的部分，分别解决，然后合并。
适用于具有自相似性的问题。

经典示例：

- 归并排序 → 分割与合并
- 二分查找 → 对半缩小搜索空间
- 快速排序 → 分区与排序

通用形式：

$$
T(n) = aT(n/b) + f(n)
$$

使用递归关系和主定理来分析它们。

#### 3. 贪心法

在每一步做出局部最优决策。
仅当局部最优选择能导致全局最优时才有效。

示例：

- 活动选择
- 哈夫曼编码
- Dijkstra（适用于非负权重）

当适用时，贪心算法简单且快速。

#### 4. 动态规划

当子问题重叠时，存储结果并复用它们。
可以理解为递归加记忆化。

两种主要风格：

- 自顶向下（记忆化）：带缓存的递归
- 自底向上（制表法）：迭代填充表格

应用于：

- 斐波那契数列
- 背包问题
- 最长递增子序列
- 矩阵链乘法

动态规划将指数级递归转化为多项式时间。

#### 5. 回溯法

探索所有可能性，但在不满足约束条件时进行剪枝。
它是带有提前退出的暴力法。

非常适合：

- N皇后问题
- 数独
- 排列生成
- 子集和问题

回溯法逐步构建解，放弃那些无法通向有效结果的路径。

#### 6. 双指针

在序列中移动两个索引以寻找模式或满足条件。

常见用途：

- 有序数组（求和配对、分区）
- 字符串问题（回文、滑动窗口）
- 链表（快慢指针）

简单，但出奇地强大。

#### 7. 滑动窗口

在数据上维护一个窗口，根据需要扩展或收缩它。

用于：

- 最大和子数组（Kadane 算法）
- 长度为 $k$ 的子串
- 无重复字符的最长子串

有助于在序列问题中将 $O(n^2)$ 降低到 $O(n)$。

#### 8. 答案二分查找

有时，输入并未排序，但答案空间是单调的。
如果你能定义一个单调的函数 `check(mid)`（真或假只改变一次），你就可以在可能的答案上应用二分查找。

示例：

- 在 D 天内运送包裹的最小容量
- 满足约束的最小可行值

对于单调条件下的优化问题非常强大。

#### 9. 基于图的算法

从节点、边、路径和流的角度思考。

模式包括：

- BFS 和 DFS（探索）
- 拓扑排序（排序）
- Dijkstra 和 Bellman-Ford（最短路径）
- 并查集（连通性）
- Kruskal 和 Prim（生成树）

图通常能揭示数据中隐藏的关系。

#### 10. 中间相遇法

将问题分成两半，分别计算所有可能性，然后高效地组合。
用于那些暴力法 $O(2^n)$ 太大但 $O(2^{n/2})$ 可处理的问题。

示例：

- 子集和问题（分成两半）
- 组合学中的搜索问题

这是暴力法和效率之间一个巧妙的折衷。

#### 微型代码

示例：使用双指针查找配对和

```c
int find_pair_sum(int arr[], int n, int target) {
    int i = 0, j = n - 1;
    while (i < j) {
        int sum = arr[i] + arr[j];
        if (sum == target) return 1;
        else if (sum < target) i++;
        else j--;
    }
    return 0;
}
```

对于有序数组，时间复杂度为 $O(n)$，优雅且快速。

#### 为何重要

模式是思维的捷径。
它们将“白纸”问题变成了“我以前见过这种模式”。

一旦你识别出结构，你就可以选择合适的模式并加以调整。
这就是顶尖程序员在时间压力下解决复杂问题的方式——不是靠死记硬背算法，而是靠识别模式。

#### 亲自尝试

1.  为最大子数组和问题编写一个暴力解和一个分治解。比较速度。
2.  使用贪心法和动态规划解决零钱兑换问题。
3.  用回溯法实现 N 皇后问题。
4.  使用双指针找到具有给定和的最小子窗口。
5.  选择一个你以前解决过的问题。你能用不同的设计模式重新表述它吗？

你练习的模式越多，就越快能将新问题映射到已知策略，你的算法直觉也会变得更强大。

## 第 2 章. 排序与搜索
### 11. 基础排序算法（冒泡、插入、选择）

在学习归并排序或堆排序等高级排序算法之前，理解基础排序算法非常重要，它们是构建排序知识的基石。这些算法简单、直观，非常适合学习排序在底层是如何工作的。

本节将介绍三种经典算法：

-   **冒泡排序** - 交换相邻的无序对
-   **选择排序** - 每次选择最小的元素
-   **插入排序** - 逐个按顺序插入元素

这些算法的时间复杂度都是 $O(n^2)$，但在行为和稳定性上有所不同。

#### 1. 冒泡排序

**思想**：比较相邻的元素对，如果它们顺序错误就交换。重复此过程直到数组有序。每一轮都会将最大的元素“冒泡”到末尾。

**步骤**：

1.  比较 `arr[j]` 和 `arr[j+1]`
2.  如果 `arr[j] > arr[j+1]` 则交换
3.  继续执行多轮，直到不再需要交换

**代码**：

```c
void bubble_sort(int arr[], int n) {
    for (int i = 0; i < n - 1; i++) {
        int swapped = 0;
        for (int j = 0; j < n - i - 1; j++) {
            if (arr[j] > arr[j + 1]) {
                int temp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = temp;
                swapped = 1;
            }
        }
        if (!swapped) break; // 如果没有发生交换，说明数组已有序，提前结束
    }
}
```

**复杂度**：

-   **最佳情况**：$O(n)$（已排序）
-   **最坏情况**：$O(n^2)$
-   **空间**：$O(1)$
-   **稳定**：是

**直观理解**：想象气泡上升，每一轮过后，最大的“气泡”都会浮到顶部。

#### 2. 选择排序

**思想**：找到最小的元素并将其放在最前面。

**步骤**：

1.  对于每个位置 `i`，在数组剩余部分中找到最小的元素
2.  将其与 `arr[i]` 交换

**代码**：

```c
void selection_sort(int arr[], int n) {
    for (int i = 0; i < n - 1; i++) {
        int min_idx = i;
        for (int j = i + 1; j < n; j++) {
            if (arr[j] < arr[min_idx])
                min_idx = j;
        }
        int temp = arr[i];
        arr[i] = arr[min_idx];
        arr[min_idx] = temp;
    }
}
```

**复杂度**：

-   **最佳情况**：$O(n^2)$
-   **最坏情况**：$O(n^2)$
-   **空间**：$O(1)$
-   **稳定**：否

**直观理解**：选择排序“选择”下一个正确的元素并将其固定到位。它最小化了交换次数，但仍然需要扫描所有元素。

#### 3. 插入排序

**思想**：通过将每个新元素插入到其正确位置，一次一个元素地构建有序数组。

**步骤**：

1.  从索引 1 开始
2.  与前一个元素比较
3.  将所有大于当前元素（key）的元素向右移动
4.  将 key 插入到正确位置

**代码**：

```c
void insertion_sort(int arr[], int n) {
    for (int i = 1; i < n; i++) {
        int key = arr[i];
        int j = i - 1;
        while (j >= 0 && arr[j] > key) {
            arr[j + 1] = arr[j];
            j--;
        }
        arr[j + 1] = key;
    }
}
```

**复杂度**：

-   **最佳情况**：$O(n)$（接近有序）
-   **最坏情况**：$O(n^2)$
-   **空间**：$O(1)$
-   **稳定**：是

**直观理解**：就像整理手中的扑克牌，拿起下一张牌并将其滑入正确的位置。

#### 4. 三种算法对比

| 算法         | 最佳情况 | 平均情况 | 最坏情况 | 稳定 | 原地 | 备注                                 |
| ------------ | -------- | -------- | -------- | ---- | ---- | ------------------------------------ |
| 冒泡排序     | O(n)     | O(n²)    | O(n²)    | 是   | 是   | 可以提前退出                         |
| 选择排序     | O(n²)    | O(n²)    | O(n²)    | 否   | 是   | 交换次数少                           |
| 插入排序     | O(n)     | O(n²)    | O(n²)    | 是   | 是   | 对小规模或接近有序的数据表现很好     |

#### 小例子

让我们看看插入排序如何在 `[5, 3, 4, 1, 2]` 上工作：

-   从 3 开始 → 插入到 5 之前 → `[3, 5, 4, 1, 2]`
-   插入 4 → `[3, 4, 5, 1, 2]`
-   插入 1 → `[1, 3, 4, 5, 2]`
-   插入 2 → `[1, 2, 3, 4, 5]`

经过五轮完成排序。

#### 为何重要

基础排序算法教会你：

-   比较和交换如何驱动排序
-   简单性和效率之间的权衡
-   如何推理稳定性和适应性

虽然这些算法在实践中不用于大型数据集，但它们被*用于*混合算法（如 Timsort 和 IntroSort）的内部，这些算法会为小块数据切换到插入排序。

#### 动手实践

1.  实现所有三种算法，并在每一轮后打印数组。
2.  在以下数组上测试：已排序、逆序、随机、部分有序。
3.  修改冒泡排序使其降序排序。
4.  在 10,000 个元素上尝试插入排序，并观察其行为。
5.  你能检测列表何时已排序并提前停止吗？

从简单开始。掌握这些模式。它们将是你学习从归并排序到基数排序等一切算法的基础。
### 12. 分治排序（归并、快速、堆）

基础排序算法对于学习来说很棒，但其 $O(n^2)$ 的运行时间很快就会成为瓶颈。为了处理超出小数组规模的问题，我们需要能够将问题分解成更小的部分、独立排序、然后合并结果的算法。

这就是**分治法**的精髓：分解问题，解决子问题，合并解决方案。在排序中，这种方法产生了一些最快的通用算法：归并排序、快速排序和堆排序。

#### 1. 归并排序

**思路**：
将数组分成两半，递归地对每一半进行排序，然后将两个已排序的半部分合并。

归并排序是稳定的，在链表上表现良好，并且保证 $O(n \log n)$ 的时间复杂度。

**步骤**：
1.  将数组分成两半
2.  递归地对每一半进行排序
3.  将两个已排序的半部分合并成一个

**代码**：

```c
void merge(int arr[], int l, int m, int r) {
    int n1 = m - l + 1;
    int n2 = r - m;
    int L[n1], R[n2];
    for (int i = 0; i < n1; i++) L[i] = arr[l + i];
    for (int j = 0; j < n2; j++) R[j] = arr[m + 1 + j];
    int i = 0, j = 0, k = l;
    while (i < n1 && j < n2) {
        if (L[i] <= R[j]) arr[k++] = L[i++];
        else arr[k++] = R[j++];
    }
    while (i < n1) arr[k++] = L[i++];
    while (j < n2) arr[k++] = R[j++];
}

void merge_sort(int arr[], int l, int r) {
    if (l < r) {
        int m = (l + r) / 2;
        merge_sort(arr, l, m);
        merge_sort(arr, m + 1, r);
        merge(arr, l, m, r);
    }
}
```

**复杂度**：
-   时间：$O(n \log n)$（总是）
-   空间：$O(n)$（临时数组）
-   稳定：是

归并排序是可预测的，使其成为外部排序（如在磁盘上排序数据）的理想选择。

#### 2. 快速排序

**思路**：
选择一个基准，对数组进行分区，使得较小的元素移到左边，较大的元素移到右边，然后递归地对两边进行排序。

由于良好的缓存局部性和较低的常数因子，快速排序在实践中通常是最快的。

**步骤**：
1.  选择一个基准（通常是中间或随机元素）
2.  分区：将较小的元素移到左边，较大的元素移到右边
3.  递归地对两个分区进行排序

**代码**：

```c
int partition(int arr[], int low, int high) {
    int pivot = arr[high];
    int i = low - 1;
    for (int j = low; j < high; j++) {
        if (arr[j] < pivot) {
            i++;
            int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp;
        }
    }
    int tmp = arr[i + 1]; arr[i + 1] = arr[high]; arr[high] = tmp;
    return i + 1;
}

void quick_sort(int arr[], int low, int high) {
    if (low < high) {
        int pi = partition(arr, low, high);
        quick_sort(arr, low, pi - 1);
        quick_sort(arr, pi + 1, high);
    }
}
```

**复杂度**：
-   最佳 / 平均：$O(n \log n)$
-   最坏：$O(n^2)$（基准选择不当，例如输入已排序且使用简单基准）
-   空间：$O(\log n)$（递归）
-   稳定：否（除非修改）

由于其在实际工作负载中的效率，快速排序常用于标准库中。

#### 3. 堆排序

**思路**：
将数组转换为一个堆，重复提取最大元素，并将其放在末尾。

堆是一种二叉树，其中每个父节点都 ≥ 其子节点（最大堆）。

**步骤**：
1.  构建一个最大堆
2.  将根节点（最大值）与最后一个元素交换
3.  减小堆大小，重新堆化
4.  重复直到排序完成

**代码**：

```c
void heapify(int arr[], int n, int i) {
    int largest = i;
    int l = 2 * i + 1;
    int r = 2 * i + 2;
    if (l < n && arr[l] > arr[largest]) largest = l;
    if (r < n && arr[r] > arr[largest]) largest = r;
    if (largest != i) {
        int tmp = arr[i]; arr[i] = arr[largest]; arr[largest] = tmp;
        heapify(arr, n, largest);
    }
}

void heap_sort(int arr[], int n) {
    for (int i = n / 2 - 1; i >= 0; i--)
        heapify(arr, n, i);
    for (int i = n - 1; i > 0; i--) {
        int tmp = arr[0]; arr[0] = arr[i]; arr[i] = tmp;
        heapify(arr, i, 0);
    }
}
```

**复杂度**：
-   时间：$O(n \log n)$
-   空间：$O(1)$
-   稳定：否

堆排序可靠且空间高效，但比快速排序的缓存友好性差。

#### 4. 比较

| 算法 | 最佳情况 | 平均情况 | 最坏情况 | 空间 | 稳定 | 备注 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 归并排序 | O(n log n) | O(n log n) | O(n log n) | O(n) | 是 | 可预测，稳定 |
| 快速排序 | O(n log n) | O(n log n) | O(n²) | O(log n) | 否 | 实践中速度快 |
| 堆排序 | O(n log n) | O(n log n) | O(n log n) | O(1) | 否 | 原地排序，鲁棒 |

每种算法都有其适用场景：
-   归并排序 → 稳定性和性能保证
-   快速排序 → 速度和缓存性能
-   堆排序 → 低内存使用和简单性

#### 小代码示例

尝试用归并排序对 `[5, 1, 4, 2, 8]` 进行排序：
1.  分割 → `[5,1,4]`, `[2,8]`
2.  分别排序 → `[1,4,5]`, `[2,8]`
3.  合并 → `[1,2,4,5,8]`

每次递归分割都将问题减半，产生 $O(\log n)$ 的递归深度，每层进行 $O(n)$ 的工作。

#### 为何重要

分治排序是高效处理顺序问题的基础。它引入的思想将在以下领域复用：
-   二分查找（对半分割）
-   矩阵乘法
-   快速傅里叶变换
-   动态规划

这些排序算法教你如何将递归、分区和合并结合起来，形成可扩展的解决方案。

#### 动手实践

1.  实现归并排序、快速排序和堆排序。
2.  在同一个随机数组上测试所有三种算法。比较运行时间。
3.  修改快速排序，使其使用随机基准。
4.  构建一个稳定版本的堆排序。
5.  可视化归并排序的递归树和合并过程。

掌握这些排序算法为你提供了一个高效解决任何分治问题的模板。
### 13. 计数与分配排序（计数排序、基数排序、桶排序）

到目前为止，我们已经了解了基于比较的排序算法，如归并排序和快速排序。这些算法依赖于比较元素，并且受限于基于比较的排序算法 O(n log n) 的下限。

但是，如果不需要直接比较元素呢？如果元素是整数或来自有限范围的值呢？

这就是计数排序和分配排序的用武之地。
它们利用数据的结构而不仅仅是顺序，在合适的条件下实现线性时间排序。

#### 1. 计数排序

**思想：**
如果元素是已知范围（例如 [0, k)）内的整数，可以统计每个值出现的次数，然后重建排序后的输出。

计数排序不进行比较，它进行计数。

**步骤：**

1.  找到输入的范围（最大值 k）
2.  在频率数组中统计每个值的出现次数
3.  将计数转换为累积计数
4.  将元素放置到其排序后的位置

**代码：**

```c
void counting_sort(int arr[], int n, int k) {
    int count[k + 1];
    int output[n];
    for (int i = 0; i <= k; i++) count[i] = 0;
    for (int i = 0; i < n; i++) count[arr[i]]++;
    for (int i = 1; i <= k; i++) count[i] += count[i - 1];
    for (int i = n - 1; i >= 0; i--) {
        output[count[arr[i]] - 1] = arr[i];
        count[arr[i]]--;
    }
    for (int i = 0; i < n; i++) arr[i] = output[i];
}
```

**示例：**
arr = [4, 2, 2, 8, 3, 3, 1], k = 8
→ count = [0,1,2,2,1,0,0,0,1]
→ cumulative = [0,1,3,5,6,6,6,6,7]
→ sorted = [1,2,2,3,3,4,8]

**复杂度：**

-   时间复杂度：$O(n + k)$
-   空间复杂度：$O(k)$
-   稳定性：是

**何时使用：**

-   输入是整数
-   范围 k 不比 n 大很多

#### 2. 基数排序

**思想：**
一次排序一位数字，从最低有效位 (LSD) 或最高有效位 (MSD) 开始，使用稳定的子排序算法（如计数排序）。

基数排序在所有元素具有固定长度表示（例如整数、等长字符串）时效果最佳。

**步骤（LSD 方法）：**

1.  对于每个数位位置（从最右到最左）
2.  使用稳定排序（如计数排序）根据该数位对所有元素进行排序

**代码：**

```c
int get_max(int arr[], int n) {
    int mx = arr[0];
    for (int i = 1; i < n; i++)
        if (arr[i] > mx) mx = arr[i];
    return mx;
}

void counting_sort_digit(int arr[], int n, int exp) {
    int output[n];
    int count[10] = {0};
    for (int i = 0; i < n; i++)
        count[(arr[i] / exp) % 10]++;
    for (int i = 1; i < 10; i++)
        count[i] += count[i - 1];
    for (int i = n - 1; i >= 0; i--) {
        int digit = (arr[i] / exp) % 10;
        output[count[digit] - 1] = arr[i];
        count[digit]--;
    }
    for (int i = 0; i < n; i++)
        arr[i] = output[i];
}

void radix_sort(int arr[], int n) {
    int m = get_max(arr, n);
    for (int exp = 1; m / exp > 0; exp *= 10)
        counting_sort_digit(arr, n, exp);
}
```

**示例：**
arr = [170, 45, 75, 90, 802, 24, 2, 66]
→ 按个位排序 → 十位排序 → 百位排序
→ 最终结果 = [2, 24, 45, 66, 75, 90, 170, 802]

**复杂度：**

-   时间复杂度：$O(d \times (n + b))$，其中
    -   $d$：数字位数
    -   $b$：基数（十进制为 10）
-   空间复杂度：$O(n + b)$
-   稳定性：是

**何时使用：**

-   固定长度的数字
-   有界的数位（例如，基数为 10 或 2）

#### 3. 桶排序

**思想：**
根据值的范围将元素划分到不同的桶中，分别对每个桶进行排序，然后合并。

当数据在已知区间内均匀分布时效果最佳。

**步骤：**

1.  为值范围创建 k 个桶
2.  将元素分配到桶中
3.  对每个桶进行排序（通常使用插入排序）
4.  合并桶

**代码：**

```c
void bucket_sort(float arr[], int n) {
    vector<float> buckets[n];
    for (int i = 0; i < n; i++) {
        int idx = n * arr[i]; // 假设 0 <= arr[i] < 1
        buckets[idx].push_back(arr[i]);
    }
    for (int i = 0; i < n; i++)
        sort(buckets[i].begin(), buckets[i].end());
    int idx = 0;
    for (int i = 0; i < n; i++)
        for (float val : buckets[i])
            arr[idx++] = val;
}
```

**复杂度：**

-   平均情况：$O(n + k)$
-   最坏情况：$O(n^2)$（如果所有元素都落入一个桶）
-   空间复杂度：$O(n + k)$
-   稳定性：取决于桶内排序方法

**何时使用：**

-   在 $[0,1)$ 区间内均匀分布的实数

#### 4. 对比

| 算法         | 时间复杂度       | 空间复杂度 | 稳定性 | 类型             | 最佳适用场景         |
| ------------ | ---------------- | ---------- | ------ | ---------------- | -------------------- |
| 计数排序     | $O(n + k)$       | $O(k)$     | 是     | 非比较型         | 小范围整数           |
| 基数排序     | $O(d(n + b))$    | $O(n + b)$ | 是     | 非比较型         | 固定长度数字         |
| 桶排序       | $O(n + k)$ 平均  | $O(n + k)$ | 通常   | 基于分配         | 均匀分布的浮点数     |

当假设条件成立时，这些算法可以达到 $O(n)$ 的性能，它们是专门的算法，但在适用时速度极快。

#### 小代码示例

让我们在 `arr = [4, 2, 2, 8, 3, 3, 1]` 上逐步执行计数排序：

-   统计出现次数 → [1,2,2,1,0,0,0,1]
-   累积计数 → 确定位置
-   放置元素 → [1,2,2,3,3,4,8]
排序完成，没有进行比较。

#### 为何重要

分配排序揭示了一个关键见解：

> 如果你了解数据的结构，你可以比基于比较的排序更快地完成排序。

它们展示了数据的属性——范围、分布、数字长度——如何驱动算法设计。

你将在以下领域再次遇到这些思想：

-   哈希（分桶）
-   索引（范围分区）
-   机器学习（分箱、直方图）

#### 动手实践

1.  实现一个针对 0 到 100 之间整数的计数排序。
2.  扩展基数排序，使其能够按字符对字符串进行排序。
3.  可视化对 0 到 1 之间值的桶排序过程。
4.  如果对负数使用计数排序会发生什么？如何修复？
5.  在小整数数组上比较计数排序和快速排序。

这些是线性时间排序的初步探索——利用关于数据的知识来突破 $O(n \log n)$ 的障碍。
### 14. 混合排序算法（IntroSort、Timsort）

在实践中，没有一种排序算法能完美适用于所有情况。
有些算法平均速度很快，但在最坏情况下表现不佳（如快速排序）。
另一些算法性能稳定，但由于开销而较慢（如归并排序）。
混合排序算法结合了多种技术，以取得*最佳的综合效果*：实际速度快、稳定且有性能保证。

在现代系统中，两种最广泛使用的混合排序算法是 IntroSort 和 Timsort，它们都是主流编程语言中排序函数的底层实现。

#### 1. 混合排序背后的思想

现实世界的数据是杂乱的：有时近乎有序，有时随机，有时是病态的。
一个聪明的排序算法应该能够适应数据。

混合排序根据以下因素在不同策略间切换：

- 输入大小
- 递归深度
- 有序程度
- 性能阈值

因此，算法在运行时进行"内省"或"自适应"。

#### 2. IntroSort

IntroSort（*内省排序*的简称）开始时像快速排序，但当递归深度过深时——这意味着快速排序的最坏情况可能即将出现——它会切换到堆排序，以保证 (O$n \log n$) 的时间复杂度。

步骤：

1. 只要递归深度 < $2 \log n$，就使用快速排序
2. 如果深度超过限制 → 切换到堆排序
3. 对于非常小的子数组 → 切换到插入排序

这种三重组合确保了：

- 快速的平均情况（快速排序）
- 有保证的上界（堆排序）
- 小数组上的高效性（插入排序）

代码草图：

```c
void intro_sort(int arr[], int n) {
    int depth_limit = 2 * log(n);
    intro_sort_util(arr, 0, n - 1, depth_limit);
}

void intro_sort_util(int arr[], int begin, int end, int depth_limit) {
    int size = end - begin + 1;
    if (size < 16) {
        insertion_sort(arr + begin, size);
        return;
    }
    if (depth_limit == 0) {
        heap_sort_range(arr, begin, end);
        return;
    }
    int pivot = partition(arr, begin, end);
    intro_sort_util(arr, begin, pivot - 1, depth_limit - 1);
    intro_sort_util(arr, pivot + 1, end, depth_limit - 1);
}
```

复杂度：

- 平均： (O$n \log n$)
- 最坏： (O$n \log n$)
- 空间： (O$\log n$)
- 稳定： 否（取决于分区方案）

使用场景：

- C++ STL 的 `std::sort`
- 许多需要性能保证的系统

#### 3. Timsort

Timsort 是一种稳定的混合排序算法，结合了插入排序和归并排序。
它被设计用来处理现实世界的数据，这些数据通常包含*游程*（已经排序好的片段）。

由 Tim Peters（Python 核心开发者）开发，Timsort 现在被用于：

- Python 的 `sorted()` 和 `.sort()`
- Java 中用于对象的 `Arrays.sort()`

思想：

- 识别*游程*——已经升序或降序的片段
- 反转降序游程（使其变为升序）
- 使用插入排序对小游程进行排序
- 使用归并排序合并游程

Timsort 能很好地适应部分有序的数据。

步骤：

1.  扫描数组，检测游程（已经排序的序列）
2.  将游程压入栈
3.  使用精心平衡的合并策略合并游程

伪代码（简化版）：

```python
def timsort(arr):
    RUN = 32
    n = len(arr)

    # 步骤 1： 对小块进行排序
    for i in range(0, n, RUN):
        insertion_sort(arr, i, min((i + RUN - 1), n - 1))

    # 步骤 2： 合并已排序的游程
    size = RUN
    while size < n:
        for start in range(0, n, size * 2):
            mid = start + size - 1
            end = min(start + size * 2 - 1, n - 1)
            merge(arr, start, mid, end)
        size *= 2
```

复杂度：

- 最佳： (O(n)) （数据已排序）
- 平均： (O$n \log n$)
- 最坏： (O$n \log n$)
- 空间： (O(n))
- 稳定： 是

主要优势：

- 对现实世界中部分有序的数据表现极佳
- 稳定（保持相等键的顺序）
- 优化的合并（自适应合并）

#### 4. 对比

| 算法 | 基础方法 | 稳定性 | 最佳 | 平均 | 最坏 | 实际应用 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| IntroSort | 快速 + 堆 + 插入 | 否 | O(n log n) | O(n log n) | O(n log n) | C++ STL |
| Timsort | 归并 + 插入 | 是 | O(n) | O(n log n) | O(n log n) | Python, Java |

IntroSort 优先考虑性能保证。
Timsort 优先考虑适应性和稳定性。

两者都表明，"一刀切"的排序方法并不存在——优秀的系统会检测*正在发生的情况*并进行适应。

#### 小代码示例

假设我们在 `[1, 2, 3, 7, 6, 5, 8, 9]` 上运行 Timsort：

- 检测游程： `[1,2,3]`, `[7,6,5]`, `[8,9]`
- 反转 `[7,6,5]` → `[5,6,7]`
- 合并游程 → `[1,2,3,5,6,7,8,9]`

之所以高效，是因为它利用了现有的顺序。

#### 为何重要

混合排序算法是现实世界的英雄——它们将理论与实践相结合。
它们传授了一个重要的原则：

> 当一个算法的弱点显现时，就切换到另一个算法的长处。

这些并非学术上的奇技淫巧——它们就在你的编译器、浏览器、操作系统和数据库中。
理解它们意味着你理解了现代语言如何优化基本操作。

#### 动手尝试

1.  实现 IntroSort 并在随机、已排序和逆序数组上进行测试。
2.  在近乎有序的输入上模拟 Timsort 的游程检测。
3.  比较插入排序与 Timsort 对小数组的排序速度。
4.  在快速排序中添加计数器，看看 IntroSort 应该在何时切换。
5.  探索 Python 的 `sorted()` 在不同输入形态下的表现——猜测它何时使用归并排序，何时使用插入排序。

混合排序算法提醒我们：好的算法会适应——它们不是僵化的，而是聪明的。
### 15. 特殊排序算法（Cycle, Gnome, Comb, Pancake）

并非所有的排序算法都遵循主流的分治或分布范式。有些算法是为了解决特定领域问题、阐释优雅的思想，或者仅仅是为了尝试不同的排序机制而设计的。

这些特殊排序算法——Cycle Sort（循环排序）、Gnome Sort（侏儒排序）、Comb Sort（梳排序）和 Pancake Sort（煎饼排序）——之所以引人入胜，并非因为它们是最快的，而是因为它们揭示了思考排列、局部顺序和原地操作等问题的创造性方式。

#### 1. Cycle Sort（循环排序）

**核心思想**：
最小化写入操作次数。
循环排序通过将元素重新排列成循环，将每个值直接放置在其正确的位置。
它执行的写入次数恰好等于错位元素的数量，这使其非常适合闪存或写入成本高昂的系统。

**步骤**：

1.  对于每个位置 `i`，找出 `arr[i]` 应属的位置（其排名）。
2.  如果它不在那里，则将其交换到该位置。
3.  继续这个循环，直到当前位置正确为止。
4.  移动到下一个索引。

**代码**：

```c
void cycle_sort(int arr[], int n) {
    for (int cycle_start = 0; cycle_start < n - 1; cycle_start++) {
        int item = arr[cycle_start];
        int pos = cycle_start;

        for (int i = cycle_start + 1; i < n; i++)
            if (arr[i] < item) pos++;

        if (pos == cycle_start) continue;

        while (item == arr[pos]) pos++;
        int temp = arr[pos];
        arr[pos] = item;
        item = temp;

        while (pos != cycle_start) {
            pos = cycle_start;
            for (int i = cycle_start + 1; i < n; i++)
                if (arr[i] < item) pos++;
            while (item == arr[pos]) pos++;
            temp = arr[pos];
            arr[pos] = item;
            item = temp;
        }
    }
}
```

**复杂度**：

-   时间：$O(n^2)$
-   写入次数：最小（恰好为 n-c，其中 c = 循环数）
-   稳定性：否

**适用场景**：
当最小化写入次数比运行时间更重要时。

#### 2. Gnome Sort（侏儒排序）

**核心思想**：
插入排序的一个更简单的变体。
侏儒排序像整理花盆的“侏儒”一样来回移动：如果两个相邻的花盆顺序不对，就交换它们并后退一步；否则，就前进一步。

**步骤**：

1.  从索引 1 开始
2.  如果 `arr[i] >= arr[i-1]`，则向前移动
3.  否则，交换并后退一步（如果可能）
4.  重复直到结束

**代码**：

```c
void gnome_sort(int arr[], int n) {
    int i = 1;
    while (i < n) {
        if (i == 0 || arr[i] >= arr[i - 1]) i++;
        else {
            int temp = arr[i]; arr[i] = arr[i - 1]; arr[i - 1] = temp;
            i--;
        }
    }
}
```

**复杂度**：

-   时间：$O(n^2)$
-   空间：$O(1)$
-   稳定性：是

**适用场景**：
教学上的简单性。它是一种没有嵌套循环、可读性强的插入逻辑形式。

#### 3. Comb Sort（梳排序）

**核心思想**：
通过在被比较元素之间引入一个间隙并逐渐缩小它，对冒泡排序进行改进。
通过早期跳跃更远的距离，梳排序有助于消除“卡”在末尾附近的小元素。

**步骤**：

1.  初始间隙 = n
2.  在每一轮中，缩小间隙 = gap / 1.3
3.  比较并交换相隔 `gap` 个位置的元素
4.  当间隙 = 1 且没有发生交换时停止

**代码**：

```c
void comb_sort(int arr[], int n) {
    int gap = n;
    int swapped = 1;
    while (gap > 1 || swapped) {
        gap = (gap * 10) / 13;
        if (gap == 9 || gap == 10) gap = 11;
        if (gap < 1) gap = 1;
        swapped = 0;
        for (int i = 0; i + gap < n; i++) {
            if (arr[i] > arr[i + gap]) {
                int temp = arr[i]; arr[i] = arr[i + gap]; arr[i + gap] = temp;
                swapped = 1;
            }
        }
    }
}
```

**复杂度**：

-   平均：$O(n \log n)$
-   最坏：$O(n^2)$
-   空间：$O(1)$
-   稳定性：否

**适用场景**：
当需要一个简单的、原地的、接近线性时间的冒泡排序替代方案时。

#### 4. Pancake Sort（煎饼排序）

**核心思想**：
仅使用一种操作来排序数组：翻转（反转一个前缀）。
这就像整理盘子里的煎饼，翻转煎饼堆使最大的煎饼到底部，然后对其余部分重复此过程。

**步骤**：

1.  找到未排序部分中的最大元素
2.  将其翻转到最前面
3.  再次将其翻转到其正确位置
4.  将未排序部分的大小减一

**代码**：

```c
void flip(int arr[], int i) {
    int start = 0;
    while (start < i) {
        int temp = arr[start];
        arr[start] = arr[i];
        arr[i] = temp;
        start++;
        i--;
    }
}

void pancake_sort(int arr[], int n) {
    for (int curr_size = n; curr_size > 1; curr_size--) {
        int mi = 0;
        for (int i = 1; i < curr_size; i++)
            if (arr[i] > arr[mi]) mi = i;
        if (mi != curr_size - 1) {
            flip(arr, mi);
            flip(arr, curr_size - 1);
        }
    }
}
```

**复杂度**：

-   时间：$O(n^2)$
-   空间：$O(1)$
-   稳定性：否

**趣闻**：
煎饼排序是唯一已知的模拟厨房用具操作的算法，并激发了组合数学和基因组重排理论中的“烧焦煎饼问题”。

#### 5. 对比

| 算法         | 时间复杂度        | 空间复杂度 | 稳定性 | 显著特点                     |
| ------------ | ----------------- | ---------- | ------ | ---------------------------- |
| Cycle Sort   | O(n²)             | O(1)       | 否     | 写入次数最少                 |
| Gnome Sort   | O(n²)             | O(1)       | 是     | 类似插入的简单行为           |
| Comb Sort    | 平均 O(n log n)   | O(1)       | 否     | 收缩间隙，改进的冒泡排序     |
| Pancake Sort | O(n²)             | O(1)       | 否     | 仅使用前缀翻转操作           |

每种算法都突出了不同的设计目标：

-   Cycle：最小化写入次数
-   Gnome：简化逻辑
-   Comb：优化比较次数
-   Pancake：限制操作类型

#### 小例子

示例（对 `[3, 6, 1, 9]` 进行煎饼排序）：

1.  最大值 = 9 在索引 3 → flip(3) → `[9,1,6,3]`
2.  flip(3) → `[3,6,1,9]` (9 已固定)
3.  最大值 = 6 → flip(1) → `[6,3,1,9]`
4.  flip(2) → `[1,3,6,9]`

仅使用翻转操作完成排序。

#### 为何重要

特殊排序算法表明，思考排序的方式不止一种。
它们是探索新思想的实验室：最小化交换次数、限制操作类型或优化稳定性。
即使它们不是生产环境中的首选，它们也能加深你对排序机制的理解。

#### 动手尝试

1.  实现每种算法，并逐步可视化它们的操作过程。
2.  测量循环排序与其他算法相比执行了多少次写入操作。
3.  在接近有序的数组上比较侏儒排序和插入排序。
4.  修改梳排序的收缩因子，性能如何变化？
5.  编写煎饼排序，并打印每次翻转后的状态，观察“煎饼堆”的动态变化。

这些奇特的算法证明，排序不仅是科学，也是艺术和实验。
### 16. 线性搜索与二分搜索

搜索是在数据集合中寻找目标值的过程。根据数据是否已排序，你将采用不同的策略。

在本节中，我们将重温两种最基础的搜索方法——线性搜索和二分搜索，并了解它们如何支撑许多更高级的算法和数据结构。

#### 1. 线性搜索

**思路**：
逐个检查每个元素，直到找到目标值。这是最简单的搜索方法，适用于未排序的数据。

**步骤**：
1.  从索引 0 开始
2.  比较 `arr[i]` 与目标值
3.  若匹配，返回索引
4.  若到达末尾，返回 -1

**代码**：
```c
int linear_search(int arr[], int n, int key) {
    for (int i = 0; i < n; i++) {
        if (arr[i] == key) return i;
    }
    return -1;
}
```

**示例**：
arr = [7, 2, 4, 9, 1], key = 9
- 比较 7, 2, 4, 然后 9 → 在索引 3 处找到

**复杂度**：
- 时间复杂度：$O(n)$
- 空间复杂度：$O(1)$
- 最佳情况：$O(1)$ (第一个元素)
- 最坏情况：$O(n)$

**优点**：
- 适用于任何数据（已排序或未排序）
- 实现简单

**缺点**：
- 在大型数组上效率低下

**适用场景**：
当数据量小或未排序，或者简单性比速度更重要时使用。

#### 2. 二分搜索

**思路**：
如果数组已排序，你可以反复将搜索空间减半。将中间元素与目标值比较——如果更大，则搜索左侧；如果更小，则搜索右侧。

**步骤**：
1.  找到中点
2.  如果 `arr[mid] == key`，完成
3.  如果 `arr[mid] > key`，搜索左侧
4.  如果 `arr[mid] < key`，搜索右侧
5.  重复直到范围为空

**迭代版本**：
```c
int binary_search(int arr[], int n, int key) {
    int low = 0, high = n - 1;
    while (low <= high) {
        int mid = (low + high) / 2;
        if (arr[mid] == key) return mid;
        else if (arr[mid] < key) low = mid + 1;
        else high = mid - 1;
    }
    return -1;
}
```

**递归版本**：
```c
int binary_search_rec(int arr[], int low, int high, int key) {
    if (low > high) return -1;
    int mid = (low + high) / 2;
    if (arr[mid] == key) return mid;
    else if (arr[mid] > key)
        return binary_search_rec(arr, low, mid - 1, key);
    else
        return binary_search_rec(arr, mid + 1, high, key);
}
```

**示例**：
arr = [1, 3, 5, 7, 9, 11], key = 7
- mid = 5 → key > mid → 向右移动
- mid = 7 → 找到

**复杂度**：
- 时间复杂度：$O(\log n)$
- 空间复杂度：$O(1)$ (迭代) 或 $O(\log n)$ (递归)
- 最佳情况：$O(1)$ (中间元素)

**要求**：
- 必须已排序
- 必须支持随机访问（数组，而非链表）

**优点**：
- 对于大型已排序数组非常快
- 是高级搜索（如插值搜索、指数搜索）的基础

**缺点**：
- 需要排序的数据
- 不适应频繁的插入/删除操作

#### 3. 二分搜索变体

二分搜索既是一个单一算法，也是一种*模式*。你可以调整它来查找：
- 首次出现：如果 `arr[mid] == key`，则向左移动
- 末次出现：如果 `arr[mid] == key`，则向右移动
- 下界：第一个 ≥ key 的索引
- 上界：第一个 > key 的索引

**示例（下界）**：
```c
int lower_bound(int arr[], int n, int key) {
    int low = 0, high = n;
    while (low < high) {
        int mid = (low + high) / 2;
        if (arr[mid] < key) low = mid + 1;
        else high = mid;
    }
    return low;
}
```

**用途**：
这些变体支撑着 C++ 中的 `std::lower_bound()` 等函数以及二叉搜索树的查找逻辑。

#### 4. 比较

| 算法         | 适用场景 | 时间复杂度 | 空间复杂度 | 需要排序数据 | 备注                       |
| ------------ | -------- | ---------- | ---------- | ------------ | -------------------------- |
| 线性搜索     | 任意     | O(n)       | O(1)       | 否           | 最适合小型/未排序数据      |
| 二分搜索     | 已排序   | O(log n)   | O(1)       | 是           | 在有序数组上最快           |

二分搜索用简单性换取强大能力——一旦你的数据被排序，你就解锁了亚线性搜索。

#### 小试代码

在数组 `[2, 4, 6, 8, 10]` 上比较，key = 8：
- 线性搜索：4 步
- 二分搜索：2 步

这个差距随着规模增长而变得巨大——对于 $n = 10^6$，线性搜索最多需要一百万步，二分搜索大约只需 20 步。

#### 为何重要

这两种搜索构成了检索的基础。线性搜索展示了暴力迭代；二分搜索展示了结构（排序顺序）如何带来指数级的改进。

从数据库到编译器符号表，再到树的查找，这个原则——*分而治之以更快搜索*——无处不在。

#### 动手实践

1.  实现线性搜索和二分搜索。
2.  计算 $n = 10, 100, 1000$ 时的比较次数。
3.  修改二分搜索以返回重复元素的首次出现位置。
4.  尝试在未排序数据上进行二分搜索——会发生什么？
5.  结合排序：先排序数组，然后搜索。

掌握这些搜索方法能为你建立对所有查找操作的直觉——它们是通往高效数据检索的大门。
### 17. 插值查找与指数查找

线性查找和二分查找在许多场景下都表现良好，但它们没有考虑数据的分布情况。当数值均匀分布时，我们可以*估计*目标值所在的位置，而不是总是将范围对半分。这就引出了插值查找，它会"跳跃"到目标值可能存在的附近位置。

对于无界或无限列表，我们甚至无法预先知道数组的大小，这正是指数查找大显身手的地方，它通过快速扩大搜索窗口，然后再切换到二分查找。

让我们深入探讨这两种方法。

#### 1. 插值查找

**思想：**
如果数据是已排序且均匀分布的，你可以使用线性插值来*预测*关键字可能的位置。不是从中间分割，而是根据关键字在值域范围内的比例来估算位置。

**公式：**
$$
\text{pos} = \text{low} + \frac{(key - arr[low]) \times (high - low)}{arr[high] - arr[low]}
$$

这个公式"猜测"关键字所在的位置。如果 `key == arr[pos]`，我们就找到了。否则，调整 `low` 或 `high` 并重复此过程。

**步骤：**

1.  计算估算位置 `pos`
2.  比较 `arr[pos]` 与 `key`
3.  相应地缩小范围
4.  当 `low <= high` 且 `key` 在范围内时重复

**代码：**

```c
int interpolation_search(int arr[], int n, int key) {
    int low = 0, high = n - 1;

    while (low <= high && key >= arr[low] && key <= arr[high]) {
        if (low == high) {
            if (arr[low] == key) return low;
            return -1;
        }
        int pos = low + ((double)(key - arr[low]) * (high - low)) / (arr[high] - arr[low]);

        if (arr[pos] == key)
            return pos;
        if (arr[pos] < key)
            low = pos + 1;
        else
            high = pos - 1;
    }
    return -1;
}
```

**示例：**
arr = [10, 20, 30, 40, 50], key = 40
pos = 0 + ((40 - 10) * (4 - 0)) / (50 - 10) = 3 → 在索引 3 处找到

**复杂度：**

*   **最佳情况：** $O(1)$
*   **平均情况：** $O(\log \log n)$ （数据均匀）
*   **最坏情况：** $O(n)$ （数据不均匀或偏斜）
*   **空间：** $O(1)$

**何时使用：**

*   数据已排序且接近均匀分布
*   数值数据，其值稳定增长

**注意：**
插值查找是自适应的，当数据可预测时更快，数据不规则时则较慢。

#### 2. 指数查找

**思想：**
当你不知道数组大小时（例如，无限流、链表数据、文件），你不能简单地从 0 到 n-1 进行二分查找。指数查找通过倍增步长来动态地找到一个搜索范围，直到超过目标值，然后在该范围内进行二分查找。

**步骤：**

1.  如果 `arr[0] == key`，返回 0
2.  找到一个范围 `[bound/2, bound]`，使得 `arr[bound] >= key`
3.  在该范围内执行二分查找

**代码：**

```c
int exponential_search(int arr[], int n, int key) {
    if (arr[0] == key) return 0;
    int bound = 1;
    while (bound < n && arr[bound] < key)
        bound *= 2;
    int low = bound / 2;
    int high = (bound < n) ? bound : n - 1;
    // 在 [low, high] 范围内进行二分查找
    while (low <= high) {
        int mid = (low + high) / 2;
        if (arr[mid] == key) return mid;
        else if (arr[mid] < key) low = mid + 1;
        else high = mid - 1;
    }
    return -1;
}
```

**示例：**
arr = [2, 4, 6, 8, 10, 12, 14, 16], key = 10

*   步骤：bound = 1 (4), 2 (6), 4 (10 ≥ key)
*   二分查找 [2,4] → 找到

**复杂度：**

*   **时间：** $O(\log i)$，其中 $i$ 是目标值的索引
*   **空间：** $O(1)$
*   **最佳情况：** $O(1)$

**何时使用：**

*   无界或流式数据
*   数组大小未知但已排序

#### 3. 比较

| 算法 | 最佳情况 | 平均情况 | 最坏情况 | 数据要求 | 备注 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 线性查找 | O(1) | O(n) | O(n) | 无需排序 | 适用于所有情况 |
| 二分查找 | O(1) | O(log n) | O(log n) | 已排序 | 可预测的对半分割 |
| 插值查找 | O(1) | O(log log n) | O(n) | 已排序 + 均匀 | 自适应，在均匀数据上快 |
| 指数查找 | O(1) | O(log n) | O(log n) | 已排序 | 适用于大小未知的情况 |

如果数据平滑，插值查找优于二分查找。当大小未知时，指数查找表现出色。

#### 微型代码

**插值查找的直观理解：**
如果你的数据是等间距的（10, 20, 30, 40, 50），那么值 40 应该大致在 75% 的位置。我们不是每次都折半，而是直接*跳到它附近*。这是一种数据感知的查找。

**指数查找的直观理解：**
当大小未知时，"不断扩展直到找到边界"，然后在边界内查找。

#### 为何重要

这两种查找展示了上下文如何影响算法设计：

*   *数据分布*（插值查找）
*   *边界*（指数查找）

它们告诉我们，性能不仅取决于结构（排序性），还取决于元数据——你对数据间距或限制的了解程度。

这些原则在跳表、搜索树和概率索引中会再次出现。

#### 动手实践

1.  在 [10, 20, 30, 40, 50] 上测试插值查找，注意它所需的步骤之少。
2.  在 [1, 2, 4, 8, 16, 32, 64] 上尝试同样的操作，注意速度变慢。
3.  实现指数查找，并通过在 `n` 处停止来模拟一个"无限"数组。
4.  在随机数据与均匀数据上比较二分查找和插值查找。
5.  将指数查找扩展到链表，复杂度如何变化？

理解这些查找方法有助于你根据数据的形状定制查找策略，这是算法思维中的一项关键技能。
### 18. 选择算法（快速选择、中位数的中位数）

有时你不需要对整个数组进行排序，你只想要第 k 小（或第 k 大）的元素。当你只需要一个特定排名的元素时，排序所有元素是大材小用。选择算法能高效地解决这个问题，通常在线性时间内完成。

它们是寻找中位数、百分位数和顺序统计量算法的基石，并且支撑着像快速排序中的*枢轴选择*这样的操作。

#### 1. 选择问题

给定一个包含 n 个元素的未排序数组和一个数字 k，找出如果数组被排序后，将位于位置 k 的元素。

例如：
arr = [7, 2, 9, 4, 6], (k = 3)
→ 排序后 = [2, 4, 6, 7, 9]
→ 第 3 小的元素 = 6

我们可以在不排序所有元素的情况下解决这个问题。

#### 2. 快速选择

思路：
快速选择是快速排序的一个选择变体。
它围绕一个枢轴对数组进行分区，但只递归处理包含第 k 个元素的那一侧。

它的平均时间复杂度是 O(n)，因为每次分区大致将搜索空间减半。

步骤：

1.  选择一个枢轴（随机或最后一个元素）
2.  将数组分区为小于枢轴和大于枢轴的元素
3.  让 `pos` 为分区后枢轴的索引
4.  如果 `pos == k-1` → 完成
5.  如果 `pos > k-1` → 递归处理左侧
6.  如果 `pos < k-1` → 递归处理右侧

代码：

```c
int partition(int arr[], int low, int high) {
    int pivot = arr[high];
    int i = low;
    for (int j = low; j < high; j++) {
        if (arr[j] < pivot) {
            int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;
            i++;
        }
    }
    int temp = arr[i]; arr[i] = arr[high]; arr[high] = temp;
    return i;
}

int quickselect(int arr[], int low, int high, int k) {
    if (low == high) return arr[low];
    int pos = partition(arr, low, high);
    int rank = pos - low + 1;
    if (rank == k) return arr[pos];
    if (rank > k) return quickselect(arr, low, pos - 1, k);
    return quickselect(arr, pos + 1, high, k - rank);
}
```

示例：
arr = [7, 2, 9, 4, 6], ( k = 3 )

-   枢轴 = 6
-   分区 → [2, 4, 6, 9, 7], pos = 2
-   rank = 3 → 找到 (6)

复杂度：

-   平均：$O(n)$
-   最坏：$O(n^2)$ （糟糕的枢轴）
-   空间：$O(1)$
-   原地算法

何时使用：

-   平均情况快
-   不需要完全排序

快速选择被用于 C++ 的 `nth_element()` 和许多寻找中位数的实现中。

#### 3. 中位数的中位数

思路：
通过确定性地选择一个好的枢轴，保证最坏情况下的 $O(n)$ 时间复杂度。

此方法确保枢轴每次都将数组划分为大致平衡的部分。

步骤：

1.  将数组分成每组 5 个元素
2.  找到每组的**中位数**（使用插入排序）
3.  递归地找到这些中位数的**中位数** → 枢轴
4.  围绕此枢轴对数组进行分区
5.  递归处理包含第 k 个元素的那一侧

这保证了每一步至少消除 30% 的元素 → 最坏情况下线性时间。

代码概览：

```c
int select_pivot(int arr[], int low, int high) {
    int n = high - low + 1;
    if (n <= 5) {
        insertion_sort(arr + low, n);
        return low + n / 2;
    }

    int medians[(n + 4) / 5];
    int i;
    for (i = 0; i < n / 5; i++) {
        insertion_sort(arr + low + i * 5, 5);
        medians[i] = arr[low + i * 5 + 2];
    }
    if (i * 5 < n) {
        insertion_sort(arr + low + i * 5, n % 5);
        medians[i] = arr[low + i * 5 + (n % 5) / 2];
        i++;
    }
    return select_pivot(medians, 0, i - 1);
}
```

然后你会像快速选择一样围绕 `pivot` 进行分区和递归。

复杂度：

-   最坏：$O(n)$
-   空间：$O(1)$ （原地版本）
-   稳定：否（不保持顺序）

为何重要：
中位数的中位数在实践中比快速选择慢，但提供了理论保证，在实时或关键系统中至关重要。

#### 4. 特殊情况

-   **最小 / 最大**：很简单，只需扫描一次 $O(n)$
-   **中位数**：$k = \lceil n/2 \rceil$，可以使用快速选择或中位数的中位数
-   **Top-k 元素**：使用部分选择或堆（k 个最小/最大）

示例：
要从一百万个条目中获取前 5 个分数，使用快速选择找到第 5 大的元素，然后过滤 ≥ 该阈值的元素。

#### 5. 比较

| 算法 | 最佳 | 平均 | 最坏 | 稳定 | 原地 | 备注 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 快速选择 | O(n) | O(n) | O(n²) | 否 | 是 | 实践中快 |
| 中位数的中位数 | O(n) | O(n) | O(n) | 否 | 是 | 确定性的 |
| 排序 | O(n log n) | O(n log n) | O(n log n) | 取决于 | 取决于 | 对单个元素来说是大材小用 |

快速选择快速简单；中位数的中位数安全且可预测。

#### 微型代码

在 [9, 7, 2, 5, 4, 3] 中查找第 4 小的元素：

-   枢轴 = 4 → 分区 [2,3,4,9,7,5]
-   4 在位置 2 → rank = 3 < 4 → 递归处理右侧
-   新范围 [9,7,5], ( k = 1 ) → 最小 = 5
    结果：5

#### 为何重要

选择算法揭示了一个关键见解：

> 有时你不需要一切，只需要重要的部分。

它们构成了以下内容的基础：

-   信号处理中的中值滤波器
-   排序中的分区步骤
-   第 k 阶统计量
-   稳健统计和分位数计算

它们体现了“部分工作，完整答案”的哲学，做得恰到好处。

#### 亲自尝试

1.  实现快速选择并为不同的 k 查找第 k 小的元素。
2.  比较运行时间与完全排序。
3.  修改快速选择以查找第 k 大的元素。
4.  实现中位数的中位数枢轴选择。
5.  使用快速选择查找 1000 个随机元素的中位数。

掌握选择算法有助于你思考效率，你将学会何时停止排序并开始选择。
### 19. 范围搜索与最近邻

搜索并不总是为了找到单个键值。
通常，你需要找到给定范围内的所有元素，或者找到与查询点最接近的匹配项。

这些问题在数据库、计算几何和机器学习（如 k-NN 分类）中至关重要。
本节将介绍用于范围查询（例如，查找 `L` 和 `R` 之间的所有值）和最近邻搜索（例如，查找最接近查询点 `q` 的点）的算法。

#### 1. 范围搜索

核心思想：
给定一组数据点（一维或多维），快速报告指定范围内的所有点。

在一维（简单数组）中，范围查询可以通过二分查找和前缀和来处理。
在更高维度中，我们需要为高效空间查询而设计的树结构。

#### A. 一维范围查询（有序数组）

目标：找到 `[L, R]` 中的所有元素。

步骤：

1.  使用下界查找第一个 ≥ L 的元素
2.  使用上界查找第一个 > R 的元素
3.  输出两者之间的所有元素

代码（C++ 风格伪代码）：

```c
int l = lower_bound(arr, arr + n, L) - arr;
int r = upper_bound(arr, arr + n, R) - arr;
for (int i = l; i < r; i++)
    printf("%d ", arr[i]);
```

时间复杂度：

-   二分查找边界：$O(\log n)$
-   报告结果：$O(k)$，其中 $k$ = 范围内的元素数量
    → 总计：$O(\log n + k)$

#### B. 前缀和范围查询（用于求和）

如果只需要求和（而不是实际元素），使用前缀和：

$$
\text{prefix}[i] = a_0 + a_1 + \ldots + a_i
$$

那么范围和：
$$
\text{sum}(L, R) = \text{prefix}[R] - \text{prefix}[L - 1]
$$

代码：

```c
int prefix[n];
prefix[0] = arr[0];
for (int i = 1; i < n; i++)
    prefix[i] = prefix[i - 1] + arr[i];

int range_sum(int L, int R) {
    return prefix[R] - (L > 0 ? prefix[L - 1] : 0);
}
```

时间：预处理 $O(n)$ 后，每次查询 $O(1)$。

应用场景：

-   数据库中的快速范围聚合
-   树状数组，线段树

#### C. 二维范围查询（矩形区域）

对于点 $(x, y)$，查询类似：

> "找到所有满足 $L_x ≤ x ≤ R_x$ 且 $L_y ≤ y ≤ R_y$ 的点"

使用专门的数据结构：

-   范围树（每个维度一个平衡二叉搜索树）
-   树状数组 / 线段树（用于二维数组）
-   KD 树（空间分解）

时间：二维典型为 $O(\log^2 n + k)$
空间：$O(n \log n)$

#### 2. 最近邻搜索

核心思想：
给定一组点，找到最接近查询点 $q$ 的点。
距离通常是欧几里得距离，但也可以是任何度量。

暴力法：
检查所有点 → 每次查询 $O(n)$。
对于大型数据集来说太慢。

我们需要能够快速剪枝远处区域的结构。

#### A. KD 树

KD 树 = K 维二叉树。
每一层按一个坐标轴分割点，坐标轴交替使用。
用于低维（2D-10D）中的高效最近邻搜索。

构建：

1.  选择坐标轴 = 深度 % k
2.  按坐标轴对点排序
3.  选择中位数 → 根节点
4.  递归构建左子树和右子树

查询（最近邻）：

1.  根据查询位置向下遍历树
2.  回溯，检查超球面是否与分割平面相交
3.  跟踪最佳（最近）距离

复杂度：

-   构建：$O(n \log n)$
-   查询：平均 $O(\log n)$，最坏 $O(n)$

应用场景：

-   最近城市查找
-   图像 / 特征向量匹配
-   游戏 AI 空间查询

代码框架（2D 示例）：

```c
struct Point { double x, y; };

double dist(Point a, Point b) {
    return sqrt((a.x - b.x)*(a.x - b.x) + (a.y - b.y)*(a.y - b.y));
}
```

（完整的 KD 树实现为简洁起见省略，核心思想是递归分区。）

#### B. 球树 / VP 树

对于高维数据，KD 树性能会下降。
像球树（按超球面分割）或 VP 树（优势点树）这样的替代方案表现更好。

它们基于距离度量进行分割，而不是坐标轴。

#### C. 近似最近邻

对于大规模、高维数据（例如，嵌入、向量）：

-   局部敏感哈希
-   HNSW（分层可导航小世界图）

这些方法以精确性换取速度，常见于：

-   向量数据库
-   推荐系统
-   AI 模型检索

#### 3. 总结

| 问题                          | 暴力法       | 优化方法          | 时间（查询）         | 备注                     |
| ----------------------------- | ------------ | ----------------- | -------------------- | ------------------------ |
| 一维范围查询                  | 扫描 O(n)    | 二分查找          | O(log n + k)         | 数据已排序               |
| 范围和                        | O(n)         | 前缀和            | O(1)                 | 静态数据                 |
| 二维范围查询                  | O(n)         | 范围树            | O(log² n + k)        | 空间过滤                 |
| 最近邻（低维）                | O(n)         | KD 树             | O(log n) 平均        | 精确，低维               |
| 最近邻（高维）                | O(n)         | HNSW / LSH        | ~O(1)                | 近似                     |

#### 微型代码

简单的一维范围查询：

```c
int arr[] = {1, 3, 5, 7, 9, 11};
int L = 4, R = 10;
int l = lower_bound(arr, arr + 6, L) - arr;
int r = upper_bound(arr, arr + 6, R) - arr;
for (int i = l; i < r; i++)
    printf("%d ", arr[i]); // 5 7 9
```

输出：`5 7 9`

#### 重要性

范围和最近邻查询支撑着：

-   数据库（SQL 范围过滤器，BETWEEN）
-   搜索引擎（空间索引）
-   机器学习（k-NN 分类器，向量相似度）
-   图形 / 游戏（碰撞检测，空间查询）

这些不仅仅是搜索，它们是几何查找，将算法与空间推理联系起来。

#### 动手实践

1.  编写一个函数，使用二分查找返回 `[L, R]` 中的所有数字。
2.  构建一个前缀和数组，并以 O(1) 的时间复杂度回答 5 个范围和查询。
3.  为二维点实现一个 KD 树并查询最近邻。
4.  在 1000 个随机点上比较暴力搜索与 KD 树搜索。
5.  探索 Python 的 `scipy.spatial.KDTree` 或 `sklearn.neighbors`。

这些算法将搜索与几何和分析联系起来，构成了空间计算的基础。
### 20. 搜索优化与变体

我们已经探讨了主要的搜索家族——线性搜索、二分搜索、插值搜索、指数搜索——每种都适用于不同的数据形态或约束条件。
现在让我们更进一步：为性能优化搜索，并将其适配到专门的场景中。

本节将介绍在实际系统、数据库和竞技编程中使用的实用变体和增强方法，包括跳转搜索、斐波那契搜索、三分搜索以及指数与二分搜索的组合。


#### 1. 跳转搜索

核心思想：
如果数据已排序，我们可以通过固定步长"跳转"前进，而不是线性扫描。
这就像在数组中按块跳跃——当你跳过目标时，就退回一步并在那个块内进行线性搜索。

它在线性搜索和二分搜索之间取得了平衡——比二分搜索更少的比较次数，且无需递归或对半分割。

步骤：

1. 选择跳转步长 = $\sqrt{n}$
2. 按块跳跃，直到 `arr[step] > key`
3. 在前一个块中进行线性搜索

代码：

```c
int jump_search(int arr[], int n, int key) {
    int step = sqrt(n);
    int prev = 0;

    while (arr[min(step, n) - 1] < key) {
        prev = step;
        step += sqrt(n);
        if (prev >= n) return -1;
    }

    for (int i = prev; i < min(step, n); i++) {
        if (arr[i] == key) return i;
    }
    return -1;
}
```

示例：
arr = [1, 3, 5, 7, 9, 11, 13, 15], key = 11

- step = 2- 跳转 5, 7, 9, 11 → 找到
复杂度：

- 时间：$O(\sqrt{n})$
- 空间：$O(1)$
- 适用于已排序数据

何时使用：
适用于中等大小的已排序列表，当你希望减少比较次数但又想最小化开销时。


#### 2. 斐波那契搜索

核心思想：
与二分搜索类似，但它基于斐波那契数来分割数组，而不是中点。
这使得它仅使用加法和减法（无需除法），在除法运算成本高昂的硬件上很有用。

同样，与二分搜索类似，它在每次迭代中（大致）将搜索空间减半。

步骤：

1. 找到大于等于 n 的最小斐波那契数
2. 用它来计算探测索引
3. 比较并根据结果移动区间

代码（概略）：

```c
int fibonacci_search(int arr[], int n, int key) {
    int fibMMm2 = 0; // 第 (m-2) 个斐波那契数
    int fibMMm1 = 1; // 第 (m-1) 个斐波那契数
    int fibM = fibMMm2 + fibMMm1; // 第 m 个斐波那契数

    while (fibM < n) {
        fibMMm2 = fibMMm1;
        fibMMm1 = fibM;
        fibM = fibMMm2 + fibMMm1;
    }

    int offset = -1;
    while (fibM > 1) {
        int i = min(offset + fibMMm2, n - 1);
        if (arr[i] < key) {
            fibM = fibMMm1;
            fibMMm1 = fibMMm2;
            fibMMm2 = fibM - fibMMm1;
            offset = i;
        } else if (arr[i] > key) {
            fibM = fibMMm2;
            fibMMm1 = fibMMm1 - fibMMm2;
            fibMMm2 = fibM - fibMMm1;
        } else return i;
    }
    if (fibMMm1 && arr[offset + 1] == key)
        return offset + 1;
    return -1;
}
```

复杂度：

- 时间：$O(\log n)$
- 空间：$O(1)$
- 需要已排序的输入

趣闻：
斐波那契搜索最初是为磁带驱动器设计的——在磁带驱动器中随机访问代价高昂，而可预测的跳跃很重要。


#### 3. 三分搜索

核心思想：
当函数或序列是单峰的（先严格递增后严格递减）时，你可以通过将范围分成三部分而不是两部分来定位最大值或最小值。

它不用于离散查找，而是用于在已排序函数上进行优化。

步骤：

1. 将范围分成三等份
2. 在两个中点 `m1`, `m2` 处求值
3. 根据比较结果排除三分之一的范围
4. 重复直到范围足够小

代码：

```c
double ternary_search(double low, double high, double (*f)(double)) {
    for (int i = 0; i < 100; i++) {
        double m1 = low + (high - low) / 3;
        double m2 = high - (high - low) / 3;
        if (f(m1) < f(m2))
            low = m1;
        else
            high = m2;
    }
    return (low + high) / 2;
}
```

示例：
在 [0,10] 区间内求函数 $f(x) = (x-3)^2$ 的最小值。
经过迭代后，收敛到 $x ≈ 3$。

复杂度：

- 时间：$O(\log\text{range})$
- 空间：$O(1)$
- 适用于单峰函数

应用场景：

- 数学优化
- 基于搜索的调参
- 游戏 AI 决策模型

#### 4. 二分搜索变体（回顾）

二分搜索可以定制以回答更丰富的查询：

- 下界：第一个 ≥ key 的索引
- 上界：第一个 > key 的索引
- 相等范围：所有相等元素的范围
- 旋转数组：在旋转后的有序数组中查找元素
- 无限数组：使用指数扩展

旋转数组示例：
arr = [6,7,9,1,3,4], key = 3
→ 找到枢轴点，然后在正确的一侧进行二分搜索。


#### 5. 组合搜索

实际系统通常会将算法串联使用：

- 指数搜索 + 二分搜索 → 当边界未知时
- 插值搜索 + 线性搜索 → 当接近目标时
- 跳转搜索 + 线性搜索 → 混合迭代

这些混合方法使用上下文切换——先选择一种快速搜索，然后在缩小的窗口内回退到简单的扫描。


#### 6. 总结

| 算法             | 时间复杂度     | 空间复杂度 | 数据要求       | 特殊优势         |
| ---------------- | -------------- | ---------- | -------------- | ---------------- |
| 跳转搜索         | O(√n)          | O(1)       | 已排序         | 比较次数少       |
| 斐波那契搜索     | O(log n)       | O(1)       | 已排序         | 无需除法         |
| 三分搜索         | O(log range)   | O(1)       | 单峰           | 优化             |
| 二分搜索变体     | O(log n)       | O(1)       | 已排序         | 查找边界         |
| 组合搜索         | 自适应         | O(1)       | 混合           | 实用的混合方法   |


#### 微型代码

跳转搜索直观理解：

```c
// 块大小为 sqrt(n)
[1, 3, 5, 7, 9, 11, 13, 15]
步长: 3 → 7 > 6 → 在前一个块中搜索
```

与线性扫描相比，跳转极大地减少了比较次数。


#### 为何重要

搜索优化是关于根据上下文调整结构。
你并不总是需要花哨的数据结构——有时像固定步长跳跃或斐波那契间隔这样的微调就能带来巨大的收益。

这些思想影响着：

- 数据库中的索引
- 编译器的符号解析
- 具有低级约束的嵌入式系统

它们体现了这一原则：更聪明地搜索，而不是更费力地搜索。


#### 动手尝试

1.  实现跳转搜索，并在 100 万个元素上与二分搜索进行对比测试。
2.  编写斐波那契搜索，比较所花费的步数。
3.  使用三分搜索寻找凸函数的最小值。
4.  修改二分搜索以在旋转数组中查找元素。
5.  组合跳转搜索和线性搜索，对于小的 n，它的表现如何？

理解这些变体为你提供了灵活性——这是算法掌握的核心。

## 第 3 章. 数据结构实战
### 21. 数组、链表、栈、队列

每一种数据结构都建立在一些核心基础之上，这些基础教会你数据是如何存储、访问和移动的。
在本节中，我们将重温这些基础：数组、链表、栈和队列。

它们很简单，但它们向你展示了算法中最重要的设计权衡：

- 连续存储 vs. 灵活性
- 速度 vs. 动态增长
- 后进先出 vs. 先进先出访问

#### 1. 数组

**核心思想：**
一块连续的内存块，存储相同类型的元素。
通过索引在 O(1) 时间内访问，这是它们的超能力。

**操作：**

- 访问 `arr[i]`: (O(1))
- 更新 `arr[i]`: (O(1))
- 在末尾插入: (O(1))（对于动态数组是均摊复杂度）
- 在中间插入: (O(n))
- 删除: (O(n))

**示例：**

```c
int arr[5] = {10, 20, 30, 40, 50};
printf("%d", arr[2]); // 30
```

**优点：**

- 快速随机访问
- 缓存友好（连续内存）
- 简单、可预测

**缺点：**

- 固定大小（除非使用动态数组）
- 插入/删除代价高

**动态数组：**
编程语言提供了可调整大小的数组（如 C++ 中的 `vector` 或 Java 中的 `ArrayList`），使用倍增策略：当数组满时，分配一个两倍大的新数组并复制内容。
这实现了在末尾插入的均摊 (O(1)) 复杂度。

#### 2. 链表

**核心思想：**
一个节点链，每个节点存储一个值和一个指向下一个节点的指针。
不需要连续内存。

**操作：**

- 访问: (O(n))
- 在头部插入/删除: (O(1))
- 搜索: (O(n))

**示例：**

```c
typedef struct Node {
    int data;
    struct Node* next;
} Node;

Node* head = NULL;
```

**类型：**

- 单向链表：一个指针（next）
- 双向链表：两个指针（next, prev）
- 循环链表：最后一个节点指回第一个节点

**优点：**

- 动态大小
- 快速插入/删除（无需移动元素）

**缺点：**

- 访问慢
- 需要额外内存存储指针
- 缓存局部性差

当内存碎片化或需要频繁插入/删除时，链表表现出色。

#### 3. 栈

**核心思想：**
一种后进先出（LIFO）结构，最近添加的元素最先被移除。

**应用场景：**

- 函数调用栈
- 表达式求值
- 撤销操作

**操作：**

- `push(x)`: 将元素添加到栈顶
- `pop()`: 移除栈顶元素
- `peek()`: 查看栈顶元素

**示例（基于数组的栈）：**

```c
#define MAX 100
int stack[MAX], top = -1;

void push(int x) { stack[++top] = x; }
int pop() { return stack[top--]; }
int peek() { return stack[top]; }
```

**复杂度：**
所有操作 (O(1)): push, pop, peek

**变体：**

- 基于链表的栈（无固定大小）
- 最小栈（跟踪最小值）

栈也隐式地出现在递归和回溯算法中。

#### 4. 队列

**核心思想：**
一种先进先出（FIFO）结构，最先添加的元素最先离开。

**应用场景：**

- 任务调度
- 广度优先搜索（BFS）遍历
- 生产者-消费者管道

**操作：**

- `enqueue(x)`: 添加到队尾
- `dequeue()`: 从队首移除
- `front()`: 查看队首元素

**示例（基于数组的队列）：**

```c
#define MAX 100
int queue[MAX], front = 0, rear = 0;

void enqueue(int x) { queue[rear++] = x; }
int dequeue() { return queue[front++]; }
```

这种简单实现可能会浪费空间。
循环队列通过取模 `MAX` 来循环使用索引，从而解决了这个问题：

```c
rear = (rear + 1) % MAX;
```

**复杂度：**
所有操作 (O(1)): enqueue, dequeue

**变体：**

- 双端队列（Deque）：可以从两端进行 push/pop
- 优先队列：出队优先级最高的元素（不严格遵循 FIFO）

#### 5. 对比

| 结构       | 访问   | 插入   | 删除   | 顺序       | 内存       | 备注               |
| ---------- | ------ | ------ | ------ | ---------- | ---------- | ------------------ |
| 数组       | O(1)   | O(n)   | O(n)   | 索引       | 连续       | 访问快             |
| 链表       | O(n)   | O(1)*  | O(1)*  | 顺序       | 指针       | 大小灵活           |
| 栈         | O(1)   | O(1)   | O(1)   | LIFO       | 最小       | 调用栈、语法分析   |
| 队列       | O(1)   | O(1)   | O(1)   | FIFO       | 最小       | 调度、BFS          |

（* 在头部或尾部，且有指针的情况下）

#### 微型代码

简单的栈示例：

```c
push(10);
push(20);
printf("%d", pop()); // 20
```

简单的队列示例：

```c
enqueue(5);
enqueue(8);
printf("%d", dequeue()); // 5
```

这些简短的例程几乎出现在每一个算法中，从递归栈到图遍历。

#### 为何重要

这四种结构构成了数据结构的脊梁：

- 数组教授索引和内存
- 链表教授指针和动态分配
- 栈教授递归和反转
- 队列教授调度和顺序维护

每一个复杂结构（树、堆、图）都建立在这些基础之上。

掌握它们，每一个算法都会感觉更自然。

#### 动手实践

1.  实现一个带有 `insert_front` 和 `delete_value` 操作的链表。
2.  构建一个栈并用它来反转一个数组。
3.  为轮转调度器实现一个队列。
4.  使用栈将中缀表达式转换为后缀表达式。
5.  比较在数组和链表中插入 1000 个元素所需的时间。

理解这些基础为你提供了数据结构的词汇表，即算法在内存中组织其思想的方式。
### 22. 哈希表及其变体（布谷鸟哈希、罗宾汉哈希、一致性哈希）

当你需要闪电般的查找、插入和删除速度时，很少有数据结构能与哈希表的原始效率相媲美。它们无处不在，从符号表和缓存到编译器和数据库，都依赖其平均情况 O(1) 的访问能力。

在本节中，我们将剖析哈希表的工作原理、其冲突解决策略，并探索现代变体，如布谷鸟哈希、罗宾汉哈希和一致性哈希，它们各自旨在满足不同的现实世界需求。

#### 1. 核心思想

哈希表使用一个哈希函数将键映射到值，该函数将键转换为数组中的一个索引。

$$
\text{索引} = h(\text{键}) \bmod \text{表大小}
$$

如果没有两个键哈希到同一个索引，所有操作都是 O(1)。但在实践中，冲突会发生，两个键可能映射到同一个槽，我们必须巧妙地处理它们。

#### 2. 冲突解决策略

**A. 分离链接法**
每个表槽持有一个链表（或动态数组），用于存放具有相同哈希值的条目。

优点：简单，可处理负载因子 > 1
缺点：需要额外指针，内存开销大

代码概览：

```c
typedef struct Node {
    int key, value;
    struct Node* next;
} Node;

Node* table[SIZE];

int hash(int key) { return key % SIZE; }

void insert(int key, int value) {
    int idx = hash(key);
    Node* node = malloc(sizeof(Node));
    node->key = key; node->value = value;
    node->next = table[idx];
    table[idx] = node;
}
```

**B. 开放寻址法**
所有键直接存放在表中。发生冲突时，寻找另一个槽。

三种主要策略：

- 线性探测：尝试下一个槽 (+1)
- 二次探测：步长按二次方增加
- 双重哈希：第二个哈希函数决定步长

示例（线性探测）：

```c
int hash(int key) { return key % SIZE; }
int insert(int key, int value) {
    int idx = hash(key);
    while (table[idx].used)
        idx = (idx + 1) % SIZE;
    table[idx] = (Entry){key, value, 1};
}
```

负载因子 $\alpha = \frac{n}{m}$ 影响性能，当负载因子过高时，需要重新哈希到更大的表。

#### 3. 现代变体

经典哈希表在严重冲突下性能会下降。现代变体旨在减少探测链长度并更均匀地平衡负载。

#### A. 布谷鸟哈希

思想：每个键有两个可能的位置，如果两个位置都满了，就驱逐其中一个（"踢出布谷鸟"）并重新插入。确保查找是常数时间，最多只需两次探测。

步骤：

1.  计算两个哈希值 (h_1(key)), (h_2(key))
2.  如果槽 1 为空 → 放置
3.  否则驱逐占用者，使用备用哈希值重新插入它
4.  重复直到放置成功或检测到循环（必要时重新哈希）

代码概览（概念性）：

```c
int h1(int key) { return key % SIZE; }
int h2(int key) { return (key / SIZE) % SIZE; }

void insert(int key) {
    int pos1 = h1(key);
    if (!table1[pos1]) { table1[pos1] = key; return; }
    int displaced = table1[pos1]; table1[pos1] = key;

    int pos2 = h2(displaced);
    if (!table2[pos2]) { table2[pos2] = displaced; return; }
    // 如果需要，继续驱逐
}
```

优点：

-   最坏情况 O(1) 查找（常数次探测）
-   可预测的延迟

缺点：

-   插入失败时需要重新哈希
-   逻辑更复杂

用于高性能缓存和实时系统。

#### B. 罗宾汉哈希

思想：从"更富有"（探测距离更短）的键那里"窃取"槽位，以确保公平性。插入时，如果发现某个已有键的探测距离比当前插入键的探测距离小，就交换它们，"劫富济贫"。

这平衡了探测长度，改善了方差和平均查找时间。

关键原则：
$$
\text{如果 新键探测距离} > \text{已有键探测距离} \Rightarrow \text{交换}
$$

代码概览：

```c
int insert(int key) {
    int idx = hash(key);
    int dist = 0;
    while (table[idx].used) {
        if (table[idx].dist < dist) {
            // 交换条目
            Entry tmp = table[idx];
            table[idx] = (Entry){key, dist, 1};
            key = tmp.key;
            dist = tmp.dist;
        }
        idx = (idx + 1) % SIZE;
        dist++;
    }
    table[idx] = (Entry){key, dist, 1};
}
```

优点：

-   方差减小
-   高负载下性能更好

缺点：

-   插入稍慢

用于现代语言，如 Rust (`hashbrown`) 和 Swift。

#### C. 一致性哈希

思想：
当在多个节点间分布键时，你希望在添加/移除节点时，键的移动最小化。一致性哈希将键和节点都映射到一个环形的哈希环上。

步骤：

1.  将节点哈希到环上
2.  将键哈希到同一个环上
3.  每个键属于顺时针方向的下一个节点

当一个节点被添加或移除时，只有附近的键会移动。

用于：

-   分布式缓存（Memcached, DynamoDB）
-   负载均衡
-   数据库分片

代码（概念性）：

```text
环: 0 -------------------------------- 2^32
节点: N1 在 hash("A"), N2 在 hash("B")
键: hash("User42") → 分配给顺时针方向的下一个节点
```

优点：

-   重新平衡最小化
-   可扩展

缺点：

-   设置更复杂
-   需要虚拟节点以实现均匀分布

#### 4. 复杂度概览

| 变体           | 插入      | 查找      | 删除      | 内存      | 备注                     |
| -------------- | --------- | --------- | --------- | --------- | ------------------------ |
| 分离链接法     | O(1) 平均 | O(1) 平均 | O(1) 平均 | 高        | 简单，动态               |
| 线性探测       | O(1) 平均 | O(1) 平均 | O(1) 平均 | 低        | 存在聚集风险             |
| 布谷鸟哈希     | O(1)      | O(1)      | O(1)      | 中等      | 两个表，可预测           |
| 罗宾汉哈希     | O(1)      | O(1)      | O(1)      | 低        | 平衡的探测长度           |
| 一致性哈希     | O(log n)  | O(log n)  | O(log n)  | 视情况而定 | 用于分布式键             |

#### 微型代码

使用线性探测的简单哈希表：

```c
#define SIZE 10
int keys[SIZE], values[SIZE], used[SIZE];

int hash(int key) { return key % SIZE; }

void insert(int key, int value) {
    int idx = hash(key);
    while (used[idx]) idx = (idx + 1) % SIZE;
    keys[idx] = key; values[idx] = value; used[idx] = 1;
}
```

查找：

```c
int get(int key) {
    int idx = hash(key);
    while (used[idx]) {
        if (keys[idx] == key) return values[idx];
        idx = (idx + 1) % SIZE;
    }
    return -1;
}
```

#### 为何重要

哈希表展示了结构和随机性如何结合以实现速度。它们体现了这样一个理念：一个好的哈希函数 + 聪明的冲突处理 = 接近常数的性能。

像布谷鸟哈希和罗宾汉哈希这样的变体是现代工程权衡的典范，平衡了性能、内存和可预测性。一致性哈希将这些思想扩展到了分布式系统。

#### 动手尝试

1.  实现一个使用分离链接法的哈希表，并测试其冲突处理。
2.  修改它以使用线性探测，测量探测长度。
3.  用随机插入模拟布谷鸟哈希。
4.  实现罗宾汉哈希的交换逻辑，观察公平性。
5.  绘制一个有 3 个节点和 10 个键的一致性哈希环，跟踪添加一个节点时的移动情况。

一旦你掌握了这些，你就会发现哈希无处不在，从字典到分布式数据库。
### 23. 堆（二叉堆、斐波那契堆、配对堆）

堆是一种基于优先级的数据结构，它总是能让你快速访问最重要的元素，通常是最小值或最大值。它们对于优先队列、调度、图算法（如 Dijkstra）和流式分析至关重要。

在本节中，我们将从基本的二叉堆开始，然后探索更高级的堆，如斐波那契堆和配对堆，它们在简单性、速度和摊还保证之间进行权衡。

#### 1. 堆的性质

堆是一种基于树的结构（通常用数组表示），它满足：

- 最小堆：每个节点 ≤ 其子节点
- 最大堆：每个节点 ≥ 其子节点

这确保了根节点总是保存着最小（或最大）的元素。

**完全二叉树**：
除了最后一层，所有层都被填满，最后一层从左到右填充。

示例（最小堆）：

```
        2
      /   \
     4     5
    / \   /
   9  10 15
```

这里，最小的元素 (2) 位于根节点。

#### 2. 二叉堆

**存储**：
紧凑地存储在数组中。
对于索引 `i`（从 0 开始）：

- 父节点 = `(i - 1) / 2`
- 左子节点 = `2i + 1`
- 右子节点 = `2i + 2`

**操作**：

| 操作         | 描述           | 时间复杂度   |
| ------------ | -------------- | ------------ |
| `push(x)`    | 插入元素       | (O$\log n$)  |
| `pop()`      | 移除根节点     | (O$\log n$)  |
| `peek()`     | 获取根节点     | (O(1))       |
| `heapify()`  | 建堆           | (O(n))       |

#### A. 插入（Push）

在末尾插入，然后向上冒泡直到恢复堆性质。

代码：

```c
void push(int heap[], int *n, int x) {
    int i = (*n)++;
    heap[i] = x;
    while (i > 0 && heap[(i - 1)/2] > heap[i]) {
        int tmp = heap[i];
        heap[i] = heap[(i - 1)/2];
        heap[(i - 1)/2] = tmp;
        i = (i - 1) / 2;
    }
}
```

#### B. 移除（Pop）

用最后一个元素替换根节点，然后向下冒泡（堆化）。

代码：

```c
void heapify(int heap[], int n, int i) {
    int smallest = i, l = 2*i + 1, r = 2*i + 2;
    if (l < n && heap[l] < heap[smallest]) smallest = l;
    if (r < n && heap[r] < heap[smallest]) smallest = r;
    if (smallest != i) {
        int tmp = heap[i]; heap[i] = heap[smallest]; heap[smallest] = tmp;
        heapify(heap, n, smallest);
    }
}
```

弹出：

```c
int pop(int heap[], int *n) {
    int root = heap[0];
    heap[0] = heap[--(*n)];
    heapify(heap, *n, 0);
    return root;
}
```

#### C. 建堆

从最后一个非叶子节点开始自底向上堆化：(O(n))

```c
for (int i = n/2 - 1; i >= 0; i--)
    heapify(heap, n, i);
```

#### D. 应用

- 堆排序：重复弹出最小值 (O(n log n))
- 优先队列：快速访问最小/最大值
- 图算法：Dijkstra（迪杰斯特拉）, Prim（普里姆）
- 流式处理：使用两个堆寻找中位数

#### 3. 斐波那契堆

**思想**：
一种针对执行大量 decrease-key 操作的算法（如 Dijkstra）进行优化的堆。它通过惰性合并存储一组树，提供摊还时间复杂度：

| 操作          | 摊还时间复杂度 |
| ------------- | -------------- |
| 插入          | (O(1))         |
| 查找最小值    | (O(1))         |
| 提取最小值    | (O$\log n$)    |
| 减小键值      | (O(1))         |
| 合并          | (O(1))         |

它通过将结构修复延迟到绝对必要时才进行来达成这一点（在摊还分析中使用势能法）。

**结构**：

- 一个根节点的循环链表
- 每个节点可以有多个子节点
- 在提取最小值时进行合并操作，确保最小度的重复

用于理论优化，其中渐近复杂度很重要（例如，Dijkstra 算法 (O$E + V \log V$) 对比 (O$E \log V$)）。

#### 4. 配对堆

**思想**：
斐波那契堆的一种更简单、更实用的替代方案。使用具有多个子节点的树进行自调整结构。

**操作**：

- 插入：(O(1))
- 提取最小值：摊还 (O$\log n$)
- 减小键值：摊还 (O$\log n$)

**步骤**：

- `merge` 两个堆：将一个堆作为另一个堆的子节点附加
- `extract-min`：移除根节点，将子节点两两合并，然后合并所有结果

**为什么流行**：

- 更容易实现
- 实际性能优秀
- 用于函数式编程和优先级调度器

#### 5. 比较

| 堆类型         | 插入       | 提取最小值 | 减小键值     | 合并   | 简单性     | 使用场景               |
| -------------- | ---------- | ---------- | ------------ | ------ | ---------- | ---------------------- |
| 二叉堆         | O(log n)   | O(log n)   | O(log n)     | O(n)   | 简单       | 通用                   |
| 斐波那契堆     | O(1)       | O(log n)   | O(1)         | O(1)   | 复杂       | 理论最优性             |
| 配对堆         | O(1)       | O(log n)   | O(log n)     | O(1)   | 中等       | 实用的替代方案         |

#### 微型代码

二叉堆演示：

```c
int heap[100], n = 0;
push(heap, &n, 10);
push(heap, &n, 4);
push(heap, &n, 7);
printf("%d ", pop(heap, &n)); // 4
```

输出：`4`

#### 为什么重要

堆展示了如何动态地对元素进行优先级排序。从排序到调度，它们是许多“选择最佳下一个”算法的支柱。像斐波那契堆和配对堆这样的变体展示了摊还分析如何释放更深层次的效率，这在图论和大规模优化中至关重要。

#### 亲自尝试

1.  实现一个具有 `push`、`pop` 和 `peek` 功能的二叉最小堆。
2.  使用堆对列表进行排序（堆排序）。
3.  为任务调度构建一个优先队列。
4.  研究当用堆替换数组时 Dijkstra 算法如何变化。
5.  探索斐波那契堆的伪代码，跟踪 `decrease-key` 操作。

掌握堆能让你深刻理解优先级驱动的设计，以及如何让“最佳”元素始终触手可及。
### 24. 平衡树 (AVL、红黑树、伸展树、Treap)

不平衡的树可能会退化成线性链表，将你美好的 (O$\log n$) 搜索变成令人沮丧的 (O(n)) 爬行。
平衡树解决了这个问题，它们将树高保持在对数级别，从而保证了快速的查找、插入和删除操作。

在本节中，你将学习不同的平衡哲学是如何工作的：AVL（严格平衡）、红黑树（宽松平衡）、伸展树（自调整）和 Treap（随机平衡）。

#### 1. 平衡的思想

对于一个二叉搜索树：

$$
\text{高度} = O(\log n)
$$

仅当它是平衡的时候才成立——这意味着左子树和右子树的节点数量相差不大。

不平衡的 BST（糟糕）：
```
1
 \
  2
   \
    3
```

平衡的 BST（良好）：
```
  2
 / \
1   3
```

平衡保证了高效的操作：
- `search(x)` → (O$\log n$)
- `insert(x)` → (O$\log n$)
- `delete(x)` → (O$\log n$)

#### 2. AVL 树 (Adelson-Velsky & Landis)

AVL 树发明于 1962 年，是第一个自平衡的 BST。
它强制执行严格的平衡条件：
$$
| \text{height(left)} - \text{height(right)} | \le 1
$$

每当这个条件被破坏时，通过旋转来修复它。

旋转类型：
- LL（右旋）：左-左情况的不平衡
- RR（左旋）：右-右情况的不平衡
- LR / RL：双旋转情况

代码示例（旋转）：
```c
Node* rotateRight(Node* y) {
    Node* x = y->left;
    Node* T = x->right;
    x->right = y;
    y->left = T;
    return x;
}
```

高度与平衡因子：
```c
int height(Node* n) { return n ? n->h : 0; }
int balance(Node* n) { return height(n->left) - height(n->right); }
```

特性：
- 严格的高度限制：(O$\log n$)
- 旋转次数较多（插入较慢）
- 查找速度极佳

适用于查找操作多于更新操作的场景（数据库、索引）。

#### 3. 红黑树

思想：采用稍宽松的平衡以换取更快的插入速度。
每个节点有一个颜色（红/黑），并遵循以下规则：
1. 根节点是黑色的
2. 红色节点的子节点必须是黑色的
3. 从任一节点到其每个叶子的所有路径都包含相同数量的黑色节点
4. 空节点被视为黑色

通过颜色翻转和旋转来保持平衡。

与 AVL 树对比：
- 旋转次数更少（插入/删除更快）
- 树高稍高（查找稍慢）
- 摊还平衡更简单

应用场景：
- C++ 的 `std::map`、`std::set`
- Java 的 `TreeMap`、Linux 调度器

复杂度：
所有主要操作均为 (O$\log n$)

#### 4. 伸展树

思想：通过伸展操作（一系列旋转）将最近访问的节点移动到根节点。
它能适应访问模式——你访问某个键越频繁，它就会变得越快。

伸展步骤：
- Zig：单次旋转（根节点的子节点）
- Zig-Zig：两次旋转（同侧）
- Zig-Zag：两次旋转（异侧）

代码概念示意：
```c
Node* splay(Node* root, int key) {
    if (!root || root->key == key) return root;
    if (key < root->key) {
        if (!root->left) return root;
        // 在左子树中伸展
        if (key < root->left->key)
            root->left->left = splay(root->left->left, key),
            root = rotateRight(root);
        else if (key > root->left->key)
            root->left->right = splay(root->left->right, key),
            root->left = rotateLeft(root->left);
        return rotateRight(root);
    } else {
        if (!root->right) return root;
        // 对称情况
    }
}
```

它的优点：
没有严格的平衡要求，但具有摊还的 (O$\log n$) 复杂度。
频繁访问的元素保持在靠近顶部的位置。

应用于自调整缓存、Rope 数据结构、内存分配器。

#### 5. Treap (树 + 堆)

思想：
每个节点有两个键：
- BST 键 → 维护排序属性
- 优先级 → 维护堆属性

插入 = 正常的 BST 插入 + 通过旋转修复堆属性。

平衡来自于随机化——随机的优先级保证了期望的 (O$\log n$) 高度。

代码框架：
```c
typedef struct Node {
    int key, priority;
    struct Node *left, *right;
} Node;

Node* insert(Node* root, int key) {
    if (!root) return newNode(key, rand());
    if (key < root->key) root->left = insert(root->left, key);
    else root->right = insert(root->right, key);

    if (root->left && root->left->priority > root->priority)
        root = rotateRight(root);
    if (root->right && root->right->priority > root->priority)
        root = rotateLeft(root);
    return root;
}
```

优点：
- 逻辑简单
- 随机平衡
- 期望 (O$\log n$) 复杂度

应用于随机化算法和函数式编程。

#### 6. 对比

| 树类型    | 平衡类型     | 旋转次数 | 高度                  | 插入/删除     | 查找             | 备注            |
| --------- | ------------ | -------- | --------------------- | ------------- | ---------------- | --------------- |
| AVL       | 严格         | 较多     | (O$\log n$)           | 中等          | 快速             | 查找密集型      |
| 红黑树    | 宽松         | 较少     | (O$\log n$)           | 快速          | 中等             | 标准库常用      |
| 伸展树    | 自适应       | 可变     | 摊还 (O$\log n$)      | 快速          | 快速（摊还）     | 访问模式相关    |
| Treap     | 随机化       | 平均较少 | 期望 (O$\log n$)      | 简单          | 简单             | 概率性          |

#### 微型代码

AVL 插入（框架）：
```c
Node* insert(Node* root, int key) {
    if (!root) return newNode(key);
    if (key < root->key) root->left = insert(root->left, key);
    else root->right = insert(root->right, key);
    root->h = 1 + max(height(root->left), height(root->right));
    int b = balance(root);
    if (b > 1 && key < root->left->key) return rotateRight(root);
    if (b < -1 && key > root->right->key) return rotateLeft(root);
    // 其他情况...
    return root;
}
```

#### 为什么重要

平衡树保证了在动态更新下可预测的性能。
每种变体代表了一种哲学：
- AVL：精确性
- 红黑树：实用性
- 伸展树：适应性
- Treap：随机性

它们共同传达了一个核心理念：无论进行何种操作，都要控制好树的高度。

#### 动手实践

1.  实现一个 AVL 树并可视化旋转过程。
2.  插入键值 [10, 20, 30, 40, 50] 并跟踪红黑树的颜色变化。
3.  每次访问后进行伸展操作，观察哪些键保持在顶部附近。
4.  用随机优先级构建一个 Treap，测量其平均高度。
5.  比较 BST 和 AVL 树在有序输入下的性能。

平衡树是秩序的构建者，总是通过一次旋转将混乱拒之门外。
### 25. 线段树与树状数组

当你需要快速回答区间查询（如求和、最小值、最大值）并支持对单个元素进行更新时，简单的前缀和就不再够用了。

你需要更智能的东西，即能够对区间进行分治、高效地更新和合并结果的数据结构。

这正是线段树和树状数组（二叉索引树）所做的事情：

- 在 $O(\log n)$ 时间内进行区间查询
- 在 $O(\log n)$ 时间内更新元素

它们是算法竞赛、信号处理和数据库分析的基石。

#### 1. 问题描述

给定一个数组 `A[0..n-1]`，支持以下操作：

1. `update(i, x)` → 将 `A[i]` 改为 `x`
2. `query(L, R)` → 计算 `A[L..R]` 的和（或最小值、最大值）

朴素方法：

- 更新：$O(1)$
- 查询：$O(n)$

前缀和能解决其中一个问题，但不能同时解决两个。
线段树和树状数组能同时解决这两个问题。

#### 2. 线段树

核心思想：
递归地将数组划分为多个段（区间）。
每个节点存储其对应范围的聚合值（和、最小值、最大值）。
你可以组合子节点的结果来得到任意区间的结果。

结构（求和示例）：

```
           [0,7] sum=36
         /           \
   [0,3]=10         [4,7]=26
   /     \           /      \
[0,1]=3 [2,3]=7  [4,5]=11  [6,7]=15
```

每个节点代表一个区间 [L, R]。
叶节点 = 单个元素。

#### A. 构建

递归构建：
时间复杂度：$O(n)$

```c
void build(int node, int L, int R) {
    if (L == R) tree[node] = arr[L];
    else {
        int mid = (L + R) / 2;
        build(2*node, L, mid);
        build(2*node+1, mid+1, R);
        tree[node] = tree[2*node] + tree[2*node+1];
    }
}
```

#### B. 查询（区间和）

递归查询 [l, r]：

- 如果当前区间 [L, R] 完全包含在 [l, r] 内，则返回节点值
- 如果区间不相交，则返回 0
- 否则，组合子节点的结果

```c
int query(int node, int L, int R, int l, int r) {
    if (r < L || R < l) return 0;
    if (l <= L && R <= r) return tree[node];
    int mid = (L + R) / 2;
    return query(2*node, L, mid, l, r)
         + query(2*node+1, mid+1, R, l, r);
}
```

#### C. 更新

将 `arr[i]` 改为 `x`，并更新所有覆盖 `i` 的树节点。

```c
void update(int node, int L, int R, int i, int x) {
    if (L == R) tree[node] = x;
    else {
        int mid = (L + R)/2;
        if (i <= mid) update(2*node, L, mid, i, x);
        else update(2*node+1, mid+1, R, i, x);
        tree[node] = tree[2*node] + tree[2*node+1];
    }
}
```

复杂度：

- 构建：$O(n)$
- 查询：$O(\log n)$
- 更新：$O(\log n)$
- 空间：$O(4n)$

#### D. 变体

线段树非常灵活：

- 区间最小值/最大值
- 区间最大公约数
- 惰性传播 → 区间更新
- 用于网格的二维线段树

#### 3. 树状数组（二叉索引树）

核心思想：
利用位操作存储累积频率。
每个节点覆盖的区间大小 = LSB（最低有效位）(索引)。

更简单、更小，但仅支持可结合的操作（求和、异或等）。

索引：

- 父节点：`i + (i & -i)`
- 子节点：`i - (i & -i)`

构建：
初始化为零，然后逐个添加元素。

添加 / 更新：

```c
void add(int i, int x) {
    for (; i <= n; i += i & -i)
        bit[i] += x;
}
```

前缀和：

```c
int sum(int i) {
    int res = 0;
    for (; i > 0; i -= i & -i)
        res += bit[i];
    return res;
}
```

区间和 [L, R]：
$$
\text{sum}(R) - \text{sum}(L-1)
$$

复杂度：

- 构建：$O(n \log n)$
- 查询：$O(\log n)$
- 更新：$O(\log n)$
- 空间：$O(n)$

#### 4. 对比

| 特性         | 线段树       | 树状数组     |
| ------------ | ------------ | ------------ |
| 空间         | O(4n)        | O(n)         |
| 构建         | O(n)         | O(n log n)   |
| 查询         | O(log n)     | O(log n)     |
| 更新         | O(log n)     | O(log n)     |
| 区间更新     | 支持（惰性） | 复杂         |
| 区间查询     | 灵活         | 仅求和/异或  |
| 实现难度     | 中等         | 简单         |

#### 5. 应用

- 求和 / 最小值 / 最大值 / 异或查询
- 频率计数
- 逆序对计数
- 顺序统计量
- 数组随时间更新的在线问题

应用于：

- 算法竞赛
- 数据库（对变化数据的分析）
- 时间序列查询
- 游戏（伤害/范围更新）

#### 简洁代码示例

树状数组示例：

```c
int bit[1001], n;

void update(int i, int val) {
    for (; i <= n; i += i & -i)
        bit[i] += val;
}

int query(int i) {
    int res = 0;
    for (; i > 0; i -= i & -i)
        res += bit[i];
    return res;
}

// 区间和
int range_sum(int L, int R) { return query(R) - query(L - 1); }
```

#### 为何重要

线段树和树状数组体现了对数据的分治思想，在动态更新和区间查询之间取得平衡。
它们是现代系统高效聚合实时数据的方式。

它们传授了一种强大的思维方式：

> "如果你能分解一个问题，你就能快速解决它。"

#### 动手实践

1.  构建一个用于求和查询的线段树。
2.  添加区间最小值查询（RMQ）。
3.  实现一个树状数组，并用前缀和进行测试。
4.  解决：使用树状数组计算数组中的逆序对数量。
5.  为线段树添加惰性传播以支持区间更新。

一旦你掌握了这些，区间查询将不再令你畏惧，你将以对数时间轻松解决它们。
### 26. 并查集（Union-Find）

许多问题涉及将元素分组到集合中，并高效地检查两个元素是否属于同一组，例如图中的连通分量、网络连通性、Kruskal 最小生成树算法，甚至社交网络聚类。

对于这些问题，首选的数据结构是并查集（Disjoint Set Union，简称 DSU），也称为 Union-Find。
它高效地支持两种操作：

1. `find(x)` → `x` 属于哪个集合？
2. `union(x, y)` → 合并包含 `x` 和 `y` 的集合。

通过路径压缩和按秩合并，这两种操作都可以在接近常数的时间内运行，具体为 $O(\alpha(n))$，其中 $\alpha$ 是反阿克曼函数（实际上 ≤ 4）。

#### 1. 问题描述

假设最初有 $n$ 个元素，每个元素都在独立的集合中。
随着时间的推移，你想要：

- 合并两个集合
- 检查两个元素是否属于同一集合

示例：

```text
初始集合： {1}, {2}, {3}, {4}, {5}
Union(1,2) → {1,2}, {3}, {4}, {5}
Union(3,4) → {1,2}, {3,4}, {5}
Find(2) == Find(1)? 是
Find(5) == Find(3)? 否
```

#### 2. 基础实现

每个元素都有一个父指针。
最初，每个节点都是自己的父节点。

父数组表示法：

```c
int parent[N];

void make_set(int v) {
    parent[v] = v;
}

int find(int v) {
    if (v == parent[v]) return v;
    return find(parent[v]);
}

void union_sets(int a, int b) {
    a = find(a);
    b = find(b);
    if (a != b)
        parent[b] = a;
}
```

这可以工作，但可能会形成很深的树，导致 `find` 操作变慢。
我们通过路径压缩来解决这个问题。

#### 3. 路径压缩

每次调用 `find(v)` 时，我们让路径上的所有节点都直接指向根节点。
这极大地压平了树。

优化后的 Find：

```c
int find(int v) {
    if (v == parent[v]) return v;
    return parent[v] = find(parent[v]);
}
```

这样，下次对这些节点的查找将是 $O(1)$。

#### 4. 按秩合并 / 按大小合并

合并时，总是将较小的树连接到较大的树上，以保持深度较小。

按秩合并：

```c
int parent[N], rank[N];

void make_set(int v) {
    parent[v] = v;
    rank[v] = 0;
}

void union_sets(int a, int b) {
    a = find(a);
    b = find(b);
    if (a != b) {
        if (rank[a] < rank[b])
            swap(a, b);
        parent[b] = a;
        if (rank[a] == rank[b])
            rank[a]++;
    }
}
```

按大小合并（替代方案）：
跟踪每个集合的大小，并将较小的集合附加到较大的集合上。

```c
int size[N];
void union_sets(int a, int b) {
    a = find(a);
    b = find(b);
    if (a != b) {
        if (size[a] < size[b]) swap(a, b);
        parent[b] = a;
        size[a] += size[b];
    }
}
```

#### 5. 复杂度

同时使用路径压缩和按秩合并时，
所有操作实际上都是常数时间：
$$
O(\alpha(n)) \approx O(1)
$$

对于所有实际的 $n$，$\alpha(n) \le 4$。

| 操作      | 时间复杂度 |
| --------- | ---------- |
| 创建集合  | O(1)       |
| 查找      | O(α(n))    |
| 合并      | O(α(n))    |

#### 6. 应用

- 图连通性：确定连通分量
- Kruskal 最小生成树算法：添加边，避免环
- 动态连通性
- 图像分割
- 网络聚类
- 无向图中的环检测

示例：Kruskal 算法

```c
sort(edges.begin(), edges.end());
for (edge e : edges)
    if (find(e.u) != find(e.v)) {
        union_sets(e.u, e.v);
        mst_weight += e.w;
    }
```

#### 7. 示例

```c
int parent[6], rank[6];

void init() {
    for (int i = 1; i <= 5; i++) {
        parent[i] = i;
        rank[i] = 0;
    }
}

int main() {
    init();
    union_sets(1, 2);
    union_sets(3, 4);
    union_sets(2, 3);
    printf("%d\n", find(4)); // 打印 {1,2,3,4} 的代表元素
}
```

结果：`{1,2,3,4}`, `{5}`

#### 8. 可视化

```
压缩前：
1
 \
  2
   \
    3

压缩后：
1
├─2
└─3
```

每次 `find` 调用都会使未来的查询更快。

#### 9. 对比

| 变体              | 查找      | 合并      | 备注                   |
| ----------------- | --------- | --------- | ---------------------- |
| 基础版            | O(n)      | O(n)      | 深树                   |
| 路径压缩          | O(α(n))   | O(α(n))   | 非常快                 |
| + 按秩/按大小合并 | O(α(n))   | O(α(n))   | 平衡                   |
| 持久化并查集      | O(log n)  | O(log n)  | 支持撤销/回滚          |

#### 精简代码

包含路径压缩和按秩合并的完整 DSU：

```c
int parent[1000], rank[1000];

void make_set(int v) {
    parent[v] = v;
    rank[v] = 0;
}

int find(int v) {
    if (v != parent[v])
        parent[v] = find(parent[v]);
    return parent[v];
}

void union_sets(int a, int b) {
    a = find(a);
    b = find(b);
    if (a != b) {
        if (rank[a] < rank[b]) swap(a, b);
        parent[b] = a;
        if (rank[a] == rank[b])
            rank[a]++;
    }
}
```

#### 为何重要

并查集体现了结构共享和惰性优化的思想——你不急于平衡，而是恰到好处地进行。
它是最优雅的范例之一，展示了如何通过巧妙的组织实现常数时间算法。

它传授了一个关键的算法经验：

> "只在必要时工作，并边进行边修复结构。"

#### 动手实践

1.  实现 DSU 并测试 `find`/`union`。
2.  编写一个计算连通分量数量的程序。
3.  使用 DSU 解决 Kruskal 最小生成树问题。
4.  添加 `get_size(v)` 函数以返回组件大小。
5.  尝试实现可回滚的 DSU（保留更改栈）。

并查集是许多图和连通性算法背后默默无闻的强大引擎——简单、快速且极其优雅。
### 27. 概率数据结构（布隆过滤器、Count-Min Sketch、HyperLogLog）

当你处理海量数据流时，数据量可能达到数十亿个元素，大到无法装入内存，你无法存储所有内容。
但如果你不需要*完美*的答案，只需要*快速且占用空间极小的近似*答案呢？

这就是概率数据结构大放异彩的地方。
它们牺牲一点点准确性，换来巨大的空间节省和常数时间的操作。

在本节中，我们将探讨三种最著名的概率数据结构：

-   **布隆过滤器** → 成员查询
-   **Count-Min Sketch** → 频率估计
-   **HyperLogLog** → 基数估计

它们都能高效地回答"X 可能存在吗？"或"有多少个？"这类问题，非常适合现代分析、缓存和流处理系统。

#### 1. 布隆过滤器 —— "这个元素可能在集合中吗？"

布隆过滤器回答：

> "`x` 在集合中吗？"
> 答案可能是"可能在"，或者"绝对不在"。

没有假阴性，但*存在*假阳性。

#### A. 原理

使用一个位数组（大小为 `m`），所有位初始化为 0。
使用 k 个不同的哈希函数。

插入一个元素：

1.  计算 k 个哈希值：$h_1(x), h_2(x), \ldots, h_k(x)$
2.  将每个对应的位 $b_i$ 设置为 1

查询一个元素：

1.  计算相同的 k 个哈希值
2.  如果所有位都是 1 → 可能在
3.  如果有任何位是 0 → 绝对不在

#### B. 示例

插入 `dog`：

-   $h_1(dog)=2, h_2(dog)=5, h_3(dog)=9$ 将位 2, 5, 9 设置为 1

检查 `cat`：

-   如果任何哈希对应的位 = 0 → 不存在

#### C. 复杂度

| 操作     | 时间   | 空间   | 准确性         |
| -------- | ------ | ------ | -------------- |
| 插入     | O(k)   | O(m)   | 可调           |
| 查询     | O(k)   | O(m)   | 存在假阳性     |

假阳性率 ≈ $(1 - e^{-kn/m})^k$

根据预期的元素数量 `n` 和可接受的误差来选择 `m` 和 `k`。

#### D. 代码

```c
#define M 1000
int bitset[M];

int hash1(int x) { return (x * 17) % M; }
int hash2(int x) { return (x * 31 + 7) % M; }

void add(int x) {
    bitset[hash1(x)] = 1;
    bitset[hash2(x)] = 1;
}

bool contains(int x) {
    return bitset[hash1(x)] && bitset[hash2(x)];
}
```

应用场景：

-   缓存（在磁盘查找前检查）
-   垃圾邮件过滤器
-   数据库（连接过滤）
-   区块链和对等网络

#### 2. Count-Min Sketch —— "这个出现了多少次？"

使用亚线性内存跟踪数据流中的频率计数。

它不使用完整的表，而是使用一个二维计数器数组，每一行使用不同的哈希函数进行哈希。

#### A. 插入

对于每一行 `i`：

-   计算哈希 $h_i(x)$
-   递增 `count[i][h_i(x)]++`

#### B. 查询

对于元素 `x`：

-   计算所有 $h_i(x)$
-   取所有行中 `min(count[i][h_i(x)])` → 给出真实频率的一个上界估计

#### C. 代码

```c
#define W 1000
#define D 5
int count[D][W];

int hash(int i, int x) {
    return (x * (17*i + 3)) % W;
}

void add(int x) {
    for (int i = 0; i < D; i++)
        count[i][hash(i, x)]++;
}

int query(int x) {
    int res = INT_MAX;
    for (int i = 0; i < D; i++)
        res = min(res, count[i][hash(i, x)]);
    return res;
}
```

#### D. 复杂度

| 操作     | 时间   | 空间     |
| -------- | ------ | -------- |
| 插入     | O(D)   | O(W×D)   |
| 查询     | O(D)   | O(W×D)   |

误差由以下参数控制：
$$
\varepsilon = \frac{1}{W}, \quad \delta = 1 - e^{-D}
$$

应用场景：

-   数据流中的频率计数
-   热点键检测
-   网络流量分析
-   趋势话题

#### 3. HyperLogLog —— "有多少个不同的项？"

用极小的内存（数百万数据约 1.5 KB）估计基数（不同元素的数量）。

#### A. 原理

将每个元素均匀哈希 → 得到一个 32 位值。
将哈希值分割为：

-   前缀位 → 桶索引
-   后缀位 → 计算前导零的数量

每个桶存储观察到的最大前导零数量。
最后，使用计数的调和平均数来估计不同值的数量。

#### B. 公式

$$
E = \alpha_m \cdot m^2 \cdot \Big(\sum_{i=1}^m 2^{-M[i]}\Big)^{-1}
$$

其中 `M[i]` 是桶 `i` 中的零计数，
而 $\alpha_m$ 是一个修正常数。

准确度：约 $1.04 / \sqrt{m}$

#### C. 复杂度

| 操作     | 时间   | 空间   | 误差        |
| -------- | ------ | ------ | ----------- |
| 添加     | O(1)   | O(m)   | ~1.04/√m    |
| 合并     | O(m)   | O(m)   | ,           |

应用场景：

-   网络分析（独立访客数）
-   数据库（`COUNT DISTINCT`）
-   分布式系统（可合并的估计）

#### 4. 对比

| 结构         | 用途       | 查询   | 内存       | 误差             | 备注             |
| ------------ | ---------- | ------ | ---------- | ---------------- | ---------------- |
| 布隆过滤器   | 成员查询   | O(k)   | 极小       | 假阳性           | 不支持删除       |
| Count-Min    | 频率估计   | O(D)   | 小         | 高估             | 流式计数         |
| HyperLogLog  | 基数估计   | O(1)   | 非常小     | ~1%              | 可合并           |

#### 微型代码示例

布隆过滤器演示：

```c
add(42);
add(17);
printf("%d\n", contains(42)); // 1 (可能在)
printf("%d\n", contains(99)); // 0 (绝对不在)
```

#### 为何重要

概率数据结构展示了在资源紧张时，近似算法如何战胜不可能性。
当存储所有数据变得不可能时，它们使得实时处理海量数据流成为可能。

它们揭示了一个更深刻的算法真理：

> "一点点不确定性可以为你换来一个可扩展的世界。"

#### 动手尝试

1.  实现一个带有 3 个哈希函数的布隆过滤器。
2.  测量插入 1 万个元素时的假阳性率。
3.  构建一个 Count-Min Sketch 并测试频率估计。
4.  使用 HyperLogLog 的逻辑来近似估计不同元素的数量。
5.  探索真实世界的系统：Redis（布隆过滤器/CM Sketch），PostgreSQL（HyperLogLog）。

这些微小的概率工具正是大数据变得易于处理的关键。
### 28. 跳表与 B 树

当你需要快速的查找、插入和删除操作，但又需要一个比树更容易编码，或针对磁盘和内存块进行优化的数据结构时，有两种巧妙的设计脱颖而出：

- **跳表** → 随机化的、分层的链表，其行为类似于平衡二叉搜索树。
- **B 树** → 多路树，旨在最小化磁盘 I/O 并组织大数据块。

两者都保证 (O$\log n$) 的操作复杂度，但它们在截然不同的环境中表现出色：跳表适用于内存，B 树适用于磁盘。

#### 1. 跳表

**发明者**：William Pugh (1990)
**目标**：使用带有概率性"捷径"的链表来模拟二分查找。

#### A. 核心思想

跳表是一个链表的栈，每一层都跳过更多的元素。

示例：

```
第 3 层：        ┌───────> 50 ───────┐
第 2 层：   ┌──> 10 ─────> 30 ─────> 50 ───┐
第 1 层：  5 ──> 10 ──> 20 ──> 30 ──> 40 ──> 50
```

更高的层级更稀疏，允许你"跳过"链表中的大段数据。

你从上到下进行搜索：
- 当 `下一个节点值 ≤ 目标值` 时，向右移动。
- 当无法继续前进时，向下移动一层。

这模拟了二分查找：对数层级，对数跳转次数。

#### B. 构建

每个插入的元素被赋予一个随机高度，遵循几何分布：
- 第 1 层（基础层）总是存在。
- 第 2 层以 ½ 的概率存在。
- 第 3 层以 ¼ 的概率存在，依此类推。

期望总节点数 = 2n，
期望高度 = (O$\log n$)。

#### C. 操作

| 操作   | 时间复杂度   | 空间复杂度 | 说明               |
| ------ | ------------ | ---------- | ------------------ |
| 查找   | (O$\log n$)  | O(n)       | 随机化平衡         |
| 插入   | (O$\log n$)  | O(n)       | 重建"塔"结构       |
| 删除   | (O$\log n$)  | O(n)       | 重连指针           |

查找算法：

```c
Node* search(SkipList* sl, int key) {
    Node* cur = sl->head;
    for (int lvl = sl->level; lvl >= 0; lvl--) {
        while (cur->forward[lvl] && cur->forward[lvl]->key < key)
            cur = cur->forward[lvl];
    }
    cur = cur->forward[0];
    if (cur && cur->key == key) return cur;
    return NULL;
}
```

跳表简单、快速，并且是概率平衡的——无需旋转，无需重新平衡。

#### D. 为何使用跳表？

- 比平衡树更容易实现。
- 很好地支持并发访问。
- 随机化，非确定性，但高度可靠。

应用场景：
- Redis（有序集合）
- LevelDB / RocksDB 内部
- 并发映射

#### 2. B 树

**发明者**：Rudolf Bayer & Ed McCreight (1972)
**目标**：通过将数据分组到块中来减少磁盘访问。

B 树是二叉搜索树的泛化：
- 每个节点包含多个键和子节点。
- 键保持有序。
- 子子树覆盖键之间的范围。

#### A. 结构

一棵阶为 `m` 的 B 树：
- 每个节点最多有 `m` 个子节点。
- 每个内部节点如果有 `k` 个子节点，则有 `k-1` 个键。
- 所有叶子节点在同一深度。

示例（阶为 3）：

```
        [17 | 35]
       /    |     \
 [5 10] [20 25 30] [40 45 50]
```

#### B. 操作

1.  **查找**
    - 从根节点开始遍历。
    - 在每个节点的键数组中进行二分查找。
    - 沿着合适的子节点前进。
    → 时间复杂度 (O$\log_m n$)。

2.  **插入**
    - 插入到叶子节点。
    - 如果溢出 → 分裂节点。
    - 将中间键提升到父节点。

3.  **删除**
    - 如果节点下溢（键太少），则进行借用或合并。

每次分裂或合并都保持树高最小。

#### C. 复杂度

| 操作   | 时间复杂度     | 磁盘访问次数       | 说明                 |
| ------ | -------------- | ------------------ | -------------------- |
| 查找   | (O$\log_m n$)  | (O$\log_m n$)      | m = 分支因子         |
| 插入   | (O$\log_m n$)  | (O(1)) 次分裂      | 平衡                 |
| 删除   | (O$\log_m n$)  | (O(1)) 次合并      | 平衡                 |

高度 ≈ $\log_m n$ → 当 (m) 很大时（例如 100），树非常浅。

#### D. B+ 树变体

在 B+ 树中：
- 所有数据都在叶子节点中（内部节点 = 索引）。
- 叶子节点被链接起来 → 支持高效的范围查询。

应用场景：
- 数据库（MySQL, PostgreSQL）
- 文件系统（NTFS, HFS+）
- 键值存储

#### E. 示例流程

插入 25：

```
[10 | 20 | 30] → 溢出
分裂 → [10] [30]
提升 20
根节点：[20]
```

#### 3. 对比

| 特性         | 跳表             | B 树               |
| ------------ | ---------------- | ------------------ |
| 平衡方式     | 随机化           | 确定性             |
| 扇出         | 2（链表）        | m 路               |
| 适用环境     | 内存             | 磁盘               |
| 查找         | O(log n)         | O$log_m n$         |
| 插入/删除    | O(log n)         | O$log_m n$         |
| 并发控制     | 简单             | 复杂               |
| 范围查询     | 顺序扫描         | 链接的叶子节点(B+) |

#### 微型代码示例

跳表查找（概念性）：

```c
Node* search(SkipList* list, int key) {
    Node* cur = list->head;
    for (int lvl = list->level; lvl >= 0; lvl--) {
        while (cur->next[lvl] && cur->next[lvl]->key < key)
            cur = cur->next[lvl];
    }
    cur = cur->next[0];
    return (cur && cur->key == key) ? cur : NULL;
}
```

B 树节点（框架）：

```c
#define M 4
typedef struct {
    int keys[M-1];
    Node* child[M];
    int n;
} Node;
```

#### 为何重要

跳表和 B 树展示了实现平衡的两种路径：
- 随机化的简洁性（跳表）
- 基于块的有序性（B 树）

两者都提供了对数级复杂度的保证，但一个优化了指针追逐，另一个优化了 I/O。

它们是以下领域的基础：
- 内存缓存（跳表）
- 磁盘索引（B 树，B+ 树）
- 跨系统的有序数据结构

#### 动手实践

1.  构建一个基本的跳表并插入随机键。
2.  追踪一个跨层级的搜索路径。
3.  实现 B 树的插入和分裂逻辑。
4.  比较 BST 和 B 树对于 1000 个键的高度。
5.  探索 Redis 和 MySQL 如何在内部使用这些数据结构。

总之，它们构成了链表和平衡树之间的桥梁，融合了速度、结构和可扩展性。
### 29. 持久化与函数式数据结构

大多数数据结构都是**易失的**，当你更新它们时，旧版本就会消失。
但有时，你想保留所有过去的版本，以便可以回溯时间、撤销操作或安全地进行并发读取。

这就是**持久化数据结构**的魅力所在：每次更新都会创建一个新版本，同时共享旧结构的大部分内容。

本节将介绍持久化的概念，探讨如何使数组和树等经典结构持久化，并解释函数式编程为何青睐它们。

#### 1. 什么是持久化？

持久化数据结构在更新后会保留先前的版本。
你可以访问任何版本——过去的或现在的——而不会产生副作用。

分为三个级别：

| 类型       | 描述                                           | 示例           |
| ---------- | ---------------------------------------------- | -------------- |
| 部分持久化 | 可以访问过去的版本，但只能修改最新的版本       | 撤销栈         |
| 完全持久化 | 可以访问和修改任何版本                         | 不可变映射     |
| 汇合持久化 | 可以合并不同的版本                             | 类 Git 的合并  |

这在函数式编程、撤销系统、版本控制、持久化线段树和不可变数据库中至关重要。

#### 2. 易失化 vs 持久化

易失化：

```c
arr[2] = 7; // 旧值永远丢失
```

持久化：

```c
new_arr = update(arr, 2, 7); // old_arr 仍然存在
```

持久化结构使用**结构共享**，未改变的部分被重用，而不是复制。

#### 3. 持久化链表

最简单的例子：每次更新创建一个新的头节点，重用尾部。

```c
struct Node { int val; Node* next; };

Node* push(Node* head, int x) {
    Node* newHead = malloc(sizeof(Node));
    newHead->val = x;
    newHead->next = head;
    return newHead;
}
```

现在 `old_head` 和 `new_head` 共存。
每个版本都是**不可变的**，你永远不会更改现有节点。

访问：新旧列表共享其大部分结构：

```
v0: 1 → 2 → 3
v1: 0 → 1 → 2 → 3
```

只创建了一个新节点。

#### 4. 持久化二叉树

对于树，更新会创建从根节点到被修改节点的新路径，重用其余部分。

```c
typedef struct Node {
    int key;
    struct Node *left, *right;
} Node;

Node* update(Node* root, int pos, int val) {
    if (!root) return newNode(val);
    Node* node = malloc(sizeof(Node));
    *node = *root; // 复制
    if (pos < root->key) node->left = update(root->left, pos, val);
    else node->right = update(root->right, pos, val);
    return node;
}
```

每次 `update` 都会创建一个新版本，每次更改仅创建 (O$\log n$) 个新节点。

这是竞赛编程中使用的持久化线段树的核心。

#### 5. 持久化数组（函数式技巧）

由于随机访问，数组的处理更为棘手。
解决方案：

- 使用平衡二叉树作为数组的替代品
- 每次更新替换一个节点
- 持久化向量 = 小数组构成的树（用于 Clojure, Scala）

这提供了：

- 访问： (O$\log n$)
- 更新： (O$\log n$)
- 空间：每次更新 (O$\log n$)

#### 6. 持久化线段树

用于版本化的区间查询：

- 每次更新 = 新的根节点
- 每个版本 = 历史的快照

示例：
跟踪数组随时间的变化，
查询“在版本 t 时，区间 [L,R] 的和”。

构建：

```c
Node* build(int L, int R) {
    if (L == R) return newNode(arr[L]);
    int mid = (L+R)/2;
    return newNode(
        build(L, mid),
        build(mid+1, R),
        sum
    );
}
```

更新：仅创建 (O$\log n$) 个新节点

```c
Node* update(Node* prev, int L, int R, int pos, int val) {
    if (L == R) return newNode(val);
    int mid = (L+R)/2;
    if (pos <= mid)
        return newNode(update(prev->left, L, mid, pos, val), prev->right);
    else
        return newNode(prev->left, update(prev->right, mid+1, R, pos, val));
}
```

每个版本 = 新的根节点；旧的版本仍然有效。

#### 7. 函数式视角

在函数式编程中，数据默认是不可变的。
你不是进行修改，而是创建一个新版本。

这允许：

- 线程安全（无竞争条件）
- 时间旅行调试
- 撤销/重做系统
- 无锁并发

像 Haskell、Clojure 和 Elm 这样的语言都以这种方式构建一切。

例如，Clojure 的 `persistent vector` 使用路径复制和分支因子 32，以实现 (O$\log_{32} n$) 的访问。

#### 8. 应用

- 撤销 / 重做栈（文本编辑器，IDE）
- 版本控制（Git 树）
- 不可变数据库（Datomic）
- 随时间变化的线段树（竞赛编程）
- 内存分配器或游戏中的快照

#### 9. 复杂度

| 结构                   | 更新       | 访问       | 每次更新的空间 | 注释             |
| ---------------------- | ---------- | ---------- | -------------- | ---------------- |
| 持久化链表             | O(1)       | O(1)       | O(1)           | 简单共享         |
| 持久化树               | O(log n)   | O(log n)   | O(log n)       | 路径复制         |
| 持久化数组             | O(log n)   | O(log n)   | O(log n)       | 基于树的实现     |
| 持久化线段树           | O(log n)   | O(log n)   | O(log n)       | 版本化查询       |

#### 微型代码

持久化链表示例：

```c
Node* v0 = NULL;
v0 = push(v0, 3);
v0 = push(v0, 2);
Node* v1 = push(v0, 1);
// v0 = [2,3], v1 = [1,2,3]
```

#### 为何重要

持久化将时间视为一等公民。
它让你能够：

- 回滚
- 比较版本
- 以不可变且安全的方式工作

它是函数式编程、时间旅行调试和不可变数据系统背后的算法基础。

它传授了这个强大的理念：

> “永不销毁，总是在原有基础上构建。”

#### 亲自尝试

1.  使用链表实现一个持久化栈。
2.  为区间和查询编写一个持久化线段树。
3.  跟踪每次更新后的数组版本，并查询旧状态。
4.  与易失化版本比较空间/时间开销。
5.  探索 Clojure（`conj`, `assoc`）或 Rust（`im` crate）中的持久化结构。

持久化将数据从短暂的状态转变为一段你可以导航的历史，一段关于结构和意义的时间线。
### 30. 高级树与区间查询

到目前为止，你已经了解了平衡树（AVL、红黑树、Treap）和基于区间的数据结构（线段树、树状数组）。
现在是时候结合这些思想，步入**高级树**的领域了——这些数据结构能够在对数时间内处理动态集合、顺序统计、区间、范围以及类似几何的查询。

本章介绍的树超越了简单的搜索功能，它们能存储顺序信息、追踪范围，并高效地回答复杂的查询。

我们将探讨：

- **顺序统计树**（第 k 小元素、排名查询）
- **区间树**（区间重叠查询）
- **范围树**（多维搜索）
- **KD 树**（空间划分）
- **归并排序树**（离线区间查询）

#### 1. 顺序统计树

**目标**：在 $O(\log n)$ 时间内找到第 k 小的元素，或某个元素的排名。

通过在平衡二叉搜索树（例如红黑树）的基础上，存储子树的大小来实现。

#### A. 增强的树节点

每个节点保存：

- `key`：元素值
- `left`, `right`：子节点
- `size`：子树中的节点数

```c
typedef struct Node {
    int key, size;
    struct Node *left, *right;
} Node;
```

每当进行旋转或插入操作时，更新 `size`：

```c
int get_size(Node* n) { return n ? n->size : 0; }
void update_size(Node* n) {
    if (n) n->size = get_size(n->left) + get_size(n->right) + 1;
}
```

#### B. 查找第 k 个元素

递归地利用子树大小：

```c
Node* kth(Node* root, int k) {
    int left = get_size(root->left);
    if (k == left + 1) return root;
    else if (k <= left) return kth(root->left, k);
    else return kth(root->right, k - left - 1);
}
```

时间复杂度：$O(\log n)$

#### C. 查找排名

查找一个键的位置（比它小的元素数量）：

```c
int rank(Node* root, int key) {
    if (!root) return 0;
    if (key < root->key) return rank(root->left, key);
    if (key > root->key) return get_size(root->left) + 1 + rank(root->right, key);
    return get_size(root->left) + 1;
}
```

**应用场景**：

- 数据库（`ORDER BY`、分页）
- 分位数查询
- 在线中位数维护

#### 2. 区间树

**目标**：查找所有与给定点或区间重叠的区间。

应用于计算几何、调度和基因组数据。

#### A. 结构

以区间左端点为键构建二叉搜索树。
每个节点存储：

- `low`, `high`：区间边界
- `max`：其子树中最大的 `high` 值

```c
typedef struct {
    int low, high, max;
    struct Node *left, *right;
} Node;
```

#### B. 重叠查询

检查 `x` 是否与 `node->interval` 重叠：
如果不重叠，则根据 `max` 值决定向左或向右子树搜索。

```c
bool overlap(Interval a, Interval b) {
    return a.low <= b.high && b.low <= a.high;
}

Node* overlap_search(Node* root, Interval q) {
    if (!root) return NULL;
    if (overlap(root->interval, q)) return root;
    if (root->left && root->left->max >= q.low)
        return overlap_search(root->left, q);
    return overlap_search(root->right, q);
}
```

时间复杂度：平均 $O(\log n)$

#### C. 应用场景

- 日历/日程冲突检测
- 碰撞检测
- 基因组区域查找
- 线段相交

#### 3. 范围树

**目标**：处理多维查询，例如

> "有多少个点落在矩形 [x1, x2] × [y1, y2] 内？"

**结构**：

- 主树是基于 x 坐标的二叉搜索树
- 每个节点存储一个基于 y 坐标的辅助二叉搜索树

查询时间：$O(\log^2 n)$
空间复杂度：$O(n \log n)$

**应用场景**：

- 二维搜索
- 计算几何
- 数据库（空间连接）

#### 4. KD 树

**目标**：在 k 维空间中高效搜索点。

在树的每一层交替选择分割维度：

- 第 0 层 → 按 x 分割
- 第 1 层 → 按 y 分割
- 第 2 层 → 按 z 分割

每个节点存储：

- 点（向量）
- 分割轴

**用于**：

- 最近邻搜索
- 范围查询
- 机器学习（k-NN 分类器）

**时间复杂度**：

- 构建：$O(n \log n)$
- 查询：在二维空间中平均 $O(\sqrt{n})$

#### 5. 归并排序树

**目标**：查询“在区间 [L, R] 内，有多少个元素 ≤ k”

构建方式类似于线段树，但每个节点存储其对应区间的有序列表。

构建：合并子节点的列表
查询：在节点列表中进行二分查找

**时间复杂度**：

- 构建：$O(n \log n)$
- 查询：$O(\log^2 n)$

用于离线查询和区间上的顺序统计。

#### 6. 对比

| 树类型         | 应用场景     | 查询时间       | 更新时间   | 备注                       |
| -------------- | ------------ | -------------- | ---------- | -------------------------- |
| 顺序统计树     | 第 k 小，排名 | O(log n)      | O(log n)  | 增强的二叉搜索树           |
| 区间树         | 重叠查询     | O(log n + k)  | O(log n)  | 存储区间                   |
| 范围树         | 二维范围查询 | O(log² n + k) | O(log² n) | 多维                       |
| KD 树          | 空间查询     | O(√n) 平均    | O(log n)  | 最近邻搜索                 |
| 归并排序树     | 离线排名查询 | O(log² n)     | 静态       | 由有序分段构建             |

#### 微型代码示例

顺序统计树示例：

```c
Node* root = NULL;
root = insert(root, 10);
root = insert(root, 20);
root = insert(root, 30);
printf("%d", kth(root, 2)->key); // 20
```

区间查询示例：

```c
Interval q = {15, 17};
Node* res = overlap_search(root, q);
if (res) printf("Overlap: [%d, %d]\n", res->low, res->high);
```

#### 为什么重要

这些树将平衡思想扩展到了维度和区间领域。
它们让你能够高效地查询有序数据：“有多少个？”、“哪些重叠？”、“第 k 小的在哪里？”。

它们揭示了一个更深层次的设计原则：

> “用知识增强结构——平衡性加上元数据等于强大的能力。”

#### 动手实践

1.  实现一个顺序统计树，测试排名/第 k 小查询。
2.  插入区间并测试重叠检测。
3.  为二维点构建一个简单的 KD 树。
4.  使用范围树解决矩形计数问题。
5.  为离线查询预计算一个归并排序树。

这些高级树代表了结构化查询的最终演进，融合了几何、顺序和对数级的精度。

## 第 4 章. 图算法
### 31. 图遍历（DFS、BFS、迭代加深）

图无处不在：地图、网络、依赖关系、状态空间。
在分析它们之前，你需要一种系统性地访问其顶点的方法，以免迷失方向或陷入无限循环。

这就是图遍历的用武之地。
它们是后续一切内容的基础：连通分量、最短路径、生成树、拓扑排序等等。

本节将介绍三大支柱：

- DFS（深度优先搜索）：在回溯之前深入探索
- BFS（广度优先搜索）：逐层探索
- 迭代加深：一种内存友好的混合方法

#### 1. 图的表示

在遍历之前，你需要一个好的数据结构。

邻接表（最常见）：

```c
#define MAX 1000
vector<int> adj[MAX];
```

添加边：

```c
void add_edge(int u, int v) {
    adj[u].push_back(v);
    adj[v].push_back(u); // 如果是无向图，则省略此行
}
```

记录已访问顶点：

```c
bool visited[MAX];
```

#### 2. 深度优先搜索（DFS）

DFS 深入探索，在探索其他分支之前完全探索一个分支。
它是递归的，就像探索迷宫时总是向左转，直到撞到墙。

#### A. 递归形式

```c
void dfs(int u) {
    visited[u] = true;
    for (int v : adj[u]) {
        if (!visited[v])
            dfs(v);
    }
}
```

启动它：

```c
dfs(start_node);
```

#### B. 迭代形式（使用栈）

```c
void dfs_iter(int start) {
    stack<int> s;
    s.push(start);
    while (!s.empty()) {
        int u = s.top(); s.pop();
        if (visited[u]) continue;
        visited[u] = true;
        for (int v : adj[u]) s.push(v);
    }
}
```

#### C. 复杂度

| 图类型         | 时间复杂度 | 空间复杂度 |
| -------------- | ---------- | ---------- |
| 邻接表         | O(V + E)   | O(V)       |

DFS 用于：

- 连通分量
- 环检测
- 拓扑排序
- 回溯与搜索
- 关节点/桥

#### 3. 广度优先搜索（BFS）

BFS 优先探索邻居，就像波浪一样扩展。
这保证了在无权图中找到最短路径。

#### A. 使用队列的 BFS

```c
void bfs(int start) {
    queue<int> q;
    q.push(start);
    visited[start] = true;
    while (!q.empty()) {
        int u = q.front(); q.pop();
        for (int v : adj[u]) {
            if (!visited[v]) {
                visited[v] = true;
                q.push(v);
            }
        }
    }
}
```

#### B. 跟踪距离

```c
int dist[MAX];
void bfs_dist(int s) {
    fill(dist, dist + MAX, -1);
    dist[s] = 0;
    queue<int> q; q.push(s);
    while (!q.empty()) {
        int u = q.front(); q.pop();
        for (int v : adj[u]) {
            if (dist[v] == -1) {
                dist[v] = dist[u] + 1;
                q.push(v);
            }
        }
    }
}
```

现在 `dist[v]` 给出了从 `s` 到 `v` 的最短距离。

#### C. 复杂度

与 DFS 相同：

| 时间复杂度 | 空间复杂度 |
| ---------- | ---------- |
| O(V + E)   | O(V)       |

用于：

- 最短路径（无权图）
- 层序遍历
- 二分图检测
- 连通分量

#### 4. 迭代加深搜索（IDS）

DFS 内存占用小，但可能探索过深。
BFS 是最优的，但可能占用大量内存。
迭代加深搜索（IDS）结合了两者的优点。

它执行深度限制逐渐增加的 DFS：

```c
bool dls(int u, int target, int depth) {
    if (u == target) return true;
    if (depth == 0) return false;
    for (int v : adj[u])
        if (dls(v, target, depth - 1)) return true;
    return false;
}

bool ids(int start, int target, int max_depth) {
    for (int d = 0; d <= max_depth; d++)
        if (dls(start, target, d)) return true;
    return false;
}
```

用于：

- AI 搜索问题（状态空间）
- 博弈树（国际象棋、谜题）

#### 5. 遍历顺序示例

对于下图：

```
1 - 2 - 3
|   |
4 - 5
```

DFS（从 1 开始）：1 → 2 → 3 → 5 → 4
BFS（从 1 开始）：1 → 2 → 4 → 3 → 5

#### 6. 有向图与无向图

- 无向图：标记两个方向
- 有向图：仅遵循边的方向

在有向图上进行 DFS 是以下内容的核心：

- 强连通分量（SCC）
- 拓扑排序
- 可达性分析

#### 7. 遍历树

每次遍历都隐式地构建了一棵生成树：

- DFS 树：基于递归
- BFS 树：基于层级

使用它们来：

- 检测交叉边、后向边
- 对边进行分类（对于像 Tarjan 这样的算法很重要）

#### 8. 对比

| 方面           | DFS                             | BFS                          |
| -------------- | ------------------------------- | ---------------------------- |
| 策略           | 深度优先                        | 逐层                         |
| 空间           | O(V)（栈）                      | O(V)（队列）                 |
| 路径最优性     | 不保证                          | 是（无权图）                 |
| 应用           | 环检测、回溯                    | 最短路径、层序遍历           |

#### 微型代码

DFS + BFS 组合：

```c
void traverse(int n) {
    for (int i = 1; i <= n; i++) visited[i] = false;
    dfs(1);
    for (int i = 1; i <= n; i++) visited[i] = false;
    bfs(1);
}
```

#### 为何重要

DFS 和 BFS 是图论在实际应用中的根基。
你以后遇到的每一个算法——最短路径、流、强连通分量——都建立在它们之上。

它们教会你如何导航结构，如何系统地探索未知，以及搜索如何位于计算的核心。

#### 亲自尝试

1.  构建一个邻接表并从顶点 1 运行 DFS/BFS。
2.  在 DFS 中跟踪发现时间和完成时间。
3.  使用 BFS 计算无权图中的最短路径。
4.  修改 DFS 以计算连通分量的数量。
5.  为像八数码这样的谜题实现 IDS。

图遍历是探索的艺术——一旦你掌握了它，图论的其余部分就会水到渠成。
### 32. 强连通分量（Tarjan, Kosaraju）

在有向图中，边具有方向，因此连通性变得复杂。顶点之间仅仅**可达**是不够的，你需要**相互可达**。

这就是**强连通分量**（SCC）的本质：

> 一个顶点集合，其中每个顶点都能到达集合中的其他任何顶点。

可以把 SCC 想象成相互连通的岛屿：在岛屿内部，你可以去任何地方；但无法离开岛屿。它们是简化有向图为**凝聚 DAG**（无环）的基本构件。

我们将探讨两种经典算法：

- **Kosaraju 算法**：清晰、直观，需要两次遍历。
- **Tarjan 算法**：一次遍历，基于栈的优雅实现。

#### 1. 定义

在有向图 $G = (V, E)$ 中，一个**强连通分量**（SCC）是顶点的一个极大子集 $C \subseteq V$，使得对于每一对顶点 $(u, v) \in C$：
$u \to v$ 且 $v \to u$。

换句话说，$C$ 中的每个节点都可以从 $C$ 中的任何其他节点到达。

示例：

```
1 → 2 → 3 → 1   形成一个 SCC
4 → 5           独立的 SCC
```

#### 2. 应用

- **凝聚 DAG**：将 SCC 压缩为单个节点，图中不再有环。
- **基于分量的推理**：在 SCC 构成的 DAG 上进行拓扑排序。
- **程序分析**：检测循环、依赖关系。
- **网络图**：查找相互链接的页面集群。
- **控制流**：循环和强连通的子程序。

#### 3. Kosaraju 算法

一种使用 DFS 和反转图的简单两遍算法。

步骤：

1.  运行 DFS，按照完成时间的顺序将节点压入栈。
2.  反转图（翻转边的方向）。
3.  从栈中弹出节点；在反转图上进行 DFS；每次 DFS 对应一个 SCC。

#### A. 实现

```c
vector<int> adj[MAX], rev[MAX];
bool visited[MAX];
stack<int> st;
vector<vector<int>> sccs;

void dfs1(int u) {
    visited[u] = true;
    for (int v : adj[u])
        if (!visited[v])
            dfs1(v);
    st.push(u);
}

void dfs2(int u, vector<int>& comp) {
    visited[u] = true;
    comp.push_back(u);
    for (int v : rev[u])
        if (!visited[v])
            dfs2(v, comp);
}

void kosaraju(int n) {
    // 第一遍：按完成时间排序
    for (int i = 1; i <= n; i++)
        if (!visited[i]) dfs1(i);

    // 反转图
    for (int u = 1; u <= n; u++)
        for (int v : adj[u])
            rev[v].push_back(u);

    // 第二遍：收集 SCC
    fill(visited, visited + n + 1, false);
    while (!st.empty()) {
        int u = st.top(); st.pop();
        if (!visited[u]) {
            vector<int> comp;
            dfs2(u, comp);
            sccs.push_back(comp);
        }
    }
}
```

时间复杂度：
$O(V + E)$，两次 DFS 遍历。

空间复杂度：
$O(V + E)$

#### B. 示例

图：

```
1 → 2 → 3
↑   ↓   ↓
5 ← 4 ← 6
```

SCC：

- {1,2,4,5}
- {3,6}

#### 4. Tarjan 算法

更优雅：一次 DFS 遍历，无需反转图，基于栈。它使用发现时间和低链接值来检测 SCC 的根。

#### A. 思想

- `disc[u]`：节点 `u` 的发现时间。
- `low[u]`：从 `u` 可达的最小发现时间。
- 如果 `disc[u] == low[u]`，则节点 `u` 是一个 SCC 的根。

维护一个活动节点栈（在当前 DFS 路径中）。

#### B. 实现

```c
vector<int> adj[MAX];
int disc[MAX], low[MAX], timer;
bool inStack[MAX];
stack<int> st;
vector<vector<int>> sccs;

void dfs_tarjan(int u) {
    disc[u] = low[u] = ++timer;
    st.push(u);
    inStack[u] = true;

    for (int v : adj[u]) {
        if (!disc[v]) {
            dfs_tarjan(v);
            low[u] = min(low[u], low[v]);
        } else if (inStack[v]) {
            low[u] = min(low[u], disc[v]);
        }
    }

    if (disc[u] == low[u]) {
        vector<int> comp;
        while (true) {
            int v = st.top(); st.pop();
            inStack[v] = false;
            comp.push_back(v);
            if (v == u) break;
        }
        sccs.push_back(comp);
    }
}

void tarjan(int n) {
    for (int i = 1; i <= n; i++)
        if (!disc[i])
            dfs_tarjan(i);
}
```

时间复杂度：
$O(V + E)$

空间复杂度：
$O(V)$

#### C. 流程演示

图：

```
1 → 2 → 3
↑   ↓   ↓
5 ← 4 ← 6
```

DFS 按顺序访问节点；当它找到一个 `disc == low` 的节点时，就从栈中弹出节点以形成一个 SCC。

结果：

```
SCC1: 1 2 4 5
SCC2: 3 6
```

#### 5. 比较

| 特性         | Kosaraju            | Tarjan             |
| ------------ | ------------------- | ------------------ |
| DFS 遍历次数 | 2                   | 1                  |
| 需要反转图   | 是                  | 否                 |
| 栈           | 是（完成顺序）      | 是（活动路径）     |
| 实现         | 概念简单            | 紧凑、高效         |
| 时间复杂度   | O(V + E)            | O(V + E)           |

#### 6. 凝聚图

一旦找到 SCC，你就可以构建一个 DAG：每个 SCC 变成一个节点，边代表跨 SCC 的连接。现在可以应用拓扑排序。

用于：

- 依赖分析
- 强分量压缩
- DAG 动态规划

#### 精简代码

打印 SCC（Tarjan）：

```c
tarjan(n);
for (auto &comp : sccs) {
    for (int x : comp) printf("%d ", x);
    printf("\n");
}
```

#### 为何重要

SCC 算法将混乱的有向图转化为结构化的 DAG。它们是理解循环、依赖关系和模块化的关键。

理解它们揭示了一个强大的真理：

> "一旦你找到了图的强连通核心，每个复杂的图都可以被简化为一个简单的层次结构。"

#### 动手实践

1.  实现 Kosaraju 和 Tarjan 算法，验证它们的结果是否一致。
2.  构建 SCC DAG 并在其上运行拓扑排序。
3.  通过 SCC 大小 > 1 来检测循环。
4.  使用 SCC 解决 2-SAT（布尔可满足性）问题。
5.  可视化一个包含 6 个节点的图的凝聚过程。

一旦你能找到 SCC，你就能驾驭有向性，将混乱的网络转化为有序的系统。
### 33. 最短路径算法（Dijkstra、Bellman-Ford、A*、Johnson）

一旦你能遍历一个图，下一个自然而然的问题是：

> “两个顶点之间的最短路径是什么？”

最短路径算法是路由、导航、规划和优化的核心。
它们计算最小成本路径，无论是距离、时间还是权重，并能适应不同的边条件（非负、负权、启发式）。

本节涵盖最核心的算法：

- Dijkstra（迪杰斯特拉）算法，适用于非负权重的图
- Bellman-Ford（贝尔曼-福特）算法，能处理负权边
- A* 算法，使用启发式的最佳优先搜索
- Johnson（约翰逊）算法，用于稀疏图的所有点对最短路径

#### 1. 最短路径问题

给定一个加权图 ( G = (V, E) ) 和一个源点 ( s )，
找到 $\text{dist}[v]$，即到达每个顶点 ( v ) 的最小总权重。

变体：

- 单源最短路径，从一个源点到所有其他顶点
- 单对最短路径，从一个源点到一个目标点
- 所有点对最短路径，每一对顶点之间
- 动态最短路径，支持更新操作

#### 2. Dijkstra 算法

最适合非负权重。
思想：按距离递增的顺序探索顶点，就像水波扩散。

#### A. 步骤

1. 将所有距离初始化为无穷大。
2. 将源点距离设为 0。
3. 使用优先队列始终选取具有最小暂定距离的节点。
4. 松弛所有出边。

#### B. 实现（邻接表）

```c
#include <bits/stdc++.h>
using namespace std;

const int INF = 1e9;
vector<pair<int,int>> adj[1000]; // (邻居, 权重)
int dist[1000];

void dijkstra(int n, int s) {
    fill(dist, dist + n + 1, INF);
    dist[s] = 0;
    priority_queue<pair<int,int>, vector<pair<int,int>>, greater<>> pq;
    pq.push({0, s});

    while (!pq.empty()) {
        auto [d, u] = pq.top(); pq.pop();
        if (d != dist[u]) continue;
        for (auto [v, w] : adj[u]) {
            if (dist[v] > dist[u] + w) {
                dist[v] = dist[u] + w;
                pq.push({dist[v], v});
            }
        }
    }
}
```

复杂度：

- 使用优先队列（二叉堆）：$O((V + E)\log V)$
- 空间：$O(V + E)$

#### C. 示例

图：

```
1 →(2) 2 →(3) 3
↓(4)       ↑(1)
4 →(2)─────┘
```

`dijkstra(1)` 给出最短距离：

```
dist[1] = 0  
dist[2] = 2  
dist[3] = 5  
dist[4] = 4
```

#### D. 特性

- 仅当所有边权重 $w \ge 0$ 时有效
- 可以通过 `parent[v]` 重建路径
- 用于：
  - GPS 和路由系统
  - 网络优化
  - 具有正成本的调度

#### 3. Bellman-Ford 算法

处理负权边，并能检测负权环。

#### A. 思想

对所有边进行 (V-1) 次松弛。
如果在第 (V) 次迭代中仍能松弛 → 存在负权环。

#### B. 实现

```c
struct Edge { int u, v, w; };
vector<Edge> edges;
int dist[1000];

bool bellman_ford(int n, int s) {
    fill(dist, dist + n + 1, INF);
    dist[s] = 0;
    for (int i = 1; i <= n - 1; i++) {
        for (auto e : edges) {
            if (dist[e.u] + e.w < dist[e.v])
                dist[e.v] = dist[e.u] + e.w;
        }
    }
    // 检查负权环
    for (auto e : edges)
        if (dist[e.u] + e.w < dist[e.v])
            return false; // 存在负权环
    return true;
}
```

复杂度：$O(VE)$
即使在 $w < 0$ 时也有效。

#### C. 示例

图：

```
1 →(2) 2 →(-5) 3 →(2) 4
```

Bellman-Ford 找到路径 1→2→3→4，总成本为 (-1)。

如果存在一个环能无限降低总权重，算法会检测到它。

#### D. 使用场景

- 货币套利交易
- 带有惩罚的游戏图
- 检测不可能的约束条件

#### 4. A* 搜索算法

启发式引导的最短路径，非常适合寻路（AI、地图、游戏）。

它结合了实际成本和估计成本：
$$
f(v) = g(v) + h(v)
$$
其中

- $g(v)$：已知的当前成本
- $h(v)$：启发式估计值（必须可采纳）

#### A. 伪代码

```c
priority_queue<pair<int,int>, vector<pair<int,int>>, greater<>> pq;
g[start] = 0;
pq.push({h[start], start});

while (!pq.empty()) {
    auto [f, u] = pq.top(); pq.pop();
    if (u == goal) break;
    for (auto [v, w] : adj[u]) {
        int new_g = g[u] + w;
        if (new_g < g[v]) {
            g[v] = new_g;
            pq.push({g[v] + h[v], v});
        }
    }
}
```

启发式示例：

- 欧几里得距离（用于网格）
- 曼哈顿距离（用于四方向移动）

#### B. 使用场景

- 游戏 AI（寻路）
- 机器人运动规划
- 地图导航

复杂度：最佳情况下 $O(E)$，取决于启发式质量。

#### 5. Johnson 算法

目标：在稀疏图（无负权环）中计算所有点对最短路径，允许负权边。

思想：

1. 添加一个新顶点 `q`，以权重 0 的边连接到所有其他顶点
2. 从 `q` 运行 Bellman-Ford 以得到势能 `h(v)`
3. 重新加权边：
   $w'(u, v) = w(u, v) + h(u) - h(v)$
   （现在所有权重 ≥ 0）
4. 从每个顶点运行 Dijkstra 算法

复杂度：$O(VE + V^2 \log V)$

#### 6. 总结

| 算法         | 处理负权 | 检测负权环 | 启发式 | 复杂度           | 使用场景             |
| ------------ | -------- | ---------- | ------ | ---------------- | -------------------- |
| Dijkstra     | 否       | 否         | 否     | O((V+E) log V)   | 非负权重             |
| Bellman-Ford | 是       | 是         | 否     | O(VE)            | 负权边               |
| A*           | 否（除非特别处理） | 否 | 是     | 取决于启发式     | 寻路                 |
| Johnson      | 是（无负权环） | 是 | 否     | O(VE + V² log V) | 所有点对，稀疏图     |

#### 微型代码

Dijkstra 示例：

```c
dijkstra(n, 1);
for (int i = 1; i <= n; i++)
    printf("dist[%d] = %d\n", i, dist[i]);
```

#### 为何重要

最短路径是优化的精髓，不仅在图论中，更在于推理：寻找最小成本、最小距离、最小风险。

这些算法教导我们：

> “通往目标的路径并非随机，它由结构、权重和知识所引导。”

#### 亲自尝试

1.  构建一个加权图，比较 Dijkstra 和 Bellman-Ford。
2.  引入一条负权边，观察 Bellman-Ford 如何检测它。
3.  在有障碍物的网格上实现 A*。
4.  使用 Dijkstra 在城市地图数据集中规划路线。
5.  尝试使用 Johnson 算法计算所有点对最短路径。

掌握这些，你就掌握了方向 + 成本 = 运动中的智能。
### 34. 最短路径变体（0-1 BFS、双向搜索、启发式 A*）

有时经典的最短路径算法还不够用。
你可能遇到特殊的边权重（仅为 0 或 1）、需要更快的搜索速度，或者可以利用额外的结构。

这就是最短路径变体的用武之地，
它们是三大算法（BFS、Dijkstra、A*）针对特定场景的优化改编。

在本节中，我们将探讨：

- **0-1 BFS** → 当边权重仅为 0 或 1 时
- **双向搜索** → 为速度而进行的"中间相遇"搜索
- **启发式 A*** → 由估计值引导的更智能探索

每种方法都展示了如何利用问题中的结构来获得速度提升。

#### 1. 0-1 BFS

如果所有边的权重都仅为 0 或 1，你就不需要优先队列。
一个双端队列就足以在 (O(V + E)) 时间内完成。

为什么？
因为权重为 0 的边应该被立即处理，
而权重为 1 的边可以多等待一步。

#### A. 算法

使用一个双端队列。

- 当松弛权重为 0 的边时，将节点推入队列前端。
- 当松弛权重为 1 的边时，将节点推入队列后端。

```c
const int INF = 1e9;
vector<pair<int,int>> adj[1000]; // (v, w)
int dist[1000];

void zero_one_bfs(int n, int s) {
    fill(dist, dist + n + 1, INF);
    deque<int> dq;
    dist[s] = 0;
    dq.push_front(s);

    while (!dq.empty()) {
        int u = dq.front(); dq.pop_front();
        for (auto [v, w] : adj[u]) {
            if (dist[v] > dist[u] + w) {
                dist[v] = dist[u] + w;
                if (w == 0) dq.push_front(v);
                else dq.push_back(v);
            }
        }
    }
}
```

#### B. 示例

图：

```
1 -0-> 2 -1-> 3  
|              ^  
1              |  
+--------------+
```

从 1 到 3 的最短路径 = 1（通过边 1-2-3）。
双端队列确保权重为 0 的边不会被延迟处理。

#### C. 复杂度

| 时间复杂度 | 空间复杂度 | 备注                       |
| ---------- | ---------- | -------------------------- |
| O(V + E)   | O(V)       | 对二元权重是最优的         |

应用场景：

- 分层 BFS
- 具有二元成本的网格问题
- 带有传送（权重为 0 的边）的 BFS

#### 2. 双向搜索

有时，在一个无权图中，你只需要一条从源点到目标点的路径。
与其从一端扩展，不如从两端同时扩展，并在它们相遇时停止。

这将搜索深度从 (O$b^d$) 减少到 (O$b^{d/2}$)（对于大型图来说增益巨大）。

#### A. 思路

同时从源点和目标点运行 BFS。
当它们的搜索前沿相交时，你就找到了最短路径。

#### B. 实现

```c
bool visited_from_s[MAX], visited_from_t[MAX];
queue<int> qs, qt;

int bidirectional_bfs(int s, int t) {
    qs.push(s); visited_from_s[s] = true;
    qt.push(t); visited_from_t[t] = true;

    while (!qs.empty() && !qt.empty()) {
        if (step(qs, visited_from_s, visited_from_t)) return 1;
        if (step(qt, visited_from_t, visited_from_s)) return 1;
    }
    return 0;
}

bool step(queue<int>& q, bool vis[], bool other[]) {
    int size = q.size();
    while (size--) {
        int u = q.front(); q.pop();
        if (other[u]) return true;
        for (int v : adj[u]) {
            if (!vis[v]) {
                vis[v] = true;
                q.push(v);
            }
        }
    }
    return false;
}
```

#### C. 复杂度

| 时间复杂度   | 空间复杂度     | 备注                   |
| ------------ | -------------- | ---------------------- |
| O$b^{d/2}$   | O$b^{d/2}$     | 实践中速度翻倍         |

应用场景：

- 迷宫求解器
- 大型稀疏图中的最短路径
- 社交网络中的"分离度"

#### 3. 启发式 A*（重温）

A* 通过使用启发式函数进行目标导向搜索，推广了 Dijkstra 算法。
我们在此重温，以展示启发式函数如何改变探索顺序。

#### A. 代价函数

$$
f(v) = g(v) + h(v)
$$

- (g(v))：当前已花费的代价
- (h(v))：到目标的估计代价
- (h(v)) 必须是可采纳的（即 (h(v) \le \text{真实代价})）

#### B. 实现

```c
struct Node {
    int v; int f, g;
    bool operator>(const Node& o) const { return f > o.f; }
};

priority_queue<Node, vector<Node>, greater<Node>> pq;

void astar(int start, int goal) {
    g[start] = 0;
    h[start] = heuristic(start, goal);
    pq.push({start, g[start] + h[start], g[start]});

    while (!pq.empty()) {
        auto [u, f_u, g_u] = pq.top(); pq.pop();
        if (u == goal) break;
        for (auto [v, w] : adj[u]) {
            int new_g = g[u] + w;
            if (new_g < g[v]) {
                g[v] = new_g;
                int f_v = new_g + heuristic(v, goal);
                pq.push({v, f_v, new_g});
            }
        }
    }
}
```

#### C. 启发式函数示例

- 网格地图：曼哈顿距离 (h(x, y) = |x - x_g| + |y - y_g|)
- 导航：直线距离（欧几里得距离）
- 游戏树：评估函数

#### D. 性能

| 启发式函数类型         | 效果                         |
| ---------------------- | ---------------------------- |
| 完美（h = 真实代价）   | 最优，访问的节点最少         |
| 可采纳但较弱           | 仍然正确，但访问更多节点     |
| 高估                   | 可能失败（不可采纳）         |

#### 4. 对比

| 算法           | 权重类型      | 策略           | 时间复杂度   | 空间复杂度     | 备注               |
| -------------- | ------------- | -------------- | ------------ | -------------- | ------------------ |
| 0-1 BFS        | 0 或 1        | 基于双端队列   | O(V+E)       | O(V)           | 无需堆             |
| 双向 BFS       | 无权          | 双向搜索       | O$b^{d/2}$   | O$b^{d/2}$     | 中间相遇           |
| A*             | 非负          | 启发式搜索     | 取决于启发式 | O(V)           | 有引导             |

#### 5. 示例场景

| 问题描述                             | 适用变体                     |
| ------------------------------------ | ---------------------------- |
| 带有传送（代价为 0）的网格           | 0-1 BFS                      |
| 巨大的社交图（寻找最短链）           | 双向 BFS                     |
| 游戏 AI 寻路                         | 使用曼哈顿启发式的 A*        |

#### 微型代码

0-1 BFS 快速演示：

```c
add_edge(1, 2, 0);
add_edge(2, 3, 1);
zero_one_bfs(3, 1);
printf("%d\n", dist[3]); // 最短路径 = 1
```

#### 为何重要

特殊情况需要特殊工具。
这些变体表明，理解结构（如边权重或对称性）可以带来巨大的收益。

它们体现了一个原则：

> "不要只是跑得更快，要利用你所知道的，跑得更聪明。"

#### 亲自尝试

1.  为带有代价为 0 的传送点的网格实现 0-1 BFS。
2.  在大型迷宫中比较 BFS 和双向 BFS。
3.  为 8x8 棋盘上的骑士移动谜题编写 A*。
4.  调整启发式函数，观察高估如何破坏 A*。
5.  结合 A* 和 0-1 BFS 进行混合搜索。

掌握了这些，你就能让最短路径搜索适应你问题的形状，使其高效、优雅且精确。
### 35. 最小生成树（Kruskal、Prim、Borůvka）

当一个图通过带权边连接多个点时，有时你需要的不是*最短路径*，而是连接所有点的*成本最低的网络*。

这就是最小生成树（MST）问题：

> 给定一个连通的、带权的、无向图，找到一个边的子集，该子集能以最小的总权重连接所有顶点且不包含环。

MST 无处不在，从构建网络和设计电路，到聚类和近似算法。

有三种基石算法可以完美地解决它：

- Kruskal 算法，基于边，使用并查集
- Prim 算法，基于顶点，贪心扩展
- Borůvka 算法，并行合并组件

#### 1. 什么是生成树？

一个生成树用恰好 (V-1) 条边连接所有顶点。
在所有生成树中，总权重最小的那个就是 MST。

性质：

- 不包含环
- 连接所有顶点
- 边数 = (V - 1)
- 如果所有权重都不同，则 MST 唯一

#### 2. MST 的应用

- 网络设计（道路、电缆、管道）
- 聚类（例如，层次聚类）
- 图像分割
- 近似算法（例如，TSP ~ 2 × MST）
- 图简化

#### 3. Kruskal 算法

按权重递增的顺序，逐条边地构建 MST。
使用并查集来避免形成环。

#### A. 步骤

1. 将所有边按权重排序。
2. 将每个顶点初始化为一个独立的组件。
3. 对于每条边 (u, v)：
   - 如果 `u` 和 `v` 属于不同的组件 → 包含该边
   - 合并它们的集合
   当选择了 (V-1) 条边时停止。

#### B. 实现

```c
struct Edge { int u, v, w; };
vector<Edge> edges;
int parent[MAX], rank_[MAX];

int find(int x) {
    return parent[x] == x ? x : parent[x] = find(parent[x]);
}
bool unite(int a, int b) {
    a = find(a); b = find(b);
    if (a == b) return false;
    if (rank_[a] < rank_[b]) swap(a, b);
    parent[b] = a;
    if (rank_[a] == rank_[b]) rank_[a]++;
    return true;
}

int kruskal(int n) {
    iota(parent, parent + n + 1, 0);
    sort(edges.begin(), edges.end(), [](Edge a, Edge b){ return a.w < b.w; });
    int total = 0;
    for (auto &e : edges)
        if (unite(e.u, e.v))
            total += e.w;
    return total;
}
```

复杂度：

- 排序边：$O(E \log E)$
- 并查集操作：$O(\alpha(V))$（几乎常数）
- 总计：$O(E \log E)$

#### C. 示例

图：

```
1 -4- 2  
|     |  
2     3  
 \-1-/
```

边排序后：(1-3,1), (1-2,4), (2-3,3)

选择 1-3, 2-3 → MST 权重 = 1 + 3 = 4

#### 4. Prim 算法

从一个起始顶点开始生长 MST，每一步添加最小的出边。

与 Dijkstra 算法类似，但选择的是边，而不是距离。

#### A. 步骤

1. 从一个顶点开始，标记为已访问。
2. 使用优先队列存储候选边。
3. 选择连接到未访问顶点的最小边。
4. 将顶点加入 MST，重复直到所有顶点都被访问。

#### B. 实现

```c
vector<pair<int,int>> adj[MAX]; // (v, w)
bool used[MAX];
int prim(int n, int start) {
    priority_queue<pair<int,int>, vector<pair<int,int>>, greater<>> pq;
    pq.push({0, start});
    int total = 0;

    while (!pq.empty()) {
        auto [w, u] = pq.top(); pq.pop();
        if (used[u]) continue;
        used[u] = true;
        total += w;
        for (auto [v, w2] : adj[u])
            if (!used[v]) pq.push({w2, v});
    }
    return total;
}
```

复杂度：

- 使用二叉堆：$O((V+E) \log V)$

使用场景：

- 图是稠密的
- 生长树比排序所有边更容易

#### C. 示例

图：

```
1 -2- 2  
|     |  
4     1  
 \-3-/
```

从 1 开始 → 选择 (1-2), (1-3) → MST 权重 = 2 + 3 = 5

#### 5. Borůvka 算法

不那么著名，但很优雅，并行地合并每个组件的最便宜出边。

每个组件选择一条最便宜的出边，添加它，合并组件。
重复直到只剩下一个组件。

复杂度：$O(E \log V)$

用于并行/分布式 MST 计算。

#### 6. 比较

| 算法 | 策略 | 时间复杂度 | 空间复杂度 | 最适合 |
| --- | --- | --- | --- | --- |
| Kruskal | 基于边，排序所有边 | O(E log E) | O(E) | 稀疏图 |
| Prim | 基于顶点，生长树 | O(E log V) | O(V+E) | 稠密图 |
| Borůvka | 组件合并 | O(E log V) | O(E) | 并行 MST |

#### 7. MST 的性质

- 割性质：对于任意割，最小的横跨边 ∈ MST。
- 环性质：对于任意环，最大的边 ∉ MST。
- 如果权重相等，MST 可能不唯一。

#### 8. 构建生成树

存储 MST 的边：

```c
vector<Edge> mst_edges;
if (unite(e.u, e.v)) mst_edges.push_back(e);
```

然后可以使用 MST 进行：

- 路径查询
- 聚类（移除最大的边）
- 近似 TSP（前序遍历）

#### 微型代码

Kruskal 示例：

```c
edges.push_back({1,2,4});
edges.push_back({1,3,1});
edges.push_back({2,3,3});
printf("MST = %d\n", kruskal(3)); // 4
```

#### 为何重要

MST 建模了无冗余的连接。
它们关乎效率，即以最小成本连接一切，这一原则出现在基础设施、数据乃至思想中。

它们教导我们：

> "如果你明智地选择，你可以用更少的资源连接整体。"

#### 亲自尝试

1. 使用并查集实现 Kruskal 算法。
2. 运行 Prim 算法并比较输出。
3. 在随机加权图上构建 MST，并可视化该树。
4. 从 MST 中移除最重的边以形成两个聚类。
5. 探索 Borůvka 算法用于并行执行。

MST 展示了如何以最小的努力跨越复杂性，它是一种平衡、经济和秩序的树。
### 36. 流网络（Ford-Fulkerson，Edmonds-Karp，Dinic）

有些图不仅仅是连接，它们还能*承载*某些东西。
想象水流过管道、车辆通过道路、数据通过网络。
每条边都有一个容量，而你想知道：

> "在系统堵塞之前，我能从源点向汇点发送多少？"

这就是**最大流问题**，
它是组合优化的基石，为匹配、割、调度等算法提供动力。

本节涵盖三大算法：

- Ford-Fulkerson，核心思想
- Edmonds-Karp，基于 BFS 的实现
- Dinic 算法，分层加速

#### 1. 问题定义

给定一个有向图 ( G = (V, E) )，
每条边 ( (u, v) ) 有一个容量 ( c(u, v) \ge 0 )。

我们有：

- 源点 ( s )
- 汇点 ( t )

我们想要从 ( s ) 到 ( t ) 的最大流：
一个满足以下条件的函数 ( f(u, v) )：

1. 容量约束: ( 0 \le f(u, v) \le c(u, v) )
2. 流量守恒:
   对于每个顶点 $v \neq s, t$:
   (\sum f(u, v) = \sum f(v, w))

总流量 = (\sum f(s, v))

#### 2. 核心概念

最大流 - 最小割定理：

> 最大流的值等于最小割的容量。

因此，寻找最大流等价于寻找瓶颈。

#### 3. Ford-Fulkerson 方法

核心思想：

- 只要存在一条从 (s) 到 (t) 且具有可用容量的路径，就沿该路径推送流量。

每一步：

1. 找到增广路径
2. 沿路径发送流量 = 路径上的最小剩余容量
3. 更新剩余容量

重复直到没有增广路径。

#### A. 残差图

残差容量：
$$
r(u, v) = c(u, v) - f(u, v)
$$
如果 ( f(u, v) > 0 )，则添加反向边 ( (v, u) )，容量为 ( f(u, v) )。

这允许在需要时撤销流量。

#### B. 实现（DFS 风格）

```c
const int INF = 1e9;
vector<pair<int,int>> adj[MAX];
int cap[MAX][MAX];

int dfs(int u, int t, int flow, vector<int>& vis) {
    if (u == t) return flow;
    vis[u] = 1;
    for (auto [v, _] : adj[u]) {
        if (!vis[v] && cap[u][v] > 0) {
            int pushed = dfs(v, t, min(flow, cap[u][v]), vis);
            if (pushed > 0) {
                cap[u][v] -= pushed;
                cap[v][u] += pushed;
                return pushed;
            }
        }
    }
    return 0;
}

int ford_fulkerson(int s, int t, int n) {
    int flow = 0;
    while (true) {
        vector<int> vis(n + 1, 0);
        int pushed = dfs(s, t, INF, vis);
        if (pushed == 0) break;
        flow += pushed;
    }
    return flow;
}
```

复杂度: (O$E \cdot \text{max flow}$)，取决于流量大小。

#### 4. Edmonds-Karp 算法

一种改进：

> 总是使用 BFS 选择最短（按边数）的增广路径。

保证多项式时间。

#### A. 实现（BFS + 父节点追踪）

```c
int bfs(int s, int t, vector<int>& parent, int n) {
    fill(parent.begin(), parent.end(), -1);
    queue<pair<int,int>> q;
    q.push({s, INF});
    parent[s] = -2;
    while (!q.empty()) {
        auto [u, flow] = q.front(); q.pop();
        for (auto [v, _] : adj[u]) {
            if (parent[v] == -1 && cap[u][v] > 0) {
                int new_flow = min(flow, cap[u][v]);
                parent[v] = u;
                if (v == t) return new_flow;
                q.push({v, new_flow});
            }
        }
    }
    return 0;
}

int edmonds_karp(int s, int t, int n) {
    int flow = 0;
    vector<int> parent(n + 1);
    int new_flow;
    while ((new_flow = bfs(s, t, parent, n))) {
        flow += new_flow;
        int v = t;
        while (v != s) {
            int u = parent[v];
            cap[u][v] -= new_flow;
            cap[v][u] += new_flow;
            v = u;
        }
    }
    return flow;
}
```

复杂度: (O$VE^2$)
总是终止（不依赖于流量值）。

#### 5. Dinic 算法

一个现代经典，
使用 BFS 构建层次图，使用 DFS 发送阻塞流。

它逐层工作，避免无用的探索。

#### A. 步骤

1.  通过 BFS 构建层次图（为可达节点分配层级）。
2.  DFS 沿符合层级的路径发送流量。
3.  重复直到没有路径剩余。

#### B. 实现

```c
vector<int> level, ptr;

bool bfs_level(int s, int t, int n) {
    fill(level.begin(), level.end(), -1);
    queue<int> q;
    q.push(s);
    level[s] = 0;
    while (!q.empty()) {
        int u = q.front(); q.pop();
        for (auto [v, _] : adj[u])
            if (level[v] == -1 && cap[u][v] > 0) {
                level[v] = level[u] + 1;
                q.push(v);
            }
    }
    return level[t] != -1;
}

int dfs_flow(int u, int t, int pushed) {
    if (u == t || pushed == 0) return pushed;
    for (int &cid = ptr[u]; cid < (int)adj[u].size(); cid++) {
        int v = adj[u][cid].first;
        if (level[v] == level[u] + 1 && cap[u][v] > 0) {
            int tr = dfs_flow(v, t, min(pushed, cap[u][v]));
            if (tr > 0) {
                cap[u][v] -= tr;
                cap[v][u] += tr;
                return tr;
            }
        }
    }
    return 0;
}

int dinic(int s, int t, int n) {
    int flow = 0;
    level.resize(n + 1);
    ptr.resize(n + 1);
    while (bfs_level(s, t, n)) {
        fill(ptr.begin(), ptr.end(), 0);
        while (int pushed = dfs_flow(s, t, INF))
            flow += pushed;
    }
    return flow;
}
```

复杂度: (O$EV^2$) 最坏情况，(O$E \sqrt{V}$) 实际中。

#### 6. 对比

| 算法           | 策略                 | 处理能力             | 时间复杂度      | 备注                             |
| -------------- | -------------------- | -------------------- | --------------- | -------------------------------- |
| Ford-Fulkerson | DFS 增广路径         | 整数容量             | O$E × max_flow$ | 简单，实数容量下可能循环         |
| Edmonds-Karp   | BFS 增广路径         | 所有容量             | O(VE²)          | 总是终止                         |
| Dinic          | 层次图 + DFS         | 所有容量             | O(V²E)          | 实践中很快                       |

#### 7. 应用

- 网络路由
- 二分图匹配
- 任务分配（流 = 人员 → 工作）
- 图像分割（最小割）
- 带需求的循环流
- 数据管道，最大吞吐量系统

#### 微型代码

Ford-Fulkerson 示例：

```c
add_edge(1, 2, 3);
add_edge(1, 3, 2);
add_edge(2, 3, 5);
add_edge(2, 4, 2);
add_edge(3, 4, 3);
printf("最大流 = %d\n", ford_fulkerson(1, 4, 4)); // 5
```

#### 为何重要

流算法将容量约束转化为可解的系统。
它们揭示了优化与结构之间的深刻统一：
每个最大流都定义了一个最小瓶颈割。

它们体现了一个永恒的真理：

> "要理解极限，请跟随流。"

#### 动手实践

1.  使用 DFS 实现 Ford-Fulkerson。
2.  切换到 Edmonds-Karp 并观察性能提升。
3.  构建 Dinic 的层次图并可视化层级。
4.  将任务分配建模为二分图流。
5.  在小例子中验证最大流 = 最小割。

一旦你掌握了流，你就会发现它们隐藏在所有流动的事物中，从数据到决策。
### 37. 割（Stoer-Wagner, Karger, Gomory-Hu）

流问题问的是 *"我们能发送多少？"*，
而割问题问的是 *"它在哪里断开？"*

一个割将图分割成两个不相交的集合。
最小割是移除后能使图不连通的最小边集，是维系图的最紧"瓶颈"。

本章探讨三种主要算法：

- Stoer-Wagner，用于无向图的确定性最小割算法
- Karger 随机算法，快速、概率性
- Gomory-Hu 树，将所有点对的最小割压缩到一棵树中

割揭示了隐藏的结构——聚类、脆弱点、边界——并通过最大流最小割定理构成了流的对偶。


#### 1. 最小割问题

给定一个带权无向图 ( G = (V, E) )：
找到移除后能使图不连通的边的最小总权重。

等价于：

> 跨越任意划分 ( $S, V \setminus S$ ) 的边权重之和的最小值。

对于有向图，你可以使用最大流方法；
对于无向图，则存在专门的算法。


#### 2. 应用

- 网络可靠性，最弱链路检测
- 聚类，通过最小互连划分图
- 电路设计，分割组件
- 图像分割，分离区域
- 社区检测，群体间的稀疏连接

#### 3. Stoer-Wagner 算法（确定性）

一种简洁、确定性的方法，用于求解无向图的全局最小割。


#### A. 思路

1. 从完整的顶点集 ( V ) 开始。
2. 重复运行最大邻接搜索：
   - 从一个顶点开始
   - 通过添加连接最紧密的顶点来扩展一个集合
   - 最后添加的顶点定义了一个割
3. 将最后添加的两个顶点收缩为一个顶点。
4. 记录遇到的最小割。

重复直到只剩下一个顶点。


#### B. 实现（邻接矩阵）

```c
const int INF = 1e9;
int g[MAX][MAX], w[MAX];
bool added[MAX], exist[MAX];

int stoer_wagner(int n) {
    int best = INF;
    vector<int> v(n);
    iota(v.begin(), v.end(), 0);

    while (n > 1) {
        fill(w, w + n, 0);
        fill(added, added + n, false);
        int prev = 0;
        for (int i = 0; i < n; i++) {
            int sel = -1;
            for (int j = 0; j < n; j++)
                if (!added[j] && (sel == -1 || w[j] > w[sel])) sel = j;
            if (i == n - 1) {
                best = min(best, w[sel]);
                for (int j = 0; j < n; j++)
                    g[prev][j] = g[j][prev] += g[sel][j];
                v.erase(v.begin() + sel);
                n--;
                break;
            }
            added[sel] = true;
            for (int j = 0; j < n; j++) w[j] += g[sel][j];
            prev = sel;
        }
    }
    return best;
}
```

复杂度: (O$V^3$)，或使用堆时为 (O$VE + V^2 \log V$)
输入: 带权无向图
输出: 全局最小割值


#### C. 示例

图：

```
1 -3- 2  
|     |  
4     2  
 \-5-/
```

割：

- {1,2}|{3} → 7
- {1,3}|{2} → 5
最小割 = 5


#### 4. Karger 算法（随机化）

一种简单、优雅的概率方法。
重复收缩随机边直到剩下两个顶点；
剩余的跨越边构成一个割。

运行多次 → 以高概率找到最小割。


#### A. 算法

1. 当 ( |V| > 2 ) 时：
   - 选择随机边 ((u, v))
   - 将 (u, v) 收缩为一个节点
   - 移除自环
2. 返回剩余节点之间的边数

重复 (O$n^2 \log n$) 次以获得高置信度。


#### B. 实现概览

```c
struct Edge { int u, v; };
vector<Edge> edges;
int parent[MAX];

int find(int x) { return parent[x] == x ? x : parent[x] = find(parent[x]); }
void unite(int a, int b) { parent[find(b)] = find(a); }

int karger(int n) {
    int m = edges.size();
    iota(parent, parent + n, 0);
    int vertices = n;
    while (vertices > 2) {
        int i = rand() % m;
        int u = find(edges[i].u), v = find(edges[i].v);
        if (u == v) continue;
        unite(u, v);
        vertices--;
    }
    int cuts = 0;
    for (auto e : edges)
        if (find(e.u) != find(e.v)) cuts++;
    return cuts;
}
```

期望时间: (O$n^2$) 每次运行
成功概率: (2 / (n(n-1))) 每次运行
运行多次试验并取最小值。


#### C. 使用场景

适用于大型稀疏图，或可接受近似解的情况。
直观理解：如果足够小心地选择，最小割会在随机收缩中幸存下来。


#### 5. Gomory-Hu 树

一种存储所有点对最小割的紧凑方式。
它将 (O$V^2$) 次流计算压缩为 V-1 次割计算。


#### A. 思路

- 构建一棵树，其中任意两个顶点之间的最小割 = 它们在树中路径上的最小边权重。


#### B. 算法

1. 选取顶点 (s)。
2. 对于每个顶点 $t \neq s$，
   - 运行最大流以找到 (s, t) 之间的最小割。
   - 相应地划分顶点。
3. 连接划分以形成一棵树。

结果：Gomory-Hu 树（V-1 条边）。

现在，任意点对的最小割 = 它们之间路径上的最小边。

复杂度: (O(V)) 次最大流运行。


#### C. 用途

- 快速回答所有点对割查询
- 网络可靠性
- 层次聚类

#### 6. 比较

| 算法         | 类型           | 随机化 | 图类型     | 复杂度                  | 输出                 |
| ------------ | -------------- | ------ | ---------- | ----------------------- | -------------------- |
| Stoer-Wagner | 确定性         | 否     | 无向       | O(V³)                   | 全局最小割           |
| Karger       | 随机化         | 是     | 无向       | O(n² log n) (多次运行)  | 概率性最小割         |
| Gomory-Hu    | 确定性         | 否     | 无向       | O(V × MaxFlow)          | 所有点对最小割       |


#### 7. 与流的关系

根据最大流最小割定理，
最小割容量 = 最大流值。

因此你可以找到：

- s-t 最小割 = 通过最大流
- 全局最小割 = 所有 (s, t) 点对的最小值

专门的算法只是使其更快。


#### 微型代码

Stoer-Wagner 示例：

```c
printf("全局最小割 = %d\n", stoer_wagner(n));
```

Karger 多次运行：

```c
int ans = INF;
for (int i = 0; i < 100; i++)
    ans = min(ans, karger(n));
printf("近似最小割 = %d\n", ans);
```


#### 为何重要

割向你展示了脆弱性，连接的薄弱之处。
流告诉你 *"能通过多少"*，而割揭示了 *"哪里会先断开"*。

它们教导我们：

> "要理解强度，就研究当你把东西拉开时会发生什么。"


#### 亲自尝试

1.  实现 Stoer-Wagner 并在小图上测试。
2.  运行 Karger 算法 100 次并跟踪成功率。
3.  构建一棵 Gomory-Hu 树并回答随机点对查询。
4.  在示例上验证最大流 = 最小割的等价性。
5.  在社交图中使用割进行社区检测。

掌握割使你既能把握又能洞察——系统在哪里支撑，又在哪里屈服。
### 38. 匹配问题（Hopcroft-Karp，匈牙利算法，开花算法）

在许多问题中，我们需要高效地将元素配对：学生与学校、工作与工人、任务与机器。

这些都是**匹配问题**，即寻找没有共享端点的边集，以最大化其基数或权重。

根据图的类型，适用不同的算法：

- **Hopcroft-Karp**：二分图中的快速匹配
- **匈牙利算法**：最优加权分配
- **Edmonds 开花算法**：一般图（非二分图）的匹配

匹配是一种基础的组合结构，出现在调度、流网络和资源分配中。

#### 1. 术语

- **匹配**：没有共享顶点的边集
- **最大匹配**：边数最多的匹配
- **完美匹配**：覆盖所有顶点（每个顶点恰好匹配一次）
- **最大权重匹配**：边总权重最大的匹配

**图类型**：

- **二分图**：顶点被分成两个集合（L，R）；边只存在于集合之间
- **一般图**：任意连接（可能包含奇数环）

#### 2. 应用

- 工作分配
- 网络流
- 资源分配
- 学生-项目配对
- 稳定婚姻问题（考虑偏好）
- 计算机视觉（特征对应）

#### 3. Hopcroft-Karp 算法（二分图匹配）

一种用于二分图中最大基数匹配的高效算法。

它使用分层 BFS + DFS 来同时寻找多条增广路径。

#### A. 思路

1.  初始化匹配为空。
2.  当存在增广路径时：
    - BFS 构建分层图（最短增广路径）。
    - DFS 沿着这些层次寻找所有增广路径。

每个阶段都能显著增加匹配的大小。

#### B. 复杂度

$$
O(E \sqrt{V})
$$

比一次只找一条增广路径（如 Ford-Fulkerson）快得多。

#### C. 实现

令 `pairU[u]` = 在 R 中匹配的顶点，如果未匹配则为 0
`pairV[v]` = 在 L 中匹配的顶点，如果未匹配则为 0

```c
vector<int> adjL[MAX];
int pairU[MAX], pairV[MAX], dist[MAX];
int nL, nR;

bool bfs() {
    queue<int> q;
    for (int u = 1; u <= nL; u++) {
        if (!pairU[u]) dist[u] = 0, q.push(u);
        else dist[u] = INF;
    }
    int found = INF;
    while (!q.empty()) {
        int u = q.front(); q.pop();
        if (dist[u] < found) {
            for (int v : adjL[u]) {
                if (!pairV[v]) found = dist[u] + 1;
                else if (dist[pairV[v]] == INF) {
                    dist[pairV[v]] = dist[u] + 1;
                    q.push(pairV[v]);
                }
            }
        }
    }
    return found != INF;
}

bool dfs(int u) {
    for (int v : adjL[u]) {
        if (!pairV[v] || (dist[pairV[v]] == dist[u] + 1 && dfs(pairV[v]))) {
            pairU[u] = v;
            pairV[v] = u;
            return true;
        }
    }
    dist[u] = INF;
    return false;
}

int hopcroft_karp() {
    int matching = 0;
    while (bfs()) {
        for (int u = 1; u <= nL; u++)
            if (!pairU[u] && dfs(u)) matching++;
    }
    return matching;
}
```

#### D. 示例

图：

```
U = {1,2,3}, V = {a,b}
边： 1–a, 2–a, 3–b
```

匹配： {1-a, 3-b} （大小 2）

#### 4. 匈牙利算法（加权二分图匹配）

解决**分配问题**，给定成本矩阵 $c_{ij}$，将每个 (i) 分配给一个 (j)，以最小化总成本（或最大化利润）。

#### A. 思路

逐行和逐列减去最小值 → 暴露零元素 → 寻找最小零覆盖 → 调整矩阵 → 重复。

等价于在二分图上求解最小成本完美匹配。

#### B. 复杂度

$$
O(V^3)
$$

适用于稠密图和中等规模。

#### C. 实现概览（矩阵形式）

```c
int hungarian(const vector<vector<int>>& cost) {
    int n = cost.size();
    vector<int> u(n+1), v(n+1), p(n+1), way(n+1);
    for (int i = 1; i <= n; i++) {
        p[0] = i; int j0 = 0;
        vector<int> minv(n+1, INF);
        vector<char> used(n+1, false);
        do {
            used[j0] = true;
            int i0 = p[j0], delta = INF, j1;
            for (int j = 1; j <= n; j++) if (!used[j]) {
                int cur = cost[i0-1][j-1] - u[i0] - v[j];
                if (cur < minv[j]) minv[j] = cur, way[j] = j0;
                if (minv[j] < delta) delta = minv[j], j1 = j;
            }
            for (int j = 0; j <= n; j++)
                if (used[j]) u[p[j]] += delta, v[j] -= delta;
                else minv[j] -= delta;
            j0 = j1;
        } while (p[j0]);
        do { int j1 = way[j0]; p[j0] = p[j1]; j0 = j1; } while (j0);
    }
    return -v[0]; // 最小成本
}
```

#### D. 示例

成本矩阵：

```
  a  b  c
1 3  2  1
2 2  3  2
3 3  2  3
```

最优分配 = 1-c, 2-a, 3-b
成本 = 1 + 2 + 2 = 5

#### 5. Edmonds 开花算法（一般图）

对于非二分图，简单的增广路径逻辑会失效（奇数环）。
开花算法通过收缩花（奇数环）来处理这种情况。

#### A. 思路

- 寻找增广路径
- 当遇到奇数环（花）时，将其收缩为一个顶点
- 继续搜索
- 最后展开花

#### B. 复杂度

$$
O(V^3)
$$

虽然实现复杂，但它是匹配问题的通用解决方案。

#### C. 使用场景

- 非二分图的工作/任务分配
- 一般配对问题
- 网络设计

#### 6. 比较

| 算法 | 图类型 | 加权 | 复杂度 | 输出 |
| :--- | :--- | :--- | :--- | :--- |
| Hopcroft-Karp | 二分图 | 否 | O(E√V) | 最大基数匹配 |
| 匈牙利算法 | 二分图 | 是 | O(V³) | 最小/最大成本匹配 |
| 开花算法 | 一般图 | 是 | O(V³) | 最大基数或权重匹配 |

#### 7. 与流的关系

二分图匹配 = 网络上的最大流：

- 左侧 → 源点边（容量 1）
- 右侧 → 汇点边（容量 1）
- 集合之间 → 边（容量 1）

匹配大小 = 流值

#### 微型代码

Hopcroft-Karp 演示：

```c
nL = 3; nR = 2;
adjL[1] = {1};
adjL[2] = {1};
adjL[3] = {2};
printf("最大匹配 = %d\n", hopcroft_karp()); // 2
```

#### 为何重要

匹配是配对和分配的语言。
它们表达了**没有重叠的合作**，一种平衡的结构。

它们揭示了一种深刻的二元性：

> "每一次匹配都是一次流，每一次分配都是一次优化。"

#### 动手尝试

1.  构建一个二分图并运行 Hopcroft-Karp。
2.  用匈牙利算法解决一个分配问题。
3.  从概念上探索开花算法的收缩思想。
4.  比较最大流方法与匹配方法。
5.  使用匹配来建模调度问题（人员 ↔ 任务）。

匹配教会我们如何无冲突地配对，
这是一堂兼具数学性和普适性的课程。
### 39. 树算法（LCA、HLD、重心分解）

树是许多算法的支柱，它们是连通的、无环的，并且结构优美。

由于其简单性，它们允许优雅的分治、动态规划和查询技术。
本节涵盖三种基本模式：

- **最近公共祖先（LCA）**：快速回答祖先查询
- **重链剖分（HLD）**：将树分解为链以支持线段树/路径查询
- **重心分解**：通过平衡递归分割树以进行分治

每种模式都揭示了从深度、链或平衡等不同角度理解树的方式。

#### 1. 最近公共祖先（LCA）

给定一棵树和两个节点 (u, v)。
LCA 是同时是两者祖先的最低节点（离根最远）。

应用：

- 距离查询
- 路径分解
- RMQ / 倍增法
- 树形 DP 和换根

#### A. 朴素方法

向上爬祖先直到相遇。
但每次查询是 (O(n))，对于大量查询来说太慢。

#### B. 倍增法

预处理 2 的幂次方的祖先。
然后按 2 的幂次跳跃以对齐深度。

预处理：

1. DFS 记录深度
2. `up[v][k]` = v 的第 2^k 个祖先

回答查询：

1. 将较深的节点提升到相同深度
2. 当 `up[u][k] != up[v][k]` 时，同时提升两个节点
3. 返回父节点

代码：

```c
const int LOG = 20;
vector<int> adj[MAX];
int up[MAX][LOG], depth[MAX];

void dfs(int u, int p) {
    up[u][0] = p;
    for (int k = 1; k < LOG; k++)
        up[u][k] = up[up[u][k-1]][k-1];
    for (int v : adj[u]) if (v != p) {
        depth[v] = depth[u] + 1;
        dfs(v, u);
    }
}

int lca(int u, int v) {
    if (depth[u] < depth[v]) swap(u, v);
    int diff = depth[u] - depth[v];
    for (int k = 0; k < LOG; k++)
        if (diff & (1 << k)) u = up[u][k];
    if (u == v) return u;
    for (int k = LOG-1; k >= 0; k--)
        if (up[u][k] != up[v][k])
            u = up[u][k], v = up[v][k];
    return up[u][0];
}
```

复杂度：

- 预处理： (O$n \log n$)
- 查询： (O$\log n$)

#### C. 示例

树：

```
    1
   / \
  2   3
 / \
4   5
```

- LCA(4,5) = 2
- LCA(4,3) = 1

#### 2. 重链剖分（HLD）

当你需要在树上高效地查询路径（求和、最大值、最小值等）时，可以使用重链剖分。

#### A. 思想

将树分解为链：

- **重边**：指向拥有最大子树的子节点的边
- **轻边**：其他边

结果：
从根到叶子的每条路径最多经过 (O$\log n$) 条轻边。

因此，一个路径查询可以分解为 (O$\log^2 n$) 个线段树查询。

#### B. 步骤

1. DFS 计算子树大小并识别重儿子
2. 分解为链
3. 为线段树分配 ID
4. 在线性化数组上使用线段树 / 树状数组

关键函数：

- `dfs_sz(u)` → 计算子树大小
- `decompose(u, head)` → 分配链头

代码（核心）：

```c
int parent[MAX], depth[MAX], heavy[MAX], head[MAX], pos[MAX];
int cur_pos = 0;

int dfs_sz(int u) {
    int size = 1, max_sz = 0;
    for (int v : adj[u]) if (v != parent[u]) {
        parent[v] = u;
        depth[v] = depth[u] + 1;
        int sz = dfs_sz(v);
        if (sz > max_sz) max_sz = sz, heavy[u] = v;
        size += sz;
    }
    return size;
}

void decompose(int u, int h) {
    head[u] = h;
    pos[u] = cur_pos++;
    if (heavy[u] != -1) decompose(heavy[u], h);
    for (int v : adj[u])
        if (v != parent[u] && v != heavy[u])
            decompose(v, v);
}
```

查询路径(u, v)：

- 当链头不同时，逐链向上移动
- 在线段树中查询 `[pos[head[u]], pos[u]]`
- 当处于同一链时，查询线段 `[pos[v], pos[u]]`

复杂度：

- 构建： (O(n))
- 查询/更新： (O$\log^2 n$)

#### C. 应用场景

- 路径求和
- 路径最大值
- 边更新
- 子树查询

#### 3. 重心分解

**重心** = 将树分割成每个子树大小 ≤ n/2 的节点。
通过递归移除重心，我们形成一棵重心树。

用于树上的分治。

#### A. 步骤

1. 寻找重心
   - DFS 计算子树大小
   - 选择最大子树 ≤ n/2 的节点
2. 分解：
   - 移除重心
   - 在子树上递归

代码（核心）：

```c
int subtree[MAX];
bool removed[MAX];
vector<int> adj[MAX];

int dfs_size(int u, int p) {
    subtree[u] = 1;
    for (int v : adj[u])
        if (v != p && !removed[v])
            subtree[u] += dfs_size(v, u);
    return subtree[u];
}

int find_centroid(int u, int p, int n) {
    for (int v : adj[u])
        if (v != p && !removed[v])
            if (subtree[v] > n / 2)
                return find_centroid(v, u, n);
    return u;
}

void decompose(int u, int p) {
    int n = dfs_size(u, -1);
    int c = find_centroid(u, -1, n);
    removed[c] = true;
    // 在此处理重心
    for (int v : adj[c])
        if (!removed[v])
            decompose(v, c);
}
```

复杂度： (O$n \log n$)

#### B. 应用

- 距离查询（分解 + 存储到重心的距离）
- 可通过分治解决的树问题
- 动态查询（添加/移除节点）

#### 4. 对比

| 算法 | 目的 | 查询 | 预处理 | 复杂度 | 备注 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| LCA | 祖先查询 | (O$\log n$) | (O$n \log n$) | 快速祖先查找 | |
| HLD | 路径查询 | (O$\log^2 n$) | (O(n)) | 适合线段树 | |
| 重心分解 | 分解树 | - | (O$n \log n$) | 平衡分割 | |

#### 5. 相互联系

- HLD 内部常使用 LCA。
- 重心分解可能使用到祖先的距离（通过 LCA）。
- 所有算法都利用树结构来实现亚线性查询。

#### 简短代码

LCA(4,5)：

```c
dfs(1,1);
printf("%d\n", lca(4,5)); // 2
```

HLD 路径求和：
在 `pos[u]` 顺序上构建线段树，沿链查询。

重心：
`decompose(1, -1);`

#### 为何重要

树算法展示了结构如何解锁效率。
它们将朴素的遍历转变为快速、分层或递归的解决方案。

要掌握数据结构，你必须学会如何智能地“爬升”和“切割”树。

> “每条有根路径都隐藏着一个对数。”

#### 亲自尝试

1. 实现倍增法 LCA 并测试查询。
2. 在 HLD 上添加线段树并运行路径求和。
3. 通过重心分解树并统计距离为 k 的节点数。
4. 结合 LCA + HLD 进行路径最小/最大值查询。
5. 绘制一个简单图的重心树。

掌握这些，树将不再是“仅仅是图”，它们将成为*工具*。
### 40. 高级图算法与技巧

到目前为止，你已经见识了图算法的主要家族：遍历、最短路径、流、匹配、割和树。
但现实世界的图常常带来额外的约束：动态更新、多源、分层结构或特殊性质（平面图、有向无环图、稀疏图）。

本节汇集了强大的高级图技术、技巧和模式，一旦你掌握了基础知识，这些问题就会在各种场景中出现。

我们将探讨：

- 拓扑排序与 DAG 上的动态规划
- 强连通分量（缩点图）
- 割点与桥（2-边/点连通性）
- 欧拉路径与哈密顿路径
- 图着色与二分图判定
- 环检测与有向无环推理
- 小集合合并、树上并查集、树上莫队算法
- 基于状态压缩的动态规划
- 动态图（增量/减量 BFS/DFS）
- 特殊图（平面图、稀疏图、稠密图）

这些不仅仅是算法，它们是让你能够凭借洞察力解决更困难图问题的模式。

#### 1. 拓扑排序与 DAG 上的动态规划

在有向无环图中，边总是向前指。
这使得我们可以将顶点线性排序，使得所有边都从左指向右，这就是拓扑序。

应用场景：

- 任务调度
- 依赖关系解析
- 在 DAG 上进行动态规划（最长/最短路径、路径计数）

算法（Kahn 算法）：

```c
vector<int> topo_sort(int n) {
    vector<int> indeg(n+1), res;
    queue<int> q;
    for (int u = 1; u <= n; u++)
        for (int v : adj[u]) indeg[v]++;
    for (int u = 1; u <= n; u++)
        if (!indeg[u]) q.push(u);
    while (!q.empty()) {
        int u = q.front(); q.pop();
        res.push_back(u);
        for (int v : adj[u])
            if (--indeg[v] == 0) q.push(v);
    }
    return res;
}
```

DAG 上的动态规划：

```c
vector<int> dp(n+1, 0);
for (int u : topo_order)
    for (int v : adj[u])
        dp[v] = max(dp[v], dp[u] + weight(u,v));
```

复杂度：O(V + E)

#### 2. 强连通分量（缩点）

在有向图中，顶点可能形成强连通分量（相互可达的分量）。
将 SCC 缩点后得到一个 DAG，通常更容易推理。

用途：

- 分量压缩
- 元图推理
- 环的压缩

Tarjan 算法：
使用低链接值的 DFS，单次遍历。

Kosaraju 算法：
两次遍历，分别在原图和反向图上进行 DFS。

复杂度：O(V + E)

一旦构建了 SCC，你就可以在缩点后的 DAG 上运行动态规划或拓扑排序。

#### 3. 割点与桥

找出移除后会断开图的临界顶点/边。

- 割点：移除后增加连通分量数量的顶点
- 桥：移除后增加连通分量数量的边

算法：Tarjan 的 DFS
跟踪发现时间 `tin[u]` 和最低可达祖先 `low[u]`。

```c
void dfs(int u, int p) {
    tin[u] = low[u] = ++timer;
    for (int v : adj[u]) {
        if (v == p) continue;
        if (!tin[v]) {
            dfs(v, u);
            low[u] = min(low[u], low[v]);
            if (low[v] > tin[u]) bridge(u, v);
            if (low[v] >= tin[u] && p != -1) cut_vertex(u);
        } else low[u] = min(low[u], tin[v]);
    }
}
```

应用：

- 网络可靠性
- 双连通分量
- 2-边/点连通性测试

#### 4. 欧拉路径与哈密顿路径

- 欧拉路径：恰好访问每条边一次
  - 当图连通且恰好有 0 个或 2 个顶点的度为奇数时存在
- 哈密顿路径：恰好访问每个顶点一次（NP 难问题）

欧拉回路构造：Hierholzer 算法（O(E)）

应用：

- 路线重建（例如，单词接龙）
- 邮递员问题

#### 5. 图着色与二分图判定

二分图检查：
DFS/BFS 交替着色
如果发现奇环则失败。

```c
bool bipartite(int n) {
    vector<int> color(n+1, -1);
    for (int i = 1; i <= n; i++) if (color[i] == -1) {
        queue<int> q; q.push(i); color[i] = 0;
        while (!q.empty()) {
            int u = q.front(); q.pop();
            for (int v : adj[u]) {
                if (color[v] == -1)
                    color[v] = color[u] ^ 1, q.push(v);
                else if (color[v] == color[u])
                    return false;
            }
        }
    }
    return true;
}
```

应用：

- 2-SAT 归约
- 平面图着色
- 无冲突分配

#### 6. 环检测

- 有向图：DFS + 递归栈
- 无向图：并查集

用于测试无环性、检测后向边，或为回滚或一致性检查寻找环。

#### 7. 树上并查集（小集合合并）

对于类似“统计子树中不同颜色的数量”的查询，
将结果从小子树合并到大的子树，以维持 O(n log n) 的复杂度。

模式：

1.  DFS 遍历子节点
2.  保留大孩子的数据结构
3.  合并小孩子的数据

应用：

- 离线子树查询
- 重型子问题缓存

#### 8. 树上莫队算法

离线算法，用于高效回答路径查询：

- 通过欧拉序将路径查询转换为区间查询
- 使用莫队排序以 O((N + Q)√N) 的复杂度处理

当不需要在线更新时很有用。

#### 9. 基于状态压缩的动态规划

适用于小图（n ≤ 20）：
状态 = 顶点的子集
例如，旅行商问题

```c
dp[mask][u] = 访问 mask 中的顶点并在 u 结束的最小成本
```

状态转移：

```c
dp[mask | (1<<v)][v] = min(dp[mask][u] + cost[u][v])
```

复杂度：O(n² 2ⁿ)

#### 10. 动态图

图结构会发生变化：

- 增量 BFS：在添加边时维护距离
- 减量连通性：并查集回滚或动态树

用于在线查询、演化网络或实时系统。

#### 11. 特殊图类

- 平面图：边数 ≤ 3V-6；使用面计数
- 稀疏图：邻接表最佳
- 稠密图：邻接矩阵 / 位集

优化通常取决于图的密度。

#### 精简代码

拓扑排序：

```c
auto order = topo_sort(n);
for (int u : order) printf("%d ", u);
```

桥检查：
`if (low[v] > tin[u])` 则该边是桥。

欧拉路径检查：
统计度为奇数的节点数 == 0 或 2。

#### 为何重要

这些高级技术完善了你的工具箱。
它们不是孤立的，而是结合起来解决现实世界的难题：
依赖图、鲁棒网络、优化路径、压缩状态。

它们传授了一种思维方式：

> “图不是障碍，它们是可能性的形状。”

#### 动手实践

1.  实现拓扑排序和 DAG 上的动态规划。
2.  寻找强连通分量并构建缩点图。
3.  检测割点和桥。
4.  在随机图上检查欧拉路径条件。
5.  尝试使用树上并查集进行子树统计。
6.  对于 n ≤ 15，通过状态压缩动态规划解决 TSP。

一旦你能混合搭配这些工具，
你就不仅仅是在导航图，你是在塑造它们。

## 第 5 章. 动态规划
### 41. 动态规划基础与状态转移

动态规划（DP）是算法设计中最强大的思想之一。
它的核心是将一个大问题分解为更小的重叠子问题，每个子问题只求解一次，并复用它们的答案。

当暴力解法的时间复杂度呈指数级爆炸时，动态规划能将其重新控制在可管理范围内。
本节将介绍动态规划背后的思维方式、运作机制和数学原理。

#### 1. 核心思想

许多问题具有两个关键性质：

- **重叠子问题**：相同的小规模计算会重复很多次。
- **最优子结构**：问题的最优解可以由其子问题的最优解构建出来。

动态规划对每个子问题只求解一次，存储其结果，并加以复用。
这节省了指数级的时间，通常能将 $O(2^n)$ 的复杂度降低到 $O(n^2)$ 或 $O(n)$。

#### 2. 解题步骤

处理动态规划问题时，请遵循以下模式：

1. **定义状态**。
   决定要解决哪些子问题。
   例如：`dp[i] = 前 i 个元素的最佳答案`。

2. **写出递推关系**。
   用更小的状态来表达每个状态。
   例如：`dp[i] = dp[i-1] + cost(i)`。

3. **设定基本情况**。
   递归从哪里开始？
   例如：`dp[0] = 0`。

4. **决定计算顺序**。
   自底向上（迭代）还是自顶向下（递归加记忆化）。

5. **返回最终答案**。
   通常是 `dp[n]` 或 `max(dp[i])`。

#### 3. 示例：斐波那契数列

让我们从一个经典例子开始，
第 n 个斐波那契数（$F(n) = F(n-1) + F(n-2)$）。

递归（慢）：

```c
int fib(int n) {
    if (n <= 1) return n;
    return fib(n - 1) + fib(n - 2);
}
```

这种方法会反复计算相同的值，时间复杂度是指数级的。

自顶向下动态规划（记忆化）：

```c
int dp[MAX];
int fib(int n) {
    if (n <= 1) return n;
    if (dp[n] != -1) return dp[n];
    return dp[n] = fib(n-1) + fib(n-2);
}
```

自底向上动态规划（制表法）：

```c
int fib(int n) {
    int dp[n+1];
    dp[0] = 0; dp[1] = 1;
    for (int i = 2; i <= n; i++)
        dp[i] = dp[i-1] + dp[i-2];
    return dp[n];
}
```

空间优化版本：

```c
int fib(int n) {
    int a = 0, b = 1, c;
    for (int i = 2; i <= n; i++) {
        c = a + b;
        a = b;
        b = c;
    }
    return b;
}
```

#### 4. 状态、转移与依赖关系

动态规划表是从状态到答案的映射。
每个状态通过一个转移函数依赖于其他状态。

可以将其想象成一个图，
每条边代表一个递推关系。

示例：

- 状态：`dp[i] = 到达第 i 级台阶的方法数`
- 转移：`dp[i] = dp[i-1] + dp[i-2]` （类似爬楼梯）
- 基本情况：`dp[0] = 1`

#### 5. 常见的动态规划模式

1.  **一维线性动态规划**
    - 例如斐波那契数列、爬楼梯、最长递增子序列（LIS）。
2.  **二维动态规划**
    - 网格、序列或组合问题（最长公共子序列 LCS、背包问题）。
3.  **状态压缩动态规划**
    - 子集、旅行商问题（TSP）、组合优化。
4.  **树形动态规划**
    - 子树计算（求和、直径）。
5.  **数位动态规划**
    - 统计某个范围内具有特定性质的数字数量。
6.  **区间动态规划**
    - 矩阵链乘法、区间合并。

#### 6. 自顶向下 vs 自底向上

| 方法       | 实现方式              | 优点                     | 缺点                       |
| ---------- | --------------------- | ------------------------ | -------------------------- |
| 自顶向下   | 递归 + 记忆化         | 易于编写，直观           | 栈开销，需要记忆化数组     |
| 自底向上   | 迭代                  | 快速，可进行空间优化     | 推导计算顺序较难           |

当依赖关系简单且无环时，自底向上方法表现出色。
当依赖关系复杂时，自顶向下方法更容易实现。

#### 7. 示例 2：爬楼梯

每次可以爬 1 级或 2 级台阶。
有多少种不同的方式可以到达第 $n$ 级台阶？

状态：`dp[i] = 到达第 i 级台阶的方法数`
转移：`dp[i] = dp[i-1] + dp[i-2]`
基本情况：`dp[0] = 1, dp[1] = 1`

代码：

```c
int climb(int n) {
    int dp[n+1];
    dp[0] = dp[1] = 1;
    for (int i = 2; i <= n; i++)
        dp[i] = dp[i-1] + dp[i-2];
    return dp[n];
}
```

#### 8. 调试动态规划

调试动态规划的方法：

- 打印中间状态。
- 可视化表格（尤其是二维表格）。
- 检查基本情况。
- 手动追踪一个小例子。

#### 9. 复杂度分析

大多数动态规划算法的时间复杂度与状态数量呈线性或平方关系：

- 时间复杂度 = (状态数量) × (每次转移的工作量)
- 空间复杂度 = (状态数量)

示例：
斐波那契数列：$O(n)$ 时间，$O(1)$ 空间
背包问题：$O(n \times W)$
最长公共子序列：$O(n \times m)$

#### 微型代码

斐波那契数列（制表法）：

```c
int dp[100];
dp[0] = 0; dp[1] = 1;
for (int i = 2; i <= n; i++)
    dp[i] = dp[i-1] + dp[i-2];
printf("%d", dp[n]);
```

#### 为何重要

动态规划是记忆的艺术。
它将递归转化为迭代，将混乱转化为秩序。

从优化到计数，从路径到序列，
一旦你看到了子结构，动态规划就会成为你的得力工具。

> "每一次重复都隐藏着一个递推关系。"

#### 动手实践

1.  编写自顶向下和自底向上的斐波那契数列求解代码。
2.  计算使用 {1,2,3} 级步长爬楼梯的方法数。
3.  计算 n×m 网格中的路径数量。
4.  尝试在每个问题中识别状态、递推关系和基本情况。
5.  绘制依赖关系图以可视化状态转移。

动态规划不是一个公式，而是一种思维方式：
将问题分解成部分，记住过去，并在此基础上构建未来。
### 42. 经典问题（背包、子集和、硬币找零）

既然你已经知道动态规划*是什么*，现在让我们深入探讨每个程序员早期都会遇到的经典三剑客问题：

- 背包问题（在重量限制下最大化价值）
- 子集和问题（我们能否组成给定的和？）
- 硬币找零问题（有多少种方式或最少硬币数能达到总额）

这些是动态规划的练兵场：每个问题都清晰地展示了如何定义状态、转移和基本情况。

#### 1. 0/1 背包问题

**问题描述：**
你有 `n` 件物品，每件物品有重量 `w[i]` 和价值 `v[i]`。
一个容量为 `W` 的背包。
选择物品（每件最多一次）以最大化总价值，且不超过重量限制。

#### A. 状态

`dp[i][w]` = 使用前 `i` 件物品，在容量 `w` 下的最大价值

#### B. 递推关系

对于物品 `i`：
- 如果不拿它：`dp[i-1][w]`
- 如果拿它（如果 `w[i] ≤ w`）：`dp[i-1][w - w[i]] + v[i]`

所以，
$$
dp[i][w] = \max(dp[i-1][w], dp[i-1][w - w[i]] + v[i])
$$

#### C. 基本情况

对所有 w，`dp[0][w] = 0`（没有物品 = 没有价值）

#### D. 实现

```c
int knapsack(int n, int W, int w[], int v[]) {
    int dp[n+1][W+1];
    for (int i = 0; i <= n; i++) {
        for (int j = 0; j <= W; j++) {
            if (i == 0 || j == 0) dp[i][j] = 0;
            else if (w[i-1] <= j)
                dp[i][j] = max(dp[i-1][j], dp[i-1][j - w[i-1]] + v[i-1]);
            else
                dp[i][j] = dp[i-1][j];
        }
    }
    return dp[n][W];
}
```

**复杂度：**
时间复杂度：$O(nW)$
空间复杂度：$O(nW)$（可优化为一维 $O(W)$）

#### E. 空间优化（一维 DP）

```c
int dp[W+1] = {0};
for (int i = 0; i < n; i++)
    for (int w = W; w >= weight[i]; w--)
        dp[w] = max(dp[w], dp[w - weight[i]] + value[i]);
```

#### F. 示例

物品：
```
w = [2, 3, 4, 5]
v = [3, 4, 5, 6]
W = 5
```

最优解：拿物品 1 + 2 → 价值 7

#### 2. 子集和问题

**问题描述：**
给定一个整数集合 `S`，我们能否选择其中一些数，使其和等于 `target`？

#### A. 状态

`dp[i][sum] = true` 如果我们可以使用前 `i` 个元素组成和 `sum`。

#### B. 递推关系

- 不拿：`dp[i-1][sum]`
- 拿（如果 `a[i] ≤ sum`）：`dp[i-1][sum - a[i]]`

所以，
$$
dp[i][sum] = dp[i-1][sum] \; || \; dp[i-1][sum - a[i]]
$$

#### C. 基本情况

`dp[0][0] = true`（没有元素时和为 0 是可能的）
对于 sum > 0，`dp[0][sum] = false`

#### D. 实现

```c
bool subset_sum(int a[], int n, int target) {
    bool dp[n+1][target+1];
    for (int i = 0; i <= n; i++) dp[i][0] = true;
    for (int j = 1; j <= target; j++) dp[0][j] = false;

    for (int i = 1; i <= n; i++) {
        for (int j = 1; j <= target; j++) {
            if (a[i-1] > j) dp[i][j] = dp[i-1][j];
            else dp[i][j] = dp[i-1][j] || dp[i-1][j - a[i-1]];
        }
    }
    return dp[n][target];
}
```

**复杂度：**
时间复杂度：$O(n \cdot target)$

#### E. 示例

S = [3, 34, 4, 12, 5, 2], target = 9
可以 → 4 + 5

#### 3. 硬币找零问题

两个变体：

#### (a) 计算方式数（硬币数量无限）

"使用硬币 `c[]` 凑成总额 `T` 有多少种方法？"
顺序无关紧要。

**状态：** `dp[i][t] = 使用前 i 种硬币凑成总额 t 的方法数`

**递推关系：**
- 跳过硬币：`dp[i-1][t]`
- 拿硬币（无限次）：`dp[i][t - c[i]]`
$$
dp[i][t] = dp[i-1][t] + dp[i][t - c[i]]
$$

**基本情况：** `dp[0][0] = 1`

**一维简化版：**

```c
int dp[T+1] = {0};
dp[0] = 1;
for (int coin : coins)
    for (int t = coin; t <= T; t++)
        dp[t] += dp[t - coin];
```

#### (b) 最少硬币数（凑成总额所需的最少硬币）

**状态：** `dp[t] = 凑成总额 t 所需的最少硬币数`

**递推关系：**
$$
dp[t] = \min_{c_i \le t}(dp[t - c_i] + 1)
$$

**基本情况：** `dp[0] = 0`，其余 = INF

```c
int dp[T+1];
fill(dp, dp+T+1, INF);
dp[0] = 0;
for (int t = 1; t <= T; t++)
    for (int c : coins)
        if (t >= c) dp[t] = min(dp[t], dp[t - c] + 1);
```

#### 示例

Coins = [1,2,5], Total = 5
- 方式数：4 (5; 2+2+1; 2+1+1+1; 1+1+1+1+1)
- 最少硬币数：1 (5)

#### 4. 总结

| 问题                | 类型         | 状态        | 转移关系                 | 复杂度      |
| ------------------- | ------------ | ----------- | ------------------------ | ----------- |
| 0/1 背包            | 最大值       | dp[i][w]    | max(拿, 不拿)            | O(nW)       |
| 子集和              | 可行性       | dp[i][sum]  | 包含/排除的 OR 运算      | O(n * sum)  |
| 硬币找零（方式数）  | 计数         | dp[t]       | dp[t] + dp[t - coin]     | O(nT)       |
| 硬币找零（最少）    | 最优化       | dp[t]       | min(dp[t - coin] + 1)    | O(nT)       |

#### 精简代码

最少硬币找零（一维）：

```c
int dp[T+1];
fill(dp, dp+T+1, INF);
dp[0] = 0;
for (int c : coins)
    for (int t = c; t <= T; t++)
        dp[t] = min(dp[t], dp[t - c] + 1);
printf("%d\n", dp[T]);
```

#### 为何重要

这三个问题是原型：
- 背包问题：在约束下优化
- 子集和问题：选择可行性
- 硬币找零问题：计数或最小化

一旦你掌握了它们，你就能在更复杂的问题中识别出它们的模式，从资源分配到路径查找。

> "每个约束背后都隐藏着一个选择；每个选择背后都隐藏着一个状态。"

#### 亲自尝试

1.  实现 0/1 背包问题（二维和一维）。
2.  用随机列表解决目标为 30 的子集和问题。
3.  计算总额为 10 的硬币组合方式数。
4.  比较"最少硬币数"与"组成方式数"。
5.  为每个问题写下状态转移图。

这三个问题构成了你的动态规划基础，是构建更复杂算法的语法。
### 43. 序列问题（LIS、LCS、编辑距离）

序列问题是动态规划的*核心*。
它们出现在字符串、数组、基因组、文本比较和版本控制中。
它们的力量来自于比较前缀，并从对齐的较小答案构建出更大的答案。

本节探讨三个基石问题：

- LIS（最长递增子序列）- LCS（最长公共子序列）- 编辑距离（Levenshtein 距离）
每个问题都教授一种思考子问题、状态转移和结构的新方式。


#### 1. 最长递增子序列 (LIS)

问题：
给定一个数组，找出*严格递增*的最长子序列的长度。

子序列不一定是连续的，你可以跳过元素。

示例：
`[10, 9, 2, 5, 3, 7, 101, 18]` → LIS 是 `[2, 3, 7, 18]` → 长度为 4


#### A. 状态定义

`dp[i]` = 以索引 `i` 结尾的 LIS 的长度


#### B. 递推关系

$$
dp[i] = 1 + \max_{j < i \land a[j] < a[i]} dp[j]
$$

如果没有更小的 `a[j]`，则 `dp[i] = 1`。


#### C. 基础情况

对所有 i，`dp[i] = 1`（每个元素自身就是一个 LIS）


#### D. 实现

```c
int lis(int a[], int n) {
    int dp[n], best = 0;
    for (int i = 0; i < n; i++) {
        dp[i] = 1;
        for (int j = 0; j < i; j++)
            if (a[j] < a[i])
                dp[i] = max(dp[i], dp[j] + 1);
        best = max(best, dp[i]);
    }
    return best;
}
```

复杂度：$O(n^2)$


#### E. 二分查找优化

使用一个 tail 数组：

- `tail[len] = 长度为 len 的 LIS 可能的最小结尾值`
对于每个 `x`：

- 通过 lower_bound 替换 `tail[idx]`
```c
int lis_fast(vector<int>& a) {
    vector<int> tail;
    for (int x : a) {
        auto it = lower_bound(tail.begin(), tail.end(), x);
        if (it == tail.end()) tail.push_back(x);
        else *it = x;
    }
    return tail.size();
}
```

复杂度：$O(n \log n)$


#### 2. 最长公共子序列 (LCS)

问题：
给定两个字符串，找出同时存在于两者中的最长子序列。

示例：
`s1 = "ABCBDAB"`, `s2 = "BDCABA"`
LCS = "BCBA" → 长度为 4


#### A. 状态定义

`dp[i][j]` = `s1[0..i-1]` 和 `s2[0..j-1]` 之间的 LCS 长度


#### B. 递推关系

$$
dp[i][j] =
\begin{cases}
dp[i-1][j-1] + 1, & \text{if } s_1[i-1] = s_2[j-1], \\
\max(dp[i-1][j],\, dp[i][j-1]), & \text{otherwise.}
\end{cases}
$$


#### C. 基础情况

`dp[0][*] = dp[*][0] = 0`（空字符串）


#### D. 实现

```c
int lcs(string a, string b) {
    int n = a.size(), m = b.size();
    int dp[n+1][m+1];
    for (int i = 0; i <= n; i++)
        for (int j = 0; j <= m; j++)
            if (i == 0 || j == 0) dp[i][j] = 0;
            else if (a[i-1] == b[j-1])
                dp[i][j] = dp[i-1][j-1] + 1;
            else
                dp[i][j] = max(dp[i-1][j], dp[i][j-1]);
    return dp[n][m];
}
```

复杂度：$O(nm)$


#### E. 重构 LCS

从 `dp[n][m]` 回溯：

- 如果字符相等 → 取该字符并向对角线移动- 否则向值更大的邻居移动

#### F. 示例

a = "AGGTAB", b = "GXTXAYB"
LCS = "GTAB" → 4


#### 3. 编辑距离 (Levenshtein 距离)

问题：
将字符串 `a` 转换为 `b` 所需的最小操作次数（插入、删除、替换）。

示例：
`kitten → sitting` = 3（替换 k→s，插入 i，插入 g）


#### A. 状态定义

`dp[i][j]` = 将 `a[0..i-1]` 转换为 `b[0..j-1]` 所需的最小编辑次数


#### B. 递推关系

如果 `a[i-1] == b[j-1]`：
$$
dp[i][j] = dp[i-1][j-1]
$$

否则：
$$
dp[i][j] = 1 + \min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
$$
（删除、插入、替换）


#### C. 基础情况

- `dp[0][j] = j`（全部插入）- `dp[i][0] = i`（全部删除）

#### D. 实现

```c
int edit_distance(string a, string b) {
    int n = a.size(), m = b.size();
    int dp[n+1][m+1];
    for (int i = 0; i <= n; i++)
        for (int j = 0; j <= m; j++) {
            if (i == 0) dp[i][j] = j;
            else if (j == 0) dp[i][j] = i;
            else if (a[i-1] == b[j-1])
                dp[i][j] = dp[i-1][j-1];
            else
                dp[i][j] = 1 + min({dp[i-1][j], dp[i][j-1], dp[i-1][j-1]});
        }
    return dp[n][m];
}
```

复杂度：$O(nm)$


#### E. 示例

a = "horse", b = "ros"

- 替换 h→r，删除 r，删除 e → 3

#### 4. 总结

| 问题          | 类型       | 状态      | 转移方程                | 复杂度               |
| ------------- | ---------- | -------- | ----------------------- | -------------------- |
| LIS           | 单序列     | dp[i]    | 1 + max(dp[j])          | O(n²) / O(n log n)   |
| LCS           | 双序列     | dp[i][j] | if match +1 else max    | O(nm)                |
| 编辑距离      | 双序列     | dp[i][j] | if match 0 else 1 + min | O(nm)                |


#### 5. 常见洞见

- LIS 从较小的序列*向上构建*。- LCS *对齐*两个序列，比较前缀。- 编辑距离*量化差异*，即最小编辑次数。
它们是生物信息学、文本差异比较、版本控制等领域的模板。


#### 精简代码

LCS：

```c
if (a[i-1] == b[j-1])
    dp[i][j] = dp[i-1][j-1] + 1;
else
    dp[i][j] = max(dp[i-1][j], dp[i][j-1]);
```


#### 为何重要

序列 DP 教你如何比较*进展*，以及结构和相似性如何随时间演变。

它们将模糊的“比较这些”任务转化为清晰的递推关系。

> “对齐即理解。”


#### 动手实践

1. 实现 LIS（O(n²) 和 O(n log n)）
2. 找出两个给定字符串的 LCS
3. 计算 "intention" 和 "execution" 之间的编辑距离
4. 修改 LCS 代码以打印出一个有效的子序列
5. 尝试在单个表格中统一 LCS 和编辑距离

掌握这些，你就能处理任何关于序列的 DP 问题，它们是算法思维的 DNA。
### 44. 矩阵与链式问题

当一个问题涉及在区间上做选择时——选择顺序、选择分割点、选择括号化方式——动态规划便能大显身手。本章探讨一类基于链和矩阵的问题，其中顺序至关重要，子结构由区间定义。

我们将学习：

- 矩阵链乘法 (MCM) - 最优括号化
- 多边形三角剖分 - 将形状划分为成本最小的三角形
- 最优二叉搜索树 / 合并模式 - 带权重的合并决策

这些问题教授区间动态规划，其中每个状态代表一个段 ([i, j])。

#### 1. 矩阵链乘法 (MCM)

问题：
给定矩阵 $A_1, A_2, ..., A_n$，找到使总标量乘法次数最少的括号化方案。

矩阵 $A_i$ 的维度为 $p[i-1] \times p[i]$。
只有当内部维度匹配时，我们才能相乘 $A_i \cdot A_{i+1}$。

目标：最小化操作次数：
$$
\text{cost}(i, j) = \min_k \big(\text{cost}(i, k) + \text{cost}(k+1, j) + p[i-1] \cdot p[k] \cdot p[j]\big)
$$

#### A. 状态

`dp[i][j]` = 计算 $A_i...A_j$ 所需的最小乘法次数

#### B. 基础情况

`dp[i][i] = 0`（单个矩阵无需乘法）

#### C. 递推关系

$$
dp[i][j] = \min_{i \le k < j} { dp[i][k] + dp[k+1][j] + p[i-1] \times p[k] \times p[j] }
$$

#### D. 实现

```c
int matrix_chain(int p[], int n) {
    int dp[n][n];
    for (int i = 1; i < n; i++) dp[i][i] = 0;

    for (int len = 2; len < n; len++) {
        for (int i = 1; i + len - 1 < n; i++) {
            int j = i + len - 1;
            dp[i][j] = INT_MAX;
            for (int k = i; k < j; k++)
                dp[i][j] = min(dp[i][j],
                    dp[i][k] + dp[k+1][j] + p[i-1]*p[k]*p[j]);
        }
    }
    return dp[1][n-1];
}
```

复杂度：($O(n^3)$) 时间，($O(n^2)$) 空间

#### E. 示例

p = [10, 20, 30, 40, 30]
最优顺序：((A1A2)A3)A4 → 成本 30000

#### 2. 多边形三角剖分

给定一个有 `n` 个顶点的凸多边形，连接不相交的对角线以最小化总成本。
三角形的成本 = 周长或边权重的乘积。

这与 MCM 结构相同，通过对角线分割多边形。

#### A. 状态

`dp[i][j]` = 从顶点 i 到 j 的多边形三角剖分的最小成本。

#### B. 递推关系

$$
dp[i][j] = \min_{i < k < j} (dp[i][k] + dp[k][j] + cost(i, j, k))
$$

基础情况：dp[i][i+1] = 0（少于 3 个点）

#### C. 实现

```c
double polygon_triangulation(vector<Point> &p) {
    int n = p.size();
    double dp[n][n];
    for (int i = 0; i < n; i++) for (int j = 0; j < n; j++) dp[i][j] = 0;
    for (int len = 2; len < n; len++) {
        for (int i = 0; i + len < n; i++) {
            int j = i + len;
            dp[i][j] = 1e18;
            for (int k = i+1; k < j; k++)
                dp[i][j] = min(dp[i][j],
                    dp[i][k] + dp[k][j] + dist(p[i],p[k])+dist(p[k],p[j])+dist(p[j],p[i]));
        }
    }
    return dp[0][n-1];
}
```

复杂度：($O(n^3)$)

#### 3. 最优二叉搜索树 (OBST)

给定排序好的键 $k_1 < k_2 < \dots < k_n$ 及其搜索频率 ( f[i] )，
构建一棵期望搜索成本最小的二叉搜索树。

访问频率越高的节点应离根越近。

#### A. 状态

`dp[i][j]` = 用键 `i..j` 构建 BST 的最小成本
`sum[i][j]` = 从 i 到 j 的频率之和（预计算）

#### B. 递推关系

$$
dp[i][j] = \min_{k=i}^{j} (dp[i][k-1] + dp[k+1][j] + sum[i][j])
$$

每个根节点会使其子树深度加一 → 额外成本 = `sum[i][j]`

#### C. 实现

```c
int optimal_bst(int freq[], int n) {
    int dp[n][n], sum[n][n];
    for (int i = 0; i < n; i++) {
        dp[i][i] = freq[i];
        sum[i][i] = freq[i];
        for (int j = i+1; j < n; j++)
            sum[i][j] = sum[i][j-1] + freq[j];
    }
    for (int len = 2; len <= n; len++) {
        for (int i = 0; i+len-1 < n; i++) {
            int j = i + len - 1;
            dp[i][j] = INT_MAX;
            for (int r = i; r <= j; r++) {
                int left = (r > i) ? dp[i][r-1] : 0;
                int right = (r < j) ? dp[r+1][j] : 0;
                dp[i][j] = min(dp[i][j], left + right + sum[i][j]);
            }
        }
    }
    return dp[0][n-1];
}
```

复杂度：($O(n^3)$)

#### 4. 合并模式问题

许多问题——合并文件、连接绳索、霍夫曼编码——都涉及以最小总成本重复合并元素。

它们都遵循这个模板：
$$
dp[i][j] = \min_{k} (dp[i][k] + dp[k+1][j] + \text{合并成本})
$$

与 MCM 结构相同。

#### 5. 关键模式：区间动态规划

状态：`dp[i][j] = 子数组 [i..j] 的最佳答案`
转移：尝试 `i` 和 `j` 之间的所有分割点 `k`

模板：

```c
for (len = 2; len <= n; len++)
 for (i = 0; i + len - 1 < n; i++) {
    j = i + len - 1;
    dp[i][j] = INF;
    for (k = i; k < j; k++)
       dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + cost(i,j,k));
 }
```

#### 6. 总结

| 问题                  | 状态      | 递推关系                                      | 复杂度     |
| --------------------- | --------- | --------------------------------------------- | ---------- |
| MCM                   | dp[i][j]  | min(dp[i][k]+dp[k+1][j]+p[i-1]*p[k]*p[j])     | O(n³)      |
| 多边形三角剖分        | dp[i][j]  | min(dp[i][k]+dp[k][j]+cost)                   | O(n³)      |
| OBST                  | dp[i][j]  | min(dp[i][k-1]+dp[k+1][j]+sum[i][j])          | O(n³)      |
| 合并问题              | dp[i][j]  | min(dp[i][k]+dp[k+1][j]+合并成本)             | O(n³)      |

#### 精简代码

矩阵链乘法（紧凑版）：

```c
for (len = 2; len < n; len++)
  for (i = 1; i + len - 1 < n; i++) {
    j = i + len - 1; dp[i][j] = INF;
    for (k = i; k < j; k++)
      dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + p[i-1]*p[k]*p[j]);
  }
```

#### 为何重要

这些问题本质上是二维动态规划——在区间和分割点上进行推理。它们训练你在每个可能的点“切割问题”的能力。

> “在每一个起点和终点之间，都存在一个如何分割的选择。”

#### 动手实践

1.  实现 MCM 并打印括号化方案。
2.  解决带边权重的多边形三角剖分问题。
3.  为频率 [34, 8, 50] 构建 OBST。
4.  可视化对角线填充的动态规划表。
5.  推广到一次合并 `k` 个段。

掌握这些，你将能在解析、合并乃至人工智能规划中识别出隐藏的区间动态规划模式。
### 45. 状态压缩动态规划与旅行商问题

某些动态规划问题需要你追踪哪些物品已被使用，或者哪些元素子集在给定时刻是活跃的。这正是状态压缩动态规划（Bitmask DP）的用武之地。它通过二进制掩码编码子集，使你能够高效地表示状态空间。

这项技术是以下领域的必备知识：
- 旅行商问题
- 子集覆盖/访问问题
- 集合的排列与组合
- 游戏状态与开关切换

#### 1. 状态压缩动态规划的思想

状态压缩（bitmask）是一个整数，其二进制表示编码了一个子集。

对于 `n` 个元素：
- 存在 $2^n$ 个子集。
- 一个子集由一个从 `0` 到 `(1 << n) - 1` 的掩码表示。

以 `n = 4` 为例：

| 子集      | 掩码（二进制） | 掩码（十进制） |
| --------- | -------------- | -------------- |
| ∅         | 0000           | 0              |
| {0}       | 0001           | 1              |
| {1}       | 0010           | 2              |
| {0, 1, 3} | 1011           | 11             |

我们可以检查元素是否在子集中：
- `mask & (1 << i)` → 判断元素 `i` 是否在子集中

我们可以添加元素：
- `mask | (1 << i)` → 添加元素 `i`

我们可以移除元素：
- `mask & ~(1 << i)` → 移除元素 `i`

#### 2. 示例：旅行商问题

问题：
给定 `n` 个城市和成本矩阵 `cost[i][j]`，
找到一个哈密顿回路，该回路恰好访问所有城市一次并返回起点，且总成本最小。

#### A. 状态定义

`dp[mask][i]` = 访问了子集 `mask` 中的城市后，到达城市 `i` 的最小成本

- `mask` → 已访问城市的集合
- `i` → 当前所在城市

#### B. 基础情况

`dp[1<<0][0] = 0` （从城市 0 出发，仅访问了城市 0）

#### C. 状态转移

对于每个子集 `mask` 和 `mask` 中的城市 `i`，
尝试从 `i` 移动到不在 `mask` 中的城市 `j`：

$$
dp[mask \cup (1 << j)][j] = \min \big(dp[mask \cup (1 << j)][j], dp[mask][i] + cost[i][j]\big)
$$

#### D. 实现

```c
int tsp(int n, int cost[20][20]) {
    int N = 1 << n;
    const int INF = 1e9;
    int dp[N][n];
    for (int m = 0; m < N; m++)
        for (int i = 0; i < n; i++)
            dp[m][i] = INF;

    dp[1][0] = 0; // 从城市 0 出发

    for (int mask = 1; mask < N; mask++) {
        for (int i = 0; i < n; i++) {
            if (!(mask & (1 << i))) continue; // 如果 i 不在掩码中，跳过
            for (int j = 0; j < n; j++) {
                if (mask & (1 << j)) continue; // 如果 j 已在掩码中，跳过
                int next = mask | (1 << j);
                dp[next][j] = min(dp[next][j], dp[mask][i] + cost[i][j]);
            }
        }
    }

    int ans = INF;
    for (int i = 1; i < n; i++)
        ans = min(ans, dp[N-1][i] + cost[i][0]); // 返回起点城市 0
    return ans;
}
```

复杂度：
- 状态数：$O(n \cdot 2^n)$
- 每个状态的转移：$O(n)$
- 总计：$O(n^2 \cdot 2^n)$

#### E. 示例

```
n = 4
cost = {
 {0, 10, 15, 20},
 {10, 0, 35, 25},
 {15, 35, 0, 30},
 {20, 25, 30, 0}
}
```

最优路径：0 → 1 → 3 → 2 → 0
成本 = 80

#### 3. 其他常见的状态压缩动态规划模式

1.  **子集和 / 划分**
    `dp[mask] = true` 如果掩码 `mask` 所代表的子集满足特定性质。

2.  **计算置位比特数**
    `__builtin_popcount(mask)` 返回子集中元素的数量。

3.  **遍历所有子掩码**

```c
for (int sub = mask; sub; sub = (sub-1) & mask)
    // 处理子集 sub
```

4.  **分配任务（分配问题）**
    - 每个掩码代表已分配工人的集合。
    - 状态：`dp[mask] = 完成已分配任务的最小成本`。

```c
for (mask) for (task)
 if (!(mask & (1 << task)))
   dp[mask | (1 << task)] = min(dp[mask | (1 << task)],
        dp[mask] + cost[__builtin_popcount(mask)][task]);
```

#### 4. 内存优化技巧

- 如果只需要前一个掩码的状态，可以使用滚动数组：
```c
dp[next][j] = ...
swap(dp, next_dp)
```

- 压缩维度：对于较小的 `n`，使用 $O(2^n)$ 的内存。

#### 5. 总结

| 问题类型   | 状态定义      | 状态转移                           | 复杂度        |
| ---------- | ------------- | ---------------------------------- | ------------- |
| 旅行商问题 | dp[mask][i]   | min(dp[mask][i] + cost[i][j])      | $O(n^2 \cdot 2^n)$ |
| 分配问题   | dp[mask]      | 添加一个新元素                     | $O(n^2 \cdot 2^n)$ |
| 子集和问题 | dp[mask]      | 有效子集的并集                     | $O(2^n \cdot n)$ |

#### 核心代码片段

核心状态转移：

```c
for (mask)
  for (i)
    if (mask & (1<<i))
      for (j)
        if (!(mask & (1<<j)))
          dp[mask|(1<<j)][j] = min(dp[mask|(1<<j)][j], dp[mask][i] + cost[i][j]);
```

#### 为何重要

状态压缩动态规划是你高效枚举子集的方式。它连接了组合数学与优化，以可管理的常数复杂度解决指数级问题。

> "每个子集都是一个故事，而比特就是它的字母表。"

#### 动手实践

1.  求解 4 个城市的旅行商问题（手动追踪表格）。
2.  使用状态压缩动态规划实现分配问题。
3.  计算和为偶数的子集数量。
4.  使用状态压缩动态规划找出最大兼容任务集合。
5.  探索如何利用位运算技巧优化内存。

状态压缩动态规划开启了基于子集推理的世界，这是组合优化的基础。
### 46. 数位 DP 与 SOS DP

在某些问题中，你不需要遍历索引或子集，而是遍历数位或掩码，以便在结构化的状态上进行计数或优化。有两种主要类型尤为突出：

- **数位 DP** - 统计具有特定属性的数字（例如，数位和、约束条件）。
- **SOS DP（子集和 DP）** - 高效地计算所有子集上的函数。

当暴力方法需要枚举每个数字或子集（这很快会变得不可能）时，这些是必不可少的技术。

#### 1. 数位 DP（带约束的计数）

数位 DP 用于统计或求和所有满足某个条件且 ≤ N 的数字，例如：

- 数位之和等于目标值。
- 数字不包含禁止的数位。
- 数字具有特定的奇偶性或可除性。

我们不是遍历所有数字（最多可达 10¹⁸！），而是逐位遍历。

#### A. 状态设计

典型的 DP 状态：

`dp[pos][sum][tight][leading_zero]`

- `pos`：当前数位索引（从最高有效位到最低有效位）。
- `sum`：属性跟踪器（例如，数位和、余数）。
- `tight`：我们是否仍然受到 N 的前缀限制。
- `leading_zero`：我们是否已经开始放置非零数位。

#### B. 状态转移

在每个数位位置，我们选择一个数字 `d`：

```c
limit = tight ? (digit at pos in N) : 9
for (d = 0; d <= limit; d++) {
    new_tight = tight && (d == limit)
    new_sum = sum + d
    // 或者 new_mod = (mod * 10 + d) % M
}
```

状态转移在所有有效选择上累积结果。

#### C. 基本情况

当 `pos == len(N)`（数位结束时）：

- 如果条件成立（例如 `sum == target`），则返回 1，否则返回 0。

#### D. 示例：统计 ≤ N 且数位和等于 S 的数字数量

```c
long long dp[20][200][2];

long long solve(string s, int pos, int sum, bool tight) {
    if (pos == s.size()) return sum == 0;
    if (sum < 0) return 0;
    if (dp[pos][sum][tight] != -1) return dp[pos][sum][tight];

    int limit = tight ? (s[pos] - '0') : 9;
    long long res = 0;
    for (int d = 0; d <= limit; d++)
        res += solve(s, pos+1, sum-d, tight && (d==limit));

    return dp[pos][sum][tight] = res;
}
```

用法：

```c
string N = "12345";
int S = 9;
memset(dp, -1, sizeof dp);
cout << solve(N, 0, S, 1);
```

复杂度：
O(数位数量 × 和 × 2) → 通常为 O(20 × 200 × 2)。

#### E. 示例变体

1.  统计能被 3 整除的数字数量
    → 跟踪余数：`new_rem = (rem*10 + d) % 3`。
2.  统计没有连续相同数位的数字数量
    → 在状态中添加 `last_digit`。
3.  统计美丽数字（如回文数、无重复数位）
    → 跟踪已使用数位的位掩码。

#### F. 总结

| 问题                 | 状态                   | 转移               | 复杂度         |
| -------------------- | ---------------------- | ------------------ | -------------- |
| 数位和 = S           | dp[pos][sum][tight]    | sum-d              | O(len·S)       |
| 能被 k 整除          | dp[pos][rem][tight]    | (rem*10+d)%k       | O(len·k)       |
| 无重复数位           | dp[pos][mask][tight]   | mask               | O(len·2¹⁰)     |

#### 精简代码

```c
for (int d = 0; d <= limit; d++)
    res += solve(pos+1, sum-d, tight && (d==limit));
```

#### 2. SOS DP（子集和 DP）

当处理子集上的函数时，我们有时需要计算：

$$
f(S) = \sum_{T \subseteq S} g(T)
$$

朴素方法复杂度为 O(3ⁿ)。SOS DP 将其降低到 O(n·2ⁿ)。

#### A. 设置

初始令 `f[mask] = g[mask]`。
对于每个位 `i`：

```c
for (mask = 0; mask < (1<<n); mask++)
    if (mask & (1<<i))
        f[mask] += f[mask^(1<<i)];
```

在此之后，`f[mask]` = 所有 `sub ⊆ mask` 的 `g[sub]` 之和。

#### B. 示例

给定数组 `a[mask]`，计算 `sum[mask] = sum_{sub ⊆ mask} a[sub]`。

```c
int n = 3;
int N = 1 << n;
int f[N], a[N];
// 初始化 a[]
for (int mask = 0; mask < N; mask++) f[mask] = a[mask];
for (int i = 0; i < n; i++)
  for (int mask = 0; mask < N; mask++)
    if (mask & (1 << i))
        f[mask] += f[mask ^ (1 << i)];
```

#### C. 原理

每次迭代都会添加因一位不同而产生的子集的贡献。
通过处理所有位，每个子集的贡献都会向上传播。

#### D. 变体

- 超集和：反转方向。
- 取最大值而不是求和：将 `+=` 替换为 `max=`。
- 异或卷积：在异或子集关系下组合值。

#### E. 应用

- 加速容斥原理计算。
- 预计算子集统计信息。
- 带有子集转移的掩码 DP。

#### F. 复杂度

| 问题       | 朴素方法 | SOS DP   |
| ---------- | -------- | -------- |
| 子集和     | O(3ⁿ)    | O(n·2ⁿ)  |
| 超集和     | O(3ⁿ)    | O(n·2ⁿ)  |

#### 重要性

数位 DP 教授如何在约束下进行计数，逐位思考。
SOS DP 教授子集传播，高效地传播信息。

它们共同展示了如何利用结构来驯服指数级的状态空间。

> "当搜索空间爆炸时，对称性和结构就是你的指南针。"

#### 动手实践

1.  统计 ≤ 10⁹ 且数位和等于 10 的数字数量。
2.  统计 ≤ 10⁶ 且没有重复数位的数字数量。
3.  对于 n=4，计算 f[mask] = sum_{sub⊆mask} a[sub]。
4.  使用 SOS DP 找出有多少个位子集的位和为偶数。
5.  修改数位 DP 以显式处理前导零。

掌握这些，你就能优雅而快速地处理结构化的指数级问题。
### 47. 动态规划优化（分治、凸包技巧、Knuth）

动态规划通常从一个简单的递推关系开始，但朴素的实现可能太慢（例如 ( O$n^2$ ) 或更糟）。
当递推关系具有特殊结构时，例如单调性或凸性，我们可以利用它来大幅降低时间复杂度。

本章介绍三种强大的优化方法：

1. 分治动态规划
2. 凸包技巧
3. Knuth 优化

每一种都基于发现隐藏在状态转移中的顺序或几何结构。

#### 1. 分治优化

如果你有一个像这样的递推关系：
$$
dp[i] = \min_{k < i} { dp[k] + C(k, i) }
$$

并且对于 dp[i] 的最优 k ≤ 对于 dp[i+1] 的最优 k，
你可以使用分治法在 ( O$n \log n$ ) 或 ( O$n \log^2 n$ ) 内计算 dp。

这个性质被称为 argmin 的单调性。

#### A. 条件

令 ( C(k, i) ) 为从状态 ( k ) 转移到状态 ( i ) 的代价。
分治优化适用于以下情况：

$$
opt(i) \le opt(i+1)
$$

并且 ( C ) 满足四边形不等式（或类似的凸结构）。

#### B. 模板

```c
void compute(int l, int r, int optL, int optR) {
    if (l > r) return;
    int mid = (l + r) / 2;
    pair<long long,int> best = {INF, -1};
    for (int k = optL; k <= min(mid, optR); k++) {
        long long val = dp_prev[k] + cost(k, mid);
        if (val < best.first) best = {val, k};
    }
    dp[mid] = best.first;
    int opt = best.second;
    compute(l, mid-1, optL, opt);
    compute(mid+1, r, opt, optR);
}
```

调用方式如下：

```c
compute(1, n, 0, n-1);
```

#### C. 示例：将数组划分为 K 段

给定数组 `a[1..n]`，将其划分为 `k` 部分以最小化
$$
dp[i][k] = \min_{j < i} dp[j][k-1] + cost(j+1, i)
$$
如果代价函数满足四边形不等式，你可以在 ( O$n \log n$ ) 内优化每一层。

#### D. 复杂度

朴素： ( O$n^2$ ) → 优化后： ( O$n \log n$ )

#### 2. 凸包技巧

适用于 DP 递推关系在 i 和 k 上是线性的情况：
$$
dp[i] = \min_{k < i} (m_k \cdot x_i + b_k)
$$

其中：

- $m_k$ 是斜率（依赖于 k）
- ( b_k = dp[k] + c(k) )
- $x_i$ 是已知的（单调的）

你可以在一个凸包中维护直线 $y = m_k x + b_k$，并高效地查询最小值。

#### A. 条件

- 斜率 $m_k$ 是单调的（递增或递减）
- 查询点 $x_i$ 是排序好的

如果两者都单调，我们可以在每次查询 O(1) 的摊销复杂度内使用指针移动。
否则，使用李超线段树（O(log n)）。

#### B. 实现（单调斜率）

```c
struct Line { long long m, b; };
deque<Line> hull;

bool bad(Line l1, Line l2, Line l3) {
    return (l3.b - l1.b)*(l1.m - l2.m) <= (l2.b - l1.b)*(l1.m - l3.m);
}

void add(long long m, long long b) {
    Line l = {m, b};
    while (hull.size() >= 2 && bad(hull[hull.size()-2], hull.back(), l))
        hull.pop_back();
    hull.push_back(l);
}

long long query(long long x) {
    while (hull.size() >= 2 && 
          hull[0].m*x + hull[0].b >= hull[1].m*x + hull[1].b)
        hull.pop_front();
    return hull.front().m*x + hull.front().b;
}
```

#### C. 示例：基于线性递推关系的 DP

$$
dp[i] = a_i^2 + \min_{j < i} (dp[j] + b_j \cdot a_i)
$$
这里 $m_j = b_j$, $x_i = a_i$, $b_j = dp[j]$

#### D. 复杂度

- 朴素： ( O$n^2$ )
- CHT： ( O(n) ) 或 ( O$n \log n$ )

#### 3. Knuth 优化

用于区间 DP 问题（如矩阵链乘法、合并石子）。

如果：

1. $dp[i][j] = \min_{k=i}^{j-1} (dp[i][k] + dp[k+1][j] + w(i,j))$
2. 代价函数 $w(i,j)$ 满足四边形不等式：
$$
w(a,c) + w(b,d) \le w(a,d) + w(b,c)
$$
3. 并且满足单调性条件：
$$
opt[i][j-1] \le opt[i][j] \le opt[i+1][j]
$$

那么你可以将每个单元格的搜索空间从 ( O(n) ) 减少到 ( O(1) )，
使得总复杂度从 ( O$n^3$ ) 变为 ( O$n^2$ )。

#### A. 实现

```c
for (int len = 2; len <= n; len++) {
    for (int i = 1; i + len - 1 <= n; i++) {
        int j = i + len - 1;
        dp[i][j] = INF;
        for (int k = opt[i][j-1]; k <= opt[i+1][j]; k++) {
            long long val = dp[i][k] + dp[k+1][j] + cost(i,j);
            if (val < dp[i][j]) {
                dp[i][j] = val;
                opt[i][j] = k;
            }
        }
    }
}
```

#### B. 示例

最优二叉搜索树或合并石子（具有可加性代价）。
典型的改进： ( O$n^3$ \to O$n^2$ )

#### 4. 总结

| 技术                | 适用于             | 关键性质               | 复杂度             |
| ------------------- | ------------------ | ---------------------- | ------------------ |
| 分治动态规划        | 一维状态转移       | 最优决策点单调性       | O(n log n)         |
| 凸包技巧            | 线性转移           | 斜率单调               | O(n) / O(n log n)  |
| Knuth 优化          | 区间动态规划       | 四边形不等式 + 单调性  | O(n²)              |

#### 微型代码

分治模板

```c
void compute(int l, int r, int optL, int optR);
```

CHT 查询

```c
while (size>=2 && f[1](x) < f[0](x)) pop_front();
```

#### 为何重要

这些优化表明，动态规划不仅仅是带有记忆的暴力搜索，
它更是对结构进行数学推理。

一旦你发现了单调性或线性，你就可以将二次方复杂度的解决方案压缩到接近线性的时间。

> "优化是一门发现隐藏在结构中的简洁性的艺术。"

#### 亲自尝试

1. 使用 Knuth 优化优化矩阵链乘法 DP。
2. 在 `dp[i] = min_{k<i}(dp[k]+(i-k)^2)` 上应用分治优化。
3. 使用 CHT 解决具有凸代价函数的斜率 DP。
4. 在随机数据上比较优化后与朴素 DP 的运行时间。
5. 为你自定义的递推关系推导最优决策点单调性的条件。

掌握这些技术，你将能把你的动态规划从缓慢的原型转变为闪电般快速的解决方案。
### 48. 树形动态规划与换根技术

树形动态规划是算法设计中最优雅、最强大的模式之一。与线性数组或网格不同，树形结构是层次化的，其中每个节点依赖于其子节点或父节点。树形动态规划教你如何在树上自底向上或自顶向下地聚合结果，处理子树间相互影响的问题。

本节将涵盖：

1. 基础树形动态规划（有根树）
2. 基于子节点的动态规划（自底向上聚合）
3. 换根技术（自顶向下传播）
4. 常见应用与示例

#### 1. 基础树形动态规划：核心思想

我们定义 `dp[u]` 来表示以 `u` 为根的子树的某个属性。然后我们组合子节点的结果来计算 `dp[u]`。

这种自底向上的方法类似于后序遍历。

示例结构：

```
function dfs(u, parent):
    dp[u] = base_value
    for v in adj[u]:
        if v == parent: continue
        dfs(v, u)
        dp[u] = combine(dp[u], dp[v])
```

#### 示例 1：子树大小

令 `dp[u] = 以 u 为根的子树中的节点数`

```c
void dfs(int u, int p) {
    dp[u] = 1;
    for (int v : adj[u]) {
        if (v == p) continue;
        dfs(v, u);
        dp[u] += dp[v];
    }
}
```

核心思想：组合子节点的大小以得到父节点的大小。
复杂度：$O(n)$

#### 示例 2：树的高度

令 `dp[u] = 以 u 为根的子树的高度`

```c
void dfs(int u, int p) {
    dp[u] = 0;
    for (int v : adj[u]) {
        if (v == p) continue;
        dfs(v, u);
        dp[u] = max(dp[u], 1 + dp[v]);
    }
}
```

这给出了以任意节点为根时的高度。

#### 2. 基于子节点的动态规划（自底向上聚合）

树形动态规划的核心在于合并子节点。

例如，如果你想计算染色方案数或独立集数量，你需要计算子节点的动态规划结果，并在父节点处合并结果。

#### 示例 3：统计独立集数量

在树中，独立集是指任意两个节点都不相邻的节点集合。

状态定义：

- `dp[u][0]` = 不选择节点 `u` 时的方案数
- `dp[u][1]` = 选择节点 `u` 时的方案数

递推关系：
$$
dp[u][0] = \prod_{v \in children(u)} (dp[v][0] + dp[v][1])
$$

$$
dp[u][1] = \prod_{v \in children(u)} dp[v][0]
$$

实现：

```c
void dfs(int u, int p) {
    dp[u][0] = dp[u][1] = 1;
    for (int v : adj[u]) {
        if (v == p) continue;
        dfs(v, u);
        dp[u][0] *= (dp[v][0] + dp[v][1]);
        dp[u][1] *= dp[v][0];
    }
}
```

最终答案 = `dp[root][0] + dp[root][1]`

#### 示例 4：树中的最大路径和

令 `dp[u]` = 从 `u` 开始向下延伸的最大路径和。
为了找到任意位置的最佳路径，需要存储一个基于子节点对的全局最大值。

```c
int ans = 0;
int dfs(int u, int p) {
    int best1 = 0, best2 = 0;
    for (int v : adj[u]) {
        if (v == p) continue;
        int val = dfs(v, u) + weight(u, v);
        if (val > best1) swap(best1, val);
        if (val > best2) best2 = val;
    }
    ans = max(ans, best1 + best2);
    return best1;
}
```

这可以用于求解树的直径或最大路径和。

#### 3. 换根技术

换根动态规划允许你计算以每个节点为根时的答案，而无需从头重新计算 $O(n^2)$ 次。它也被称为带换根的树形动态规划。

#### 核心思想

1.  首先，计算 `dp_down[u]` = 以 `u` 为根时，其子树的答案。
2.  然后，将信息从父节点传播到子节点（`dp_up[u]`），这样每个节点都能获得其子树之外的信息。
3.  结合 `dp_down` 和 `dp_up` 得到 `dp_all[u]`。

#### 示例 5：每个节点到所有节点的距离之和

让我们找到 `ans[u]` = 从节点 `u` 到所有节点的距离之和。

1.  将树以节点 0 为根。
2.  计算子树大小和从根节点出发的总距离。
3.  使用父节点信息进行换根以调整距离。

步骤 1：自底向上：

```c
void dfs1(int u, int p) {
    sz[u] = 1;
    for (int v : adj[u]) {
        if (v == p) continue;
        dfs1(v, u);
        sz[u] += sz[v];
        dp[u] += dp[v] + sz[v];
    }
}
```

步骤 2：自顶向下：

```c
void dfs2(int u, int p) {
    for (int v : adj[u]) {
        if (v == p) continue;
        dp[v] = dp[u] - sz[v] + (n - sz[v]);
        dfs2(v, u);
    }
}
```

在 `dfs2` 之后，`dp[u]` 即为从节点 `u` 出发的距离之和。

复杂度：$O(n)$

#### 4. 通用换根模板

```c
// 1. 后序遍历：根据子节点计算 dp_down[u]
void dfs_down(u, p):
    dp_down[u] = base
    for v in adj[u]:
        if v != p:
            dfs_down(v, u)
            dp_down[u] = merge(dp_down[u], dp_down[v])

// 2. 前序遍历：使用父节点的 dp_up 计算 dp_all[u]
void dfs_up(u, p, dp_up_parent):
    ans[u] = merge(dp_down[u], dp_up_parent)
    prefix, suffix = prefix products of children
    for each child v:
        dp_up_v = merge(prefix[v-1], suffix[v+1], dp_up_parent)
        dfs_up(v, u, dp_up_v)
```

此模板将换根技术推广到许多问题：

-   从每个节点出发的最大距离
-   选择子树的方案数
-   从每个根节点视角看到的子树大小之和

#### 5. 总结

| 模式               | 描述                               | 复杂度       |
| ------------------ | ---------------------------------- | ------------ |
| 基础树形动态规划   | 组合子节点结果                     | $O(n)$       |
| 基于子节点的动态规划 | 每个节点依赖于其子节点             | $O(n)$       |
| 换根动态规划       | 计算以每个节点为根时的结果         | $O(n)$       |
| 多状态动态规划     | 追踪选择（例如包含/排除）          | $O(n·state)$ |

#### 精简代码

子树大小

```c
void dfs(int u, int p) {
    dp[u] = 1;
    for (int v: adj[u]) if (v != p) {
        dfs(v,u);
        dp[u] += dp[v];
    }
}
```

换根求距离和

```c
dp[v] = dp[u] - sz[v] + (n - sz[v]);
```

#### 为何重要

树形动态规划是我们对结构进行递归思考的方式，每个节点的真相都从其子节点中浮现。换根技术将这一思想扩展到全局，赋予每个节点其自身的视角。

> "在状态的森林中，每个根都看到一个不同的世界，然而它们都遵循着相同的法则。"

#### 动手实践

1.  计算每个子树中的节点数。
2.  计算从每个节点出发的深度之和。
3.  使用动态规划求树的直径。
4.  计算独立集数量，结果对 $1e9+7$ 取模。
5.  实现换根算法，找出从每个节点出发的最大距离。

树形动态规划将递归模式转化为处理层次化数据的通用策略。
### 49. 动态规划的重建与回溯

到目前为止，我们一直专注于计算最优值（最小成本、最高分数、方案数量）。
但在大多数实际问题中，我们不仅仅想要那个数字，我们还想知道是如何得到它的。

这就是重建的作用：一旦你填满了动态规划表，你就可以回溯导致最优答案的决策过程。

本章将向你展示如何：

1.  在动态规划计算过程中记录转移
2.  重建路径、子集或序列
3.  处理多重重建（路径、集合、父链接）
4.  理解一维、二维和基于图的动态规划中的回溯

#### 1. 核心理念

每个动态规划状态都源于一个选择。
如果你存储了*哪个选择是最优的*，你就可以从最终状态向后走，重建解决方案。

可以这样理解：

```
dp[i] = 所有选项中的最优值
choice[i] = 最优选项的索引（argmin 或 argmax）
```

然后：

```
reconstruction_path = []
i = n
while i > 0:
    reconstruction_path.push(choice[i])
    i = choice[i].prev
```

你不仅仅是在求解，你还在记住路径。

#### 2. 一维动态规划中的重建

#### 示例：零钱兑换（最少硬币数）

问题：
找出凑成价值 `n` 所需的最少硬币数量。

递推关系：
$$
dp[x] = 1 + \min_{c \in coins, c \le x} dp[x-c]
$$

为了重建使用了哪些硬币：

```c
int dp[MAXN], prev_coin[MAXN];
dp[0] = 0;
for (int x = 1; x <= n; x++) {
    dp[x] = INF;
    for (int c : coins) {
        if (x >= c && dp[x-c] + 1 < dp[x]) {
            dp[x] = dp[x-c] + 1;
            prev_coin[x] = c;
        }
    }
}
```

重建：

```c
vector<int> used;
int cur = n;
while (cur > 0) {
    used.push_back(prev_coin[cur]);
    cur -= prev_coin[cur];
}
```

输出：一个最优解中使用的硬币序列。

#### 示例：最长递增子序列（LIS）重建

你已经知道如何找到 LIS 的长度。现在来重建序列本身。

```c
int dp[n], prev[n];
int best_end = 0;
for (int i = 0; i < n; i++) {
    dp[i] = 1; prev[i] = -1;
    for (int j = 0; j < i; j++)
        if (a[j] < a[i] && dp[j] + 1 > dp[i]) {
            dp[i] = dp[j] + 1;
            prev[i] = j;
        }
    if (dp[i] > dp[best_end]) best_end = i;
}
```

重建 LIS：

```c
vector<int> lis;
for (int i = best_end; i != -1; i = prev[i])
    lis.push_back(a[i]);
reverse(lis.begin(), lis.end());
```

#### 3. 二维动态规划中的重建

#### 示例：最长公共子序列（LCS）

我们使用以下公式填充 `dp[i][j]`：

$$
dp[i][j] =
\begin{cases}
dp[i-1][j-1] + 1, & \text{if } a[i-1] = b[j-1], \\
\max(dp[i-1][j], dp[i][j-1]), & \text{otherwise.}
\end{cases}
$$

为了重建 LCS：

```c
int i = n, j = m;
string lcs = "";
while (i > 0 && j > 0) {
    if (a[i-1] == b[j-1]) {
        lcs.push_back(a[i-1]);
        i--; j--;
    }
    else if (dp[i-1][j] > dp[i][j-1]) i--;
    else j--;
}
reverse(lcs.begin(), lcs.end());
```

输出：一个有效的 LCS 字符串。

#### 示例：编辑距离

操作：插入、删除、替换。

你可以存储操作类型：

```c
if (a[i-1] == b[j-1]) op[i][j] = "match";
else if (dp[i][j] == dp[i-1][j-1] + 1) op[i][j] = "replace";
else if (dp[i][j] == dp[i-1][j] + 1) op[i][j] = "delete";
else op[i][j] = "insert";
```

然后回溯以列出操作序列：

```c
while (i > 0 || j > 0) {
    if (op[i][j] == "match") i--, j--;
    else if (op[i][j] == "replace") { print("Replace"); i--; j--; }
    else if (op[i][j] == "delete") { print("Delete"); i--; }
    else { print("Insert"); j--; }
}
```

#### 4. 路径问题中的重建

当动态规划追踪最短路径时，你可以保留父指针。

#### 示例：Bellman-Ford 算法路径重建

```c
int dist[n], parent[n];
dist[src] = 0;
for (int k = 0; k < n-1; k++)
  for (auto [u,v,w] : edges)
    if (dist[u] + w < dist[v]) {
        dist[v] = dist[u] + w;
        parent[v] = u;
    }

vector<int> path;
for (int v = dest; v != src; v = parent[v])
    path.push_back(v);
path.push_back(src);
reverse(path.begin(), path.end());
```

现在你就得到了实际的最短路径。

#### 5. 处理多重解

有时存在多个最优路径。
你可以：

*   存储所有前驱状态，而不是一个
*   递归回溯以枚举所有解
*   确定性地打破平局（例如，字典序最小的解）

示例：

```c
if (new_val == dp[i]) parents[i].push_back(j);
```

然后递归生成所有可能的路径。

#### 6. 可视化

动态规划的重建通常看起来像是在网格或图中沿着箭头走：

*   LCS：对角线（↖）、向上（↑）、向左（←）
*   最短路径：父边
*   LIS：前驱链

你是在遍历决策过程，而不仅仅是数字。

#### 7. 总结

| 类型        | 状态         | 重建方法           |
| ----------- | ------------ | ------------------ |
| 一维动态规划 | `prev[i]`    | 追踪链             |
| 二维动态规划 | `op[i][j]`   | 跟随选择           |
| 图动态规划   | `parent[v]`  | 跟随边             |
| 计数动态规划 | 可选         | 恢复计数 / 路径    |

#### 微型代码

通用模式：

```c
for (state)
  for (choice)
    if (better) {
        dp[state] = value;
        parent[state] = choice;
    }
```

然后：

```c
while (state != base) {
    path.push_back(parent[state]);
    state = parent[state];
}
```

#### 为什么这很重要

解决动态规划问题让你得到分数，而重建则向你展示故事。
这是知道答案和理解推理过程之间的区别。

> "数字告诉你结果；指针告诉你路径。"

#### 亲自尝试

1.  重建一条 LIS 路径。
2.  为小字符串打印所有 LCS。
3.  展示将 "cat" 转换为 "cut" 的编辑操作。
4.  追踪在背包问题中用于达到精确重量的子集。
5.  在矩阵链乘法动态规划中恢复最优合并顺序。

重建将动态规划从一个静态表格转变为决策的叙事，一张穿越最优选择迷宫的路线图。
### 50. 元动态规划与优化模板

我们已经探索了动态规划的多种形式，包括序列、网格、树、图、子集和数位上的动态规划。动态规划篇章的最后一章将视角提升到*元级别*：如何识别动态规划模式、将其泛化并转化为可复用的模板。

如果说经典动态规划是关于解决单个问题，那么元动态规划则是关于识别共享结构的*问题族*。你将学习如何构建自己的动态规划框架、使用通用模板以及从基本原理出发进行推理。

#### 1. 什么是元动态规划？

*元动态规划* 是对动态规划模式的高层抽象。它编码了：

- 状态定义模式
- 转移模式
- 优化结构
- 维度依赖关系

通过学习这些模式，你可以更快地设计动态规划方案，在不同问题间复用逻辑，并及早发现优化点。

可以将元动态规划理解为：

> "与其记忆 100 个动态规划问题，不如掌握 10 个动态规划蓝图。"

#### 2. 四个基本构件

每个动态规划都有相同的核心组成部分：

1.  **状态**：你正在解决的子问题是什么
    - 通常是 `dp[i]`、`dp[i][j]` 或 `dp[mask]`
    - 代表进展的最小单位
2.  **转移**：如何从较小的子问题构建较大的子问题
    - 例如 `dp[i] = min(dp[j] + cost(j, i))`
3.  **基本情况**：已知的平凡答案
    - 例如 `dp[0] = 0`
4.  **顺序**：如何填充状态
    - 例如递增 `i`、递减 `i` 或拓扑序

一旦你能用这四个部分描述一个问题，它*就是*一个动态规划问题。

#### 3. 通用结构的元模板

以下是可供使用和调整的通用模板。

#### A. 线性动态规划（1D 序列）

**形状**：线性递进
**示例**：
- 斐波那契数列
- 背包问题（1D 容量）
- 最长递增子序列（序列依赖）

```c
for (int i = 1; i <= n; i++) {
    dp[i] = base;
    for (int j : transitions(i))
        dp[i] = min(dp[i], dp[j] + cost(j, i));
}
```

**可视化**：
→ → →
每个状态依赖于之前的位置。

#### B. 网格动态规划（2D 空间）

**形状**：网格或矩阵
**示例**：
- 网格中的路径
- 编辑距离
- 带障碍物的路径计数

```c
for (i = 0; i < n; i++)
  for (j = 0; j < m; j++)
    dp[i][j] = combine(dp[i-1][j], dp[i][j-1]);
```

**可视化**：
⬇️ ⬇️
➡️
从左上角移动到右下角。

#### C. 区间动态规划

**形状**：线段或子数组
**示例**：
- 矩阵链乘法
- 最优二叉搜索树
- 合并石子

```c
for (len = 2; len <= n; len++)
  for (i = 0; i + len - 1 < n; i++) {
      j = i + len - 1;
      dp[i][j] = INF;
      for (k = i; k < j; k++)
          dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + cost(i,j));
  }
```

**关键洞察**：重叠区间，分割点。

#### D. 子集动态规划

**形状**：集合的子集
**示例**：
- 旅行商问题
- 分配问题
- SOS 动态规划

```c
for (mask = 0; mask < (1<<n); mask++)
  for (sub = mask; sub; sub = (sub-1)&mask)
      dp[mask] = combine(dp[mask], dp[sub]);
```

**关键洞察**：使用位掩码表示子集。

#### E. 树形动态规划

**形状**：层次依赖关系
**示例**：
- 子树大小
- 独立集
- 换根

```c
void dfs(u, p):
  dp[u] = base
  for (v in children)
    if (v != p)
      dfs(v, u)
      dp[u] = merge(dp[u], dp[v])
```

#### F. 图动态规划（拓扑序）

**形状**：有向无环图结构
**示例**：
- 有向无环图中的最长路径
- 路径计数
- 有向无环图最短路径

```c
for (u in topo_order)
  for (v in adj[u])
    dp[v] = combine(dp[v], dp[u] + weight(u,v));
```

**关键**：按拓扑序处理节点。

#### G. 数位动态规划

**形状**：位置数位，受限转移
**示例**：
- 统计满足数位条件的数字
- 整除性 / 数位和问题

```c
dp[pos][sum][tight] = ∑ dp[next_pos][new_sum][new_tight];
```

#### H. Knuth / 分治 / 凸包优化

**形状**：基于单调或凸转移的优化
**示例**：
- 基于成本的分割
- 基于直线的转移

```c
dp[i] = min_k (dp[k] + cost(k, i))
```

**关键**：`opt[i]` 或 `slope` 中的结构。

#### 4. 识别动态规划类型

询问以下诊断性问题：

| 问题                                         | 线索                 |
| -------------------------------------------- | -------------------- |
| "每一步是否依赖于更小的子问题？"             | 动态规划             |
| "我是否在分割一个区间？"                     | 区间动态规划         |
| "我是否在选择子集？"                         | 子集 / 位掩码动态规划 |
| "我是否沿着位置移动？"                       | 线性动态规划         |
| "我是否在合并子节点？"                       | 树形动态规划         |
| "我是否在有向无环图中处理？"                 | 图动态规划           |
| "我是否在跟踪数位或约束条件？"               | 数位动态规划         |

#### 5. 优化层

一旦你有了一个可行的动态规划方案，请思考：

- 转移是否可以简化（单调性）？
- 重叠成本是否可以缓存（前缀和）？
- 维度是否可以压缩（滚动数组）？
- 是否可以复用每个区间的解（分治 / Knuth）？

这将把你的动态规划从概念性的转变为高效的。

#### 6. 元动态规划：变换

- **压缩维度**：如果只需要 `dp[i-1]`，则使用 1D 数组。
- **反转循环**：自底向上 ↔ 自顶向下。
- **改变基础**：用于区间查询的前缀和。
- **状态提升**：为新的属性（如余数、奇偶性、位掩码）增加维度。

> "卡住时，增加一个维度。太慢时，减少一个维度。"

#### 7. 常见模板片段

**滚动 1D 背包**：

```c
for (c = C; c >= w[i]; c--)
  dp[c] = max(dp[c], dp[c-w[i]] + val[i]);
```

**自顶向下记忆化**：

```c
int solve(state):
  if (visited[state]) return dp[state];
  dp[state] = combine(solve(next_states));
```

**迭代动态规划**：

```c
for (state in order)
  dp[state] = merge(prev_states);
```

#### 8. 构建你自己的动态规划框架

你可以设计一个通用的 `DP(state, transition)` 类：

```c
struct DP {
    vector<long long> dp;
    function<long long(int,int)> cost;
    DP(int n, auto cost): dp(n, INF), cost(cost) {}
    void solve() { for (int i=1; i<n; i++) for (int j=0; j<i; j++) 
         dp[i] = min(dp[i], dp[j] + cost(j, i)); }
};
```

可复用、可读性强、灵活。

#### 9. 总结

| 动态规划类型 | 核心状态     | 形状       | 典型复杂度       |
| ------------ | ------------ | ---------- | ---------------- |
| 线性动态规划 | dp[i]        | 线性       | O(n²) → O(n)     |
| 网格动态规划 | dp[i][j]     | 矩阵       | O(n·m)           |
| 区间动态规划 | dp[i][j]     | 三角形     | O(n³)            |
| 子集动态规划 | dp[mask]     | 指数级     | O(n·2ⁿ)          |
| 树形动态规划 | dp[u]        | 树         | O(n)             |
| 数位动态规划 | dp[pos][sum] | 递归       | O(len·sum)       |
| 图动态规划   | dp[v]        | 有向无环图 | O(V+E)           |

#### 微型代码

```c
for (state in order)
  dp[state] = combine(all_prev(state));
```

#### 为何重要

元动态规划将你的思维从逐个案例转变为按模式思考。你不再记忆公式，而是开始*识别形状*：线、网格、区间、树、掩码。

一旦你能说出形状的名字，你就能写出动态规划。

> "动态规划不是关于填表。它是关于识别结构。"

#### 亲自尝试

1.  将每个经典动态规划问题归类到一种类型。
2.  为每种模式（线性、网格、树等）编写一个模板。
3.  创建一个 `dp_solve(state, transitions)` 函数来泛化逻辑。
4.  针对每种模式，练习一个小例子。
5.  用代码片段构建你自己的"动态规划模式小手册"。

这是你从具体解决方案通往算法流畅性的桥梁，是掌握接下来 950 个算法的基础。

## 第 6 章. 字符串与文本算法
### 51. 数论（最大公约数、模运算、中国剩余定理）

数论构成了许多算法的数学基础，从哈希和密码学到模组合数学和素数测试。在算法问题求解中，核心在于高效地处理整数、整除性和模系统。

本节涵盖基本工具包：

- 最大公约数（GCD）和扩展欧几里得算法
- 模运算（加法、减法、乘法、逆元）
- 模幂运算
- 中国剩余定理（CRT）

#### 1. 最大公约数（GCD）

两个整数 $a$ 和 $b$ 的最大公约数，记作 $\gcd(a, b)$，是能同时整除两者的最大整数。它是分数简化、丢番图方程和模逆元的基石。

#### A. 欧几里得算法

基于：
$$
\gcd(a, b) = \gcd(b, a \bmod b)
$$

```c
int gcd(int a, int b) {
    return b == 0 ? a : gcd(b, a % b);
}
```

时间复杂度：$O(\log \min(a,b))$

#### B. 扩展欧几里得算法

寻找整数 $( x, y )$ 使得：
$$
ax + by = \gcd(a, b)
$$

这对于寻找模逆元至关重要。

```c
int ext_gcd(int a, int b, int &x, int &y) {
    if (b == 0) {
        x = 1; y = 0;
        return a;
    }
    int x1, y1;
    int g = ext_gcd(b, a % b, x1, y1);
    x = y1;
    y = x1 - (a / b) * y1;
    return g;
}
```

#### C. 裴蜀定理

如果 $g = \gcd(a,b)$，那么 $ax + by = g$ 有整数解。
如果 $g = 1$，则 $x$ 是 $a$ 模 $b$ 的模逆元。

#### 2. 模运算

模系统在达到某个值 $m$ 后“回绕”。

我们写作：
$$
a \equiv b \pmod{m} \quad \text{如果 } m \mid (a - b)
$$

它的行为类似于普通算术，规则如下：

- $(a + b) \bmod m = ((a \bmod m) + (b \bmod m)) \bmod m$
- $(a \cdot b) \bmod m = ((a \bmod m) \cdot (b \bmod m)) \bmod m$
- $(a - b) \bmod m = ((a \bmod m) - (b \bmod m) + m) \bmod m$

#### A. 模幂运算

使用二进制幂运算高效计算 $a^b \bmod m$。

```c
long long modpow(long long a, long long b, long long m) {
    long long res = 1;
    a %= m;
    while (b > 0) {
        if (b & 1) res = (res * a) % m;
        a = (a * a) % m;
        b >>= 1;
    }
    return res;
}
```

复杂度：$O(\log b)$

#### B. 模逆元

给定 $a$，寻找 $a^{-1}$ 使得：
$$
a \cdot a^{-1} \equiv 1 \pmod{m}
$$

情况 1：如果 $m$ 是素数，使用费马小定理：
$$
a^{-1} \equiv a^{m-2} \pmod{m}
$$

```c
int modinv(int a, int m) {
    return modpow(a, m-2, m);
}
```

情况 2：如果 $a$ 和 $m$ 互质，使用扩展欧几里得算法：

```c
int inv(int a, int m) {
    int x, y;
    int g = ext_gcd(a, m, x, y);
    if (g != 1) return -1; // 无逆元
    return (x % m + m) % m;
}
```

#### C. 模除法

要计算 $a / b \bmod m$：
$$
a / b \equiv a \cdot b^{-1} \pmod{m}
$$

因此，计算逆元并相乘。

#### 3. 中国剩余定理（CRT）

中国剩余定理解同余方程组：
$$
x \equiv a_1 \pmod{m_1}
$$
$$
x \equiv a_2 \pmod{m_2}
$$
如果模数 $m_1, m_2, \dots, m_k$ 两两互质，则存在模 $M = m_1 m_2 \dots m_k$ 下的唯一解。

#### A. 二元方程示例

求解：
$$
x \equiv a_1 \pmod{m_1}, \quad x \equiv a_2 \pmod{m_2}
$$

令：

- $M = m_1 m_2$
- $M_1 = M / m_1$
- $M_2 = M / m_2$

寻找逆元 $inv_1 = M_1^{-1} \bmod m_1$, $inv_2 = M_2^{-1} \bmod m_2$

则：
$$
x = (a_1 \cdot M_1 \cdot inv_1 + a_2 \cdot M_2 \cdot inv_2) \bmod M
$$

#### B. 实现

```c
long long crt(vector<int> a, vector<int> m) {
    long long M = 1;
    for (int mod : m) M *= mod;
    long long res = 0;
    for (int i = 0; i < a.size(); i++) {
        long long Mi = M / m[i];
        long long inv = modinv(Mi % m[i], m[i]);
        res = (res + 1LL * a[i] * Mi % M * inv % M) % M;
    }
    return (res % M + M) % M;
}
```

#### C. 示例

求解：

```
x ≡ 2 (mod 3)
x ≡ 3 (mod 5)
x ≡ 2 (mod 7)
```

解：
$x = 23$ (mod 105)

验证：

- $23 \% 3 = 2$
- $23 \% 5 = 3$
- $23 \% 7 = 2$

#### 4. 精简代码

最大公约数

```c
int gcd(int a, int b) { return b ? gcd(b, a % b) : a; }
```

模幂运算

```c
modpow(a, b, m)
```

模逆元

```c
modinv(a, m)
```

中国剩余定理

```c
crt(a[], m[])
```

#### 5. 总结

| 概念             | 公式                                   | 用途                 |
| ---------------- | -------------------------------------- | -------------------- |
| 最大公约数       | $\gcd(a,b) = \gcd(b, a \bmod b)$     | 简化比例             |
| 扩展欧几里得算法 | $ax + by = \gcd(a,b)$                 | 寻找模逆元           |
| 模逆元           | $a^{-1} \equiv a^{m-2} \pmod{m}$     | 解模方程             |
| 模幂运算         | $a^b \bmod m$                         | 快速幂运算           |
| 中国剩余定理     | 组合同余式                             | 多模数系统           |

#### 为何重要

数论让算法能够使用整数的语言，将巨大的计算转化为模运算的游戏。从哈希到 RSA，从组合数学到密码学，它无处不在。

> "当数字开始回绕，数学就变得模运算化，而算法则变得神奇。"

#### 动手试试

1.  计算 gcd(48, 180)。
2.  求 7 模 13 的逆元。
3.  解 $x ≡ 1 \pmod{2}, x ≡ 2 \pmod{3}, x ≡ 3 \pmod{5}$。
4.  实现模除法 $a / b \bmod m$。
5.  使用 modpow 计算 $3^{200} \bmod 13$。

这些基础知识开启了密码学、组合数学等领域更高级算法的大门。
### 52. 素性判定与因数分解（Miller-Rabin，Pollard Rho）

素性判定和因数分解是数论、密码学和竞技编程的核心。许多现代系统（RSA、ECC）都依赖于大数分解的困难性。
在这里，我们将学习如何测试一个数是否为素数，以及如何高效地将其分解为因数。

我们将涵盖：

- 试除法
- 埃拉托斯特尼筛法（用于预计算）
- 概率性素性测试（Miller-Rabin）
- 整数因数分解（Pollard Rho）

#### 1. 试除法

测试素性的最简单方法是除以所有不超过 √n 的整数。

```c
bool is_prime(long long n) {
    if (n < 2) return false;
    if (n % 2 == 0) return n == 2;
    for (long long i = 3; i * i <= n; i += 2)
        if (n % i == 0) return false;
    return true;
}
```

时间复杂度：$O(\sqrt{n})$
适用于 $n \le 10^6$，对于大的 $n$ 不切实际。

#### 2. 埃拉托斯特尼筛法

要同时检查多个数，请使用筛法。

思想：
从 2 开始，标记每个素数的所有倍数。

```c
vector<bool> sieve(int n) {
    vector<bool> is_prime(n+1, true);
    is_prime[0] = is_prime[1] = false;
    for (int i = 2; i * i <= n; i++)
        if (is_prime[i])
            for (int j = i * i; j <= n; j += i)
                is_prime[j] = false;
    return is_prime;
}
```

时间复杂度：$O(n \log \log n)$

适用于生成不超过 $10^7$ 的素数。

#### 3. 模乘法

在进行概率性测试或因数分解之前，我们需要对大数进行安全的模乘法运算。

```c
long long modmul(long long a, long long b, long long m) {
    __int128 res = (__int128)a * b % m;
    return (long long)res;
}
```

避免 $n \approx 10^{18}$ 时的溢出。

#### 4. Miller-Rabin 素性测试

一种概率性测试，可以在 $O(k \log^3 n)$ 时间内检查 $n$ 是素数还是合数。

思想：
对于一个素数 $n$：
$$
a^{n-1} \equiv 1 \pmod{n}
$$
但对于合数，大多数 $a$ 不满足此式。

我们记 $n - 1 = 2^s \cdot d$，其中 $d$ 为奇数。

对于每个基 $a$：

- 计算 $x = a^d \bmod n$
- 如果 $x = 1$ 或 $x = n - 1$，则通过
- 否则，平方 $s-1$ 次
- 如果没有一次等于 $n - 1$，则为合数

```c
bool miller_rabin(long long n) {
    if (n < 2) return false;
    for (long long p : {2,3,5,7,11,13,17,19,23,29,31,37})
        if (n % p == 0) return n == p;
    long long d = n - 1, s = 0;
    while ((d & 1) == 0) d >>= 1, s++;
    auto modpow = [&](long long a, long long b) {
        long long r = 1;
        while (b) {
            if (b & 1) r = modmul(r, a, n);
            a = modmul(a, a, n);
            b >>= 1;
        }
        return r;
    };
    for (long long a : {2, 325, 9375, 28178, 450775, 9780504, 1795265022}) {
        if (a % n == 0) continue;
        long long x = modpow(a, d);
        if (x == 1 || x == n - 1) continue;
        bool composite = true;
        for (int r = 1; r < s; r++) {
            x = modmul(x, x, n);
            if (x == n - 1) {
                composite = false;
                break;
            }
        }
        if (composite) return false;
    }
    return true;
}
```

对于以下情况是确定性的：

- 使用上述基，$n < 2^{64}$。
  复杂度：$O(k \log^3 n)$

#### 5. Pollard Rho 因数分解

高效用于寻找大合数的非平凡因数。
基于 Floyd 的循环检测（龟兔赛跑算法）。

思想：
定义一个伪随机函数：
$$
f(x) = (x^2 + c) \bmod n
$$
然后计算 $\gcd(|x - y|, n)$，其中 $x, y$ 以不同速度移动。

```c
long long pollard_rho(long long n) {
    if (n % 2 == 0) return 2;
    auto f = [&](long long x, long long c) {
        return (modmul(x, x, n) + c) % n;
    };
    while (true) {
        long long x = rand() % (n - 2) + 2;
        long long y = x;
        long long c = rand() % (n - 1) + 1;
        long long d = 1;
        while (d == 1) {
            x = f(x, c);
            y = f(f(y, c), c);
            d = gcd(abs(x - y), n);
        }
        if (d != n) return d;
    }
}
```

使用步骤：

1. 检查 $n$ 是否为素数（Miller-Rabin）
2. 如果不是，使用 Pollard Rho 寻找一个因数
3. 对因数递归进行

复杂度：平均约 $O(n^{1/4})$

#### 6. 示例

分解 $n = 8051$：

1. Miller-Rabin → 合数
2. Pollard Rho → 因数 83
3. $8051 / 83 = 97$
4. 两者均为素数 ⇒ $8051 = 83 × 97$

#### 7. 精简代码

```c
void factor(long long n, vector<long long> &f) {
    if (n == 1) return;
    if (miller_rabin(n)) {
        f.push_back(n);
        return;
    }
    long long d = pollard_rho(n);
    factor(d, f);
    factor(n / d, f);
}
```

调用 `factor(n, f)` 来获取质因数。

#### 8. 总结

| 算法           | 用途             | 复杂度               | 类型          |
| -------------- | ---------------- | -------------------- | ------------- |
| 试除法         | 小素数判定       | $O(\sqrt{n})$        | 确定性        |
| 筛法           | 预计算素数       | $O(n \log \log n)$   | 确定性        |
| Miller-Rabin   | 素性测试         | $O(k \log^3 n)$      | 概率性        |
| Pollard Rho    | 因数分解         | $O(n^{1/4})$         | 概率性        |

#### 为何重要

现代安全、数论问题以及许多算法谜题都依赖于判断一个数是否为素数，以及当它不是素数时如何快速分解。
这些工具是进入 RSA、模组合数学和高级密码学的起点。

#### 亲自尝试

1.  使用试除法和 Miller-Rabin 检查 97 是否为素数。
2.  分解 5959（应得到 59 × 101）。
3.  使用筛法生成所有 ≤ 100 的素数。
4.  编写一个使用 Pollard Rho + Miller-Rabin 的递归分解器。
5.  测量对于 $n \approx 10^{12}$，$\sqrt{n}$ 试除法和 Pollard Rho 之间的性能差异。

这些技术使得处理大数变得可行，一次分解一个因数。
### 53. 组合数学（排列、组合、子集）

组合数学是计算结构数量的艺术，即我们有多少种方式来排列、选择或分组事物？
在算法中，它无处不在：动态规划的状态转移、路径计数、位掩码枚举以及概率推理。
在这里，我们将构建一个工具包，用于计算阶乘、组合数（nCr）、排列数（nPr）和子集数量，包括精确计算和模运算下的计算。

#### 1. 阶乘与预计算

大多数组合公式都依赖于阶乘：
$$
n! = 1 \times 2 \times 3 \times \dots \times n
$$

为了提高效率，我们可以预先计算它们模 ( m ) 的值（通常 $m = 10^9+7$）。

```c
const int MOD = 1e9 + 7;
const int MAXN = 1e6;
long long fact[MAXN + 1], invfact[MAXN + 1];

long long modpow(long long a, long long b) {
    long long res = 1;
    while (b > 0) {
        if (b & 1) res = res * a % MOD;
        a = a * a % MOD;
        b >>= 1;
    }
    return res;
}

void init_factorials() {
    fact[0] = 1;
    for (int i = 1; i <= MAXN; i++)
        fact[i] = fact[i - 1] * i % MOD;
    invfact[MAXN] = modpow(fact[MAXN], MOD - 2);
    for (int i = MAXN - 1; i >= 0; i--)
        invfact[i] = invfact[i + 1] * (i + 1) % MOD;
}
```

现在你可以在 ( O(1) ) 时间内计算 ( nCr ) 和 ( nPr )。

#### 2. 组合数 ( nCr )

从 ( n ) 个物品中选择 r 个物品的方式数：
$$
C(n, r) = \frac{n!}{r!(n-r)!}
$$

```c
long long nCr(int n, int r) {
    if (r < 0 || r > n) return 0;
    return fact[n] * invfact[r] % MOD * invfact[n - r] % MOD;
}
```

性质：

- $(C(n, 0) = 1),\ (C(n, n) = 1)$
- $C(n, r) = C(n, n - r)$
- 帕斯卡法则：$C(n, r) = C(n - 1, r - 1) + C(n - 1, r)$

#### 示例

( C(5, 2) = 10 )
从一个 5 元素集合中选取 2 个元素有 10 种方式。

#### 3. 排列数 ( nPr )

从 ( n ) 个元素中选择 r 个元素进行排列的方式数：
$$
P(n, r) = \frac{n!}{(n-r)!}
$$

```c
long long nPr(int n, int r) {
    if (r < 0 || r > n) return 0;
    return fact[n] * invfact[n - r] % MOD;
}
```

#### 示例

( P(5, 2) = 20 )
从 5 个元素中选择 2 个并进行排列，共有 20 种顺序。

#### 4. 子集与幂集

每个元素有 2 种选择：包含或不包含。
因此，子集的数量为：
$$
2^n
$$

```c
long long subsets_count(int n) {
    return modpow(2, n);
}
```

使用位掩码枚举子集：

```c
for (int mask = 0; mask < (1 << n); mask++) {
    for (int i = 0; i < n; i++)
        if (mask & (1 << i))
            ; // 包含元素 i
}
```

总计：$2^n$ 个子集，枚举时间复杂度为 ( O$n2^n$ )。

#### 5. 多重集与重复

从 ( n ) 个物品中选择 ( r ) 个物品（允许重复）的方式数：
$$
C(n + r - 1, r)
$$

例如，将 5 块糖果分给 3 个孩子（每个孩子可以得 0 块）的方式数：
( C(3+5-1, 5) = C(7,5) = 21 )

#### 6. 模组合数学

当模 ( p ) 运算时：
- 使用模逆元进行除法运算。
- $C(n, r) \bmod p = fact[n] \cdot invfact[r] \cdot invfact[n - r] \bmod p$

当 $n \ge p$ 时，使用卢卡斯定理：
$$
C(n, r) \bmod p = C(n/p, r/p) \cdot C(n%p, r%p) \bmod p
$$

#### 7. 斯特林数与贝尔数（进阶）

- 第二类斯特林数：将 ( n ) 个物品划分为 ( k ) 个非空子集的方式数
$$
S(n,k) = k \cdot S(n-1,k) + S(n-1,k-1)
$$
- 贝尔数：划分的总数
$$
B(n) = \sum_{k=0}^{n} S(n,k)
$$

用于集合划分和分组问题。

#### 8. 精简代码

```c
init_factorials();
printf("%lld\n", nCr(10, 3));  // 120
printf("%lld\n", nPr(10, 3));  // 720
printf("%lld\n", subsets_count(5)); // 32
```

#### 9. 总结

| 概念         | 公式                                   | 含义             | 示例            |
| ------------ | -------------------------------------- | ---------------- | --------------- |
| 阶乘         | $n!$                                   | 所有排列         | $5! = 120$      |
| 组合数       | $C(n, r) = \frac{n!}{r!(n - r)!}$      | 选择             | $C(5, 2) = 10$  |
| 排列数       | $P(n, r) = \frac{n!}{(n - r)!}$        | 排列             | $P(5, 2) = 20$  |
| 子集         | $2^n$                                  | 所有组合         | $2^3 = 8$       |
| 多重集组合数 | $C(n + r - 1, r)$                      | 允许重复         | $C(4, 2) = 6$   |

#### 为何重要

组合数学是概率论、动态规划计数和模运算问题的基础。
如果不能正确地计算可能性，就无法精通竞技编程或算法设计。
它教会我们如何从选择中产生结构，以及如何高效地计数。

#### 动手实践

1.  计算 $C(1000, 500) \bmod (10^9 + 7)$。
2.  计算恰好有 3 位为 1 的 5 位子集数量，即 $C(5, 3)$。
3.  编写一个循环来打印集合 `{a, b, c, d}` 的所有子集。
4.  使用卢卡斯定理计算 $C(10^6, 1000) \bmod 13$。
5.  实现斯特林数递归并打印 $S(5, 2)$。

每一个算法计数技巧，从帕斯卡三角形到二项式定理，都从这里开始。
### 54. 概率与随机化算法

概率将受控的随机性引入计算。
随机化算法使用随机选择，而非确定性步骤，以实现速度、简洁性或鲁棒性。
本节连接概率论与算法设计，教授如何建模、分析和利用随机性。

我们将涵盖：

- 概率基础
- 期望值
- 蒙特卡洛与拉斯维加斯算法
- 随机化数据结构与算法

#### 1. 概率基础

每个事件的概率在 0 到 1 之间。
如果一个样本空间有 $n$ 个等可能的结果，并且其中 $k$ 个满足某个条件，那么

$$
P(E) = \frac{k}{n}
$$

示例

- 掷一个公平的骰子：$P(\text{偶数}) = \frac{3}{6} = \frac{1}{2}$
- 从一副牌中抽到 A：$P(\text{A}) = \frac{4}{52} = \frac{1}{13}$

关键规则

- 补集：$P(\bar{E}) = 1 - P(E)$
- 加法：$P(A \cup B) = P(A) + P(B) - P(A \cap B)$
- 乘法：$P(A \cap B) = P(A) \cdot P(B \mid A)$

#### 2. 期望值

期望值是结果的加权平均值。

$$
E[X] = \sum_{i} P(x_i) \cdot x_i
$$

示例：一个骰子的期望值：
$$
E[X] = \frac{1+2+3+4+5+6}{6} = 3.5
$$

性质：

- 线性：$E[aX + bY] = aE[X] + bE[Y]$
- 适用于平均情况分析

在算法中：

- 快速排序中期望的比较次数是 $O(n \log n)$
- 哈希表查找的期望时间是 $O(1)$

#### 3. 蒙特卡洛 vs 拉斯维加斯

随机化算法主要有两种类型：

| 类型 | 输出 | 运行时间 | 示例 |
| :--- | :--- | :--- | :--- |
| 蒙特卡洛 | 可能错误（概率性地） | 固定 | Miller-Rabin 素性测试 |
| 拉斯维加斯 | 总是正确 | 随机运行时间 | 随机化快速排序 |

蒙特卡洛：

- 更快，近似
- 可以控制错误概率
- 例如，素性测试返回“可能为素数”

拉斯维加斯：

- 输出保证正确
- 运行时间随运气变化
- 例如，使用随机基准的快速排序

#### 4. 算法中的随机化

随机化有助于打破最坏情况的模式。

#### A. 随机化快速排序

选择一个随机基准，而不是第一个元素。
无论输入顺序如何，期望时间都变为 $O(n \log n)$。

```c
int partition(int a[], int l, int r) {
    int pivot = a[l + rand() % (r - l + 1)];
    // 将基准移到末尾，然后进行正常的分区操作
}
```

这避免了像已排序数组这样的对抗性输入。

#### B. 随机化哈希

哈希冲突可能被攻击者利用。
在哈希函数中使用随机系数使得攻击变得不可行。

```c
long long hash(long long x, long long a, long long b, long long p) {
    return (a * x + b) % p;
}
```

选择随机的 ( a, b ) 以提高鲁棒性。

#### C. 随机化数据结构

1.  跳表：为节点使用随机层级
    期望的 $O(\log n)$ 搜索/插入/删除

2.  Treap：随机堆优先级 + 二叉搜索树顺序
    在期望上保持平衡

```c
struct Node {
    int key, priority;
    Node *l, *r;
};
```

随机化平衡提供了良好的平均性能，而无需旋转逻辑。

#### D. 随机抽样

高效地选取随机元素：

- 蓄水池抽样：从大小未知的流中抽取 $k$ 个项目
- 洗牌：Fisher-Yates 算法
```c
for (int i = n - 1; i > 0; i--) {
    int j = rand() % (i + 1);
    swap(a[i], a[j]);
}
```

#### 5. 概率保证

随机化算法经常使用切尔诺夫界和马尔可夫不等式来界定错误：

- 马尔可夫：$P(X \ge kE[X]) \le \frac{1}{k}$
- 切比雪夫：$P(|X - E[X]| \ge c\sigma) \le \frac{1}{c^2}$
- 切尔诺夫：指数级小的尾部边界

这些确保了“以高概率”（$1 - \frac{1}{n^c}$）的保证。

#### 6. 微型代码

随机化快速排序：

```c
int partition(int arr[], int low, int high) {
    int pivotIdx = low + rand() % (high - low + 1);
    swap(arr[pivotIdx], arr[high]);
    int pivot = arr[high], i = low;
    for (int j = low; j < high; j++) {
        if (arr[j] < pivot) swap(arr[i++], arr[j]);
    }
    swap(arr[i], arr[high]);
    return i;
}

void quicksort(int arr[], int low, int high) {
    if (low < high) {
        int pi = partition(arr, low, high);
        quicksort(arr, low, pi - 1);
        quicksort(arr, pi + 1, high);
    }
}
```

#### 7. 总结

| 概念 | 核心理念 | 用例 |
| :--- | :--- | :--- |
| 期望值 | 加权平均结果 | 分析平均情况 |
| 蒙特卡洛 | 概率正确性 | 素性测试 |
| 拉斯维加斯 | 概率运行时间 | 快速排序 |
| 随机基准 | 打破最坏情况 | 排序 |
| 跳表 / Treap | 随机平衡 | 数据结构 |
| 蓄水池抽样 | 流选择 | 大数据 |

#### 为何重要

随机化不是“运气”，而是一种设计原则。
它将僵化的算法转变为自适应的、鲁棒的系统。
在复杂性理论中，随机性有助于实现确定性算法无法达到的界限。

> “一点随机性将最坏情况变成最好的朋友。”

#### 亲自尝试

1.  模拟掷两个骰子并计算期望和。
2.  实现随机化快速排序并测量平均运行时间。
3.  编写一个蒙特卡洛素性检查器。
4.  为整数创建一个随机哈希函数。
5.  为大型输入流实现蓄水池抽样。

这些实验展示了不确定性如何成为算法设计中的强大盟友。
### 55. 筛法与模运算

筛法是数论中用于高效生成素数、质因数以及函数值（φ, μ）的重要工具。结合模运算，它们构成了密码学、组合数学和竞赛编程中算法的基石。

本节介绍：

- 埃拉托斯特尼筛法
- 优化的线性筛
- 最小质因数筛
- 欧拉函数（φ）
- 模运算应用

#### 1. 埃拉托斯特尼筛法

寻找所有 ≤ n 的素数的经典算法。

思路：
从 2 开始，标记所有倍数为合数。继续到 √n。

```c
vector<int> sieve(int n) {
    vector<int> primes;
    vector<bool> is_prime(n + 1, true);
    is_prime[0] = is_prime[1] = false;
    for (int i = 2; i * i <= n; i++)
        if (is_prime[i])
            for (int j = i * i; j <= n; j += i)
                is_prime[j] = false;
    for (int i = 2; i <= n; i++)
        if (is_prime[i]) primes.push_back(i);
    return primes;
}
```

时间复杂度：$O(n \log \log n)$

空间复杂度：$O(n)$

示例：20 以内的素数 → 2, 3, 5, 7, 11, 13, 17, 19

#### 2. 线性筛（O(n)）

与基本筛法不同，每个数仅被其最小质因数标记一次。

思路：

- 对于每个素数 $p$，只标记 $p \times i$ 一次。
- 使用 `spf[i]` 存储最小质因数。

```c
const int N = 1e6;
int spf[N + 1];
vector<int> primes;

void linear_sieve() {
    for (int i = 2; i <= N; i++) {
        if (!spf[i]) {
            spf[i] = i;
            primes.push_back(i);
        }
        for (int p : primes) {
            if (p > spf[i] || 1LL * i * p > N) break;
            spf[i * p] = p;
        }
    }
}
```

优点：

- 在 $O(n)$ 时间内获取素数、最小质因数和质因数分解。
- 非常适合需要大量质因数分解的问题。

#### 3. 最小质因数表

有了 `spf[]` 数组，质因数分解可以在 $O(\log n)$ 时间内完成。

```c
vector<int> factorize(int x) {
    vector<int> f;
    while (x != 1) {
        f.push_back(spf[x]);
        x /= spf[x];
    }
    return f;
}
```

示例：
spf[12] = 2 → 质因数 = [2, 2, 3]

#### 4. 欧拉函数 $\varphi(n)$

小于等于 $n$ 且与 $n$ 互质的整数的个数。

公式：
$$
\varphi(n) = n \prod_{p|n} \left(1 - \frac{1}{p}\right)
$$

性质：

- 若 $p$ 是素数，则 $\varphi(p) = p - 1$
- 积性：若 $\gcd(a, b) = 1$，则 $\varphi(ab) = \varphi(a)\varphi(b)$

实现（线性筛）：

```c
const int N = 1e6;
int phi[N + 1];
bool is_comp[N + 1];
vector<int> primes;

void phi_sieve() {
    phi[1] = 1;
    for (int i = 2; i <= N; i++) {
        if (!is_comp[i]) {
            primes.push_back(i);
            phi[i] = i - 1;
        }
        for (int p : primes) {
            if (1LL * i * p > N) break;
            is_comp[i * p] = true;
            if (i % p == 0) {
                phi[i * p] = phi[i] * p;
                break;
            } else {
                phi[i * p] = phi[i] * (p - 1);
            }
        }
    }
}
```

示例：

- $\varphi(6) = 6(1 - \frac{1}{2})(1 - \frac{1}{3}) = 2$
- 与 6 互质的数：1, 5

#### 5. 模运算应用

一旦我们有了素数和欧拉函数值，就可以进行许多模运算。

#### A. 费马小定理

若 $p$ 是素数，
$$
a^{p-1} \equiv 1 \pmod{p}
$$
因此，
$$
a^{-1} \equiv a^{p-2} \pmod{p}
$$

用于：模逆元、组合数学。

#### B. 欧拉定理

若 $\gcd(a, n) = 1$，则

$$
a^{\varphi(n)} \equiv 1 \pmod{n}
$$

将费马小定理推广到合数模数。

#### C. 利用欧拉函数进行模幂化简

对于非常大的指数：

$$
a^b \bmod n = a^{b \bmod \varphi(n)} \bmod n
$$

（当 $a$ 和 $n$ 互质时）

#### 6. 代码片段

n 以内的素数：

```c
auto primes = sieve(100);
```

n 以内的欧拉函数值：

```c
phi_sieve();
cout << phi[10]; // 4
```

质因数分解：

```c
auto f = factorize(60); // [2, 2, 3, 5]
```

#### 7. 总结

| 概念           | 描述                 | 时间复杂度          | 用途               |
| -------------- | -------------------- | ------------------- | ------------------ |
| 埃拉托斯特尼筛 | 标记倍数             | $O(n \log \log n)$  | 简单生成素数       |
| 线性筛         | 标记一次             | $O(n)$              | 素数 + 最小质因数  |
| 最小质因数表   | 最小质因数           | $O(1)$ 查询         | 快速质因数分解     |
| $\varphi(n)$   | 互质计数             | $O(n)$              | 模幂运算           |
| 费马/欧拉定理  | 逆元、指数化简       | $O(\log n)$         | 模运算             |

#### 重要性

筛法是预处理算术信息的最快方法。它们为涉及素数、除数、模方程和密码学的问题提供了高效的解决方案。

> "在你能够推理数字之前，你必须先将其筛分干净。"

#### 动手试试

1.  使用线性筛生成所有 $\le 10^6$ 的素数。
2.  使用最小质因数数组对 $840$ 进行质因数分解。
3.  计算 $n = 1..20$ 的 $\varphi(n)$ 值。
4.  验证对于 $a = 3$, $n = 10$，有 $a^{\varphi(n)} \equiv 1 \pmod{n}$。
5.  利用 $\varphi(n)$ 求解 $a^b \bmod n$，其中 $b$ 非常大。

筛一次，模运算从此变得轻松。
### 56. 线性代数（高斯消元法、LU分解、SVD）

线性代数为算法提供了数学基础。从求解方程到驱动机器学习模型，它是优化、几何和数值计算背后的隐藏引擎。

在本节中，我们将重点介绍算法工具包：

- 高斯消元法（求解方程组，求逆矩阵）
- LU分解（高效重复求解）
- SVD（奇异值分解）概述

你将逐步看到代数如何转化为代码。

#### 1. 线性方程组

我们想求解：
$$
A \cdot x = b
$$
其中 ( A ) 是一个 $n \times n$ 矩阵，( x, b ) 是向量。

例如：
$$\begin{cases}
2x + 3y = 8 \\
x + 2y = 5
\end{cases}$$

解是两条直线的交点。
一般情况下，$A^{-1}b$ 给出 ( x )，但我们通常使用高斯消元法更直接地求解它。

#### 2. 高斯消元法（行约简）

思想：将 ( [A|b] )（增广矩阵）转换为上三角形式，然后回代。

步骤：

1. 对于每一行，选择一个主元（非零首元素）。
2. 使用行操作消去其下方的元素。
3. 处理完所有主元后，回代得到解。

#### A. 实现（C语言）

```c
const double EPS = 1e-9;

vector<double> gauss(vector<vector<double>> A, vector<double> b) {
    int n = A.size();
    for (int i = 0; i < n; i++) {
        // 1. 寻找主元
        int pivot = i;
        for (int j = i + 1; j < n; j++)
            if (fabs(A[j][i]) > fabs(A[pivot][i]))
                pivot = j;
        swap(A[i], A[pivot]);
        swap(b[i], b[pivot]);

        // 2. 归一化主元行
        double div = A[i][i];
        if (fabs(div) < EPS) continue;
        for (int k = i; k < n; k++) A[i][k] /= div;
        b[i] /= div;

        // 3. 消去下方元素
        for (int j = i + 1; j < n; j++) {
            double factor = A[j][i];
            for (int k = i; k < n; k++) A[j][k] -= factor * A[i][k];
            b[j] -= factor * b[i];
        }
    }

    // 4. 回代
    vector<double> x(n);
    for (int i = n - 1; i >= 0; i--) {
        x[i] = b[i];
        for (int j = i + 1; j < n; j++)
            x[i] -= A[i][j] * x[j];
    }
    return x;
}
```

时间复杂度： ( O$n^3$ )

#### B. 示例

求解：
$$\begin{cases}
2x + 3y = 8 \\
x + 2y = 5
\end{cases}$$

增广矩阵：
$$\begin{bmatrix}
2 & 3 & | & 8 \\
1 & 2 & | & 5
\end{bmatrix}$$

约简：

- 行2 ← 行2 − ½ 行1 → $[1, 2 | 5] \to [0, 0.5 | 1]$
- 回代 → ( y = 2, x = 1 )

#### 3. LU分解

LU分解表示为：
$$
A = L \cdot U
$$
其中 ( L ) 是下三角矩阵（对角线为1），( U ) 是上三角矩阵。

这使得求解 ( A x = b ) 可以通过两次三角求解完成：

1. 求解 ( L y = b )
2. 求解 ( U x = y )

当需要为多个不同的 b 求解（相同的 A）时非常高效。

#### A. 分解算法

```c
void lu_decompose(vector<vector<double>>& A, vector<vector<double>>& L, vector<vector<double>>& U) {
    int n = A.size();
    L.assign(n, vector<double>(n, 0));
    U.assign(n, vector<double>(n, 0));

    for (int i = 0; i < n; i++) {
        // 上三角部分
        for (int k = i; k < n; k++) {
            double sum = 0;
            for (int j = 0; j < i; j++)
                sum += L[i][j] * U[j][k];
            U[i][k] = A[i][k] - sum;
        }
        // 下三角部分
        for (int k = i; k < n; k++) {
            if (i == k) L[i][i] = 1;
            else {
                double sum = 0;
                for (int j = 0; j < i; j++)
                    sum += L[k][j] * U[j][i];
                L[k][i] = (A[k][i] - sum) / U[i][i];
            }
        }
    }
}
```

使用前向 + 后向回代求解。

#### 4. 奇异值分解（SVD）

SVD 将对角化推广到非方阵：
$$
A = U \Sigma V^T
$$

其中：

- ( U )：左奇异向量（正交）
- $\Sigma$：奇异值对角矩阵
- $V^T$：右奇异向量

应用：

- 数据压缩（PCA）
- 降噪
- 秩估计
- 伪逆 $A^+ = V \Sigma^{-1} U^T$

在实践中，使用库（例如 LAPACK, Eigen）。

#### 5. 数值稳定性与选主元

在浮点数运算中：

- 始终选择最大的主元（部分选主元）
- 避免除以很小的数
- 使用 EPS = 1e-9 作为阈值

小的数值误差可能迅速放大，稳定性是关键。

#### 6. 微型代码

```c
vector<vector<double>> A = {{2, 3}, {1, 2}};
vector<double> b = {8, 5};
auto x = gauss(A, b);
// 输出: x = [1, 2]
```

#### 7. 总结

| 算法 | 目的 | 复杂度 | 备注 |
| :--- | :--- | :--- | :--- |
| 高斯消元法 | 求解 Ax=b | (O$n^3$) | 直接法 |
| LU分解 | 重复求解 | (O$n^3$) | 三角分解 |
| SVD | 通用分解 | (O$n^3$) | 鲁棒，通用 |

#### 为何重要

线性代数是算法的语言，它求解方程、优化函数并投影数据。无论是构建求解器还是神经网络，这些方法都是你的基础。

> "每个算法都存在于一个向量空间中，它只需要一个基来表达自己。"

#### 亲自尝试

1.  使用高斯消元法求解一个 3×3 线性方程组。
2.  实现 LU 分解并测试 $L \cdot U = A$。
3.  使用 LU 分解求解多个不同的 ( b ) 向量。
4.  使用数学库探索 SVD；计算一个 2×2 矩阵的奇异值。
5.  比较不稳定系统在朴素消元法和选主元消元法下的结果。

从行操作开始，你将看到几何和代数如何融合到代码中。
### 57. FFT 与 NTT（快速变换）

快速傅里叶变换（FFT）是有史以来最优雅且实用的算法之一。
它能在时域（或系数域）与频域（或点值域）之间高效地转换数据。
数论变换（NTT）是其在模运算下的对应版本，非常适合在模数下进行多项式乘法。

本节涵盖：

- 为什么需要变换
- 离散傅里叶变换（DFT）
- Cooley-Tukey FFT（复数）
- NTT（模运算版本）
- 应用（多项式乘法、卷积）

#### 1. 动机

假设你想将两个多项式相乘：
$$
A(x) = a_0 + a_1x + a_2x^2
$$

$$
B(x) = b_0 + b_1x + b_2x^2
$$

它们的乘积系数为：
$$
c_k = \sum_{i+j=k} a_i \cdot b_j
$$

这就是卷积：
$$
C = A * B
$$
朴素方法需要 $O(n^2)$ 的时间。
FFT 将其降低到 $O(n \log n)$。

#### 2. 离散傅里叶变换（DFT）

DFT 将系数 $a_0, a_1, \ldots, a_{n-1}$ 映射到单位 $n$ 次根处的求值：

$$
A_k = \sum_{j=0}^{n-1} a_j \cdot e^{-2\pi i \cdot jk / n}
$$

逆变换则从 $A_k$ 恢复 $a_j$。

#### 3. Cooley-Tukey FFT

核心思想：递归地将求和拆分为偶数和奇数部分：

$$
A_k = A_{even}(w_n^2) + w_n^k \cdot A_{odd}(w_n^2)
$$

其中 $w_n = e^{-2\pi i / n}$ 是一个 $n$ 次单位根。

#### 实现（C++）

```c++
#include <complex>
#include <vector>
#include <cmath>
using namespace std;

using cd = complex<double>;
const double PI = acos(-1);

void fft(vector<cd> &a, bool invert) {
    int n = a.size();
    for (int i = 1, j = 0; i < n; i++) {
        int bit = n >> 1;
        for (; j & bit; bit >>= 1) j ^= bit;
        j ^= bit;
        if (i < j) swap(a[i], a[j]);
    }

    for (int len = 2; len <= n; len <<= 1) {
        double ang = 2 * PI / len * (invert ? -1 : 1);
        cd wlen(cos(ang), sin(ang));
        for (int i = 0; i < n; i += len) {
            cd w(1);
            for (int j = 0; j < len / 2; j++) {
                cd u = a[i + j], v = a[i + j + len / 2] * w;
                a[i + j] = u + v;
                a[i + j + len / 2] = u - v;
                w *= wlen;
            }
        }
    }

    if (invert) {
        for (cd &x : a) x /= n;
    }
}
```

#### 使用 FFT 进行多项式乘法

```c++
vector<long long> multiply(vector<int> const& a, vector<int> const& b) {
    vector<cd> fa(a.begin(), a.end()), fb(b.begin(), b.end());
    int n = 1;
    while (n < (int)a.size() + (int)b.size()) n <<= 1;
    fa.resize(n);
    fb.resize(n);

    fft(fa, false);
    fft(fb, false);
    for (int i = 0; i < n; i++) fa[i] *= fb[i];
    fft(fa, true);

    vector<long long> result(n);
    for (int i = 0; i < n; i++)
        result[i] = llround(fa[i].real());
    return result;
}
```

复杂度：$O(n \log n)$

#### 4. 数论变换（NTT）

FFT 使用复数，NTT 使用模运算下的单位根。
我们需要一个满足以下条件的素数 $p$：
$$
p = c \cdot 2^k + 1
$$
这样才存在原根 $g$。

常用选择：

- $p = 998244353, g = 3$
- $p = 7340033, g = 3$

#### 实现（NTT）

```c++
const int MOD = 998244353;
const int G = 3;

int modpow(int a, int b) {
    long long res = 1;
    while (b) {
        if (b & 1) res = res * a % MOD;
        a = 1LL * a * a % MOD;
        b >>= 1;
    }
    return res;
}

void ntt(vector<int> &a, bool invert) {
    int n = a.size();
    for (int i = 1, j = 0; i < n; i++) {
        int bit = n >> 1;
        for (; j & bit; bit >>= 1) j ^= bit;
        j ^= bit;
        if (i < j) swap(a[i], a[j]);
    }
    for (int len = 2; len <= n; len <<= 1) {
        int wlen = modpow(G, (MOD - 1) / len);
        if (invert) wlen = modpow(wlen, MOD - 2);
        for (int i = 0; i < n; i += len) {
            long long w = 1;
            for (int j = 0; j < len / 2; j++) {
                int u = a[i + j];
                int v = (int)(a[i + j + len / 2] * w % MOD);
                a[i + j] = u + v < MOD ? u + v : u + v - MOD;
                a[i + j + len / 2] = u - v >= 0 ? u - v : u - v + MOD;
                w = w * wlen % MOD;
            }
        }
    }
    if (invert) {
        int inv_n = modpow(n, MOD - 2);
        for (int &x : a) x = 1LL * x * inv_n % MOD;
    }
}
```

#### 5. 应用

1. 多项式乘法：$O(n \log n)$
2. 卷积：数字信号处理
3. 大整数乘法（Karatsuba，FFT）
4. 子集卷积与组合变换
5. 数论求和（NTT 友好模数）

#### 6. 示例代码

```c++
vector<int> A = {1, 2, 3};
vector<int> B = {4, 5, 6};
// 结果 = {4, 13, 28, 27, 18}
auto C = multiply(A, B);
```

#### 7. 总结

| 算法 | 域       | 复杂度         | 类型               |
| ---- | -------- | -------------- | ------------------ |
| DFT  | 复数     | $O(n^2)$       | 朴素               |
| FFT  | 复数     | $O(n \log n)$  | 递归               |
| NTT  | 模运算   | $O(n \log n)$  | 整数算术           |

#### 重要性

FFT 和 NTT 让多项式代数焕发生机。
它们将缓慢的卷积转变为闪电般的快速变换。
从大整数乘法到信号压缩，它们是结构上终极的分治算法。

> "要想快速相乘多项式，你先把它们变成音乐，然后再变回来。"

#### 动手尝试

1. 使用 FFT 计算 $(1 + 2x + 3x^2)$ 和 $(4 + 5x + 6x^2)$ 的乘积。
2. 在模 998244353 下实现 NTT 并验证结果。
3. 比较 $n = 1024$ 时 $O(n^2)$ 方法与 FFT 的性能。
4. 尝试逆 FFT（`invert = true`）。
5. 探索信号数据的循环卷积。

一旦你掌握了 FFT/NTT，你就掌握了代数计算中的速度之力。
### 58. 数值方法（牛顿、辛普森、龙格-库塔）

当精确的代数解难以或无法求得时，数值方法让我们能够近似求解。
它们是科学计算、模拟和优化的基础，弥合了连续数学与离散计算之间的鸿沟。

在本节中，我们将探索三种经典方法：

- 牛顿-拉弗森法：求根
- 辛普森法则：数值积分
- 龙格-库塔法（RK4）：求解微分方程

这些算法展示了迭代、近似和收敛如何构建强大的工具。

#### 1. 牛顿-拉弗森法

用于求方程 ( f(x) = 0 ) 的根。
从一个初始猜测 $x_0$ 开始，迭代改进：

$$
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
$$

如果 ( f ) 函数光滑且 $x_0$ 足够接近真根，则收敛速度是二次的。

#### A. 示例

求解 ( f(x) = x^2 - 2 = 0 )
我们知道根 = $\sqrt{2}$

从 $x_0 = 1$ 开始

| 迭代 | $x_n$ | (f$x_n$) | (f'$x_n$) | $x_{n+1}$ |
| ---- | ----- | -------- | --------- | --------- |
| 0    | 1.000 | -1.000   | 2.000     | 1.500     |
| 1    | 1.500 | 0.250    | 3.000     | 1.417     |
| 2    | 1.417 | 0.006    | 2.834     | 1.414     |

已收敛：$1.414 \approx \sqrt{2}$

#### B. 实现

```c
#include <math.h>
#include <stdio.h>

double f(double x) { return x * x - 2; }
double df(double x) { return 2 * x; }

double newton(double x0) {
    for (int i = 0; i < 20; i++) {
        double fx = f(x0);
        double dfx = df(x0);
        if (fabs(fx) < 1e-9) break;
        x0 = x0 - fx / dfx;
    }
    return x0;
}

int main() {
    printf("根: %.6f\n", newton(1.0)); // 1.414214
}
```

时间复杂度：迭代 ( O(k) ) 次，每次 ( O(1) )

#### 2. 辛普森法则（数值积分）

当你无法解析地积分 ( f(x) ) 时，可以近似计算曲线下的面积。

将区间 ([a, b]) 划分为偶数 ( n ) 个子区间（宽度为 ( h )）。

$$
I \approx \frac{h}{3} \Big( f(a) + 4 \sum f(x_{odd}) + 2 \sum f(x_{even}) + f(b) \Big)
$$

#### A. 实现

```c
#include <math.h>
#include <stdio.h>

double f(double x) { return x * x; } // 积分 x^2

double simpson(double a, double b, int n) {
    double h = (b - a) / n;
    double s = f(a) + f(b);
    for (int i = 1; i < n; i++) {
        double x = a + i * h;
        s += f(x) * (i % 2 == 0 ? 2 : 4);
    }
    return s * h / 3;
}

int main() {
    printf("∫₀¹ x² dx ≈ %.6f\n", simpson(0, 1, 100)); // ~0.333333
}
```

精度： ( O$h^4$ )
注意： ( n ) 必须为偶数。

#### B. 示例

$$
\int_0^1 x^2 dx = \frac{1}{3}
$$
当 ( n = 100 ) 时，辛普森法则给出 ( 0.333333 )。

#### 3. 龙格-库塔法（RK4）

用于求解一阶常微分方程：
$$
y' = f(x, y), \quad y(x_0) = y_0
$$

RK4 公式：

$$\begin{aligned}
k_1 &= f(x_n, y_n) \\
k_2 &= f(x_n + \frac{h}{2}, y_n + \frac{h}{2}k_1) \\
k_3 &= f(x_n + \frac{h}{2}, y_n + \frac{h}{2}k_2) \\
k_4 &= f(x_n + h, y_n + hk_3) \\
y_{n+1} &= y_n + \frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)
\end{aligned}$$

精度： ( O$h^4$ )

#### A. 示例

求解 ( y' = x + y )， ( y(0) = 1 )，步长 ( h = 0.1 )。

每次迭代通过加权平均斜率来改进 ( y )。

#### B. 实现

```c
#include <stdio.h>

double f(double x, double y) {
    return x + y;
}

double runge_kutta(double x0, double y0, double h, double xn) {
    double x = x0, y = y0;
    while (x < xn) {
        double k1 = f(x, y);
        double k2 = f(x + h / 2, y + h * k1 / 2);
        double k3 = f(x + h / 2, y + h * k2 / 2);
        double k4 = f(x + h, y + h * k3);
        y += h * (k1 + 2*k2 + 2*k3 + k4) / 6;
        x += h;
    }
    return y;
}

int main() {
    printf("y(0.1) ≈ %.6f\n", runge_kutta(0, 1, 0.1, 0.1));
}
```

#### 4. 微型代码总结

| 方法               | 用途         | 公式                               | 精度        | 类型         |
| ------------------ | ------------ | ---------------------------------- | ----------- | ------------ |
| 牛顿-拉弗森法      | 求根         | $x_{n+1}=x_n-\frac{f}{f'}$         | 二次收敛    | 迭代         |
| 辛普森法则         | 积分         | (h/3(...))                         | (O$h^4$)    | 确定性       |
| 龙格-库塔法（RK4） | 常微分方程   | 加权平均斜率                       | (O$h^4$)    | 迭代         |

#### 5. 数值稳定性

- 小步长 ( h )：精度更高，成本更高
- 大步长 ( h )：速度更快，稳定性更差
- 始终检查收敛性 ($|x_{n+1} - x_n| < \varepsilon$)
- 在牛顿法中避免除以过小的导数值

#### 为何重要

数值方法让计算机能够模拟连续世界。
从物理学到人工智能训练，它们解决了微积分无法符号化求解的问题。

> "当方程沉默不语时，迭代吧，它们会轻声告诉你答案。"

#### 动手尝试

1.  使用牛顿法求解 $\cos x - x = 0$。
2.  用辛普森法则近似计算 $\displaystyle \int_0^{\pi/2} \sin x\,dx$。
3.  使用 RK4 求解 $y' = y - x^2 + 1,\ y(0) = 0.5$。
4.  比较 RK4 与欧拉法求解同一个常微分方程。
5.  尝试步长 $h \in \{0.1, 0.01, 0.001\}$ 并观察收敛情况。

数值思维将连续问题转化为迭代算法，其精确度足以驱动你将要编写的每一个模拟器和求解器。
### 59. 数学优化（单纯形法、梯度法、凸优化）

数学优化是在给定约束条件下寻找最佳解、最小成本、最大利润、最短路径的过程。
它是机器学习、运筹学和工程设计领域的核心。

在本节中，我们将探讨三大支柱：

- 单纯形法，用于线性规划
- 梯度下降法，用于连续优化
- 凸优化，确保全局最优解的理论

#### 1. 什么是优化？

一个通用的优化问题形式如下：

$$
\min_x \ f(x)
$$
约束条件为：
$$
g_i(x) \le 0, \quad h_j(x) = 0
$$

当 ( f ) 和 $g_i, h_j$ 是线性函数时，它是一个线性规划问题。
当 ( f ) 可微时，我们可以使用梯度。
当 ( f ) 是凸函数时，每个局部最小值都是全局最小值，这是理想情况。


#### 2. 单纯形法（线性规划）

线性规划的形式如下：

$$
\max \ c^T x
$$
约束条件为：
$$
A x \le b, \quad x \ge 0
$$

从几何上看，每个约束形成一个半空间。
可行域是一个凸多面体，最优解位于一个顶点上。


#### A. 示例

最大化 ( z = 3x + 2y )
约束条件为：
$$\begin{cases}
2x + y \le 18 \\
2x + 3y \le 42 \\
x, y \ge 0
\end{cases}$$

解： ( x=9, y=8 ), ( z=43 )


#### B. 算法（概述）

1.  通过添加松弛变量将不等式转换为等式。
2.  从一个顶点（基本可行解）开始初始化。
3.  在每一步：
    - 选择进入变量（目标函数中系数最负的变量）。
    - 选择离开变量（最小比值检验）。
    - 旋转到新的顶点。
4.  重复直到达到最优。


#### C. 实现（简化伪代码）

```c
// 类似单纯形法的基本框架
while (目标行中存在负系数) {
    选择进入列 j；
    选择离开行 i（最小 b[i]/a[i][j]）；
    旋转(i, j)；
}
```

完整的实现由库（如 `GLPK` 或 `Eigen`）处理。

时间复杂度：最坏情况 ( O$2^n$ )，但在实践中很快。


#### 3. 梯度下降法

对于可微函数 ( f(x) )，我们沿着梯度的反方向移动：

$$
x_{k+1} = x_k - \eta \nabla f(x_k)
$$

其中 $\eta$ 是学习率。

直观理解：( \nabla f(x) ) 指向上坡方向，因此向相反方向移动。


#### A. 示例

最小化 ( f(x) = (x-3)^2 )

$$
f'(x) = 2(x-3)
$$

起始点 $x_0 = 0$, $\eta = 0.1$

| 迭代 | $x_k$ | (f$x_k$) | 梯度 | 新 (x) |
| ---- | ----- | -------- | -------- | ------- |
| 0    | 0     | 9        | -6       | 0.6     |
| 1    | 0.6   | 5.76     | -4.8     | 1.08    |
| 2    | 1.08  | 3.69     | -3.84    | 1.46    |
| ...  | →3    | →0       | →0       | →3      |

收敛到 ( x = 3 )


#### B. 实现

```c
#include <math.h>
#include <stdio.h>

double f(double x) { return (x - 3) * (x - 3); }
double df(double x) { return 2 * (x - 3); }

double gradient_descent(double x0, double lr) {
    for (int i = 0; i < 100; i++) {
        double g = df(x0);
        if (fabs(g) < 1e-6) break;
        x0 -= lr * g;
    }
    return x0;
}

int main() {
    printf("最小值在 x = %.6f\n", gradient_descent(0, 0.1));
}
```


#### C. 变体

- 动量法： ( v = \beta v + $1-\beta$\nabla f(x) )
- Adam：自适应学习率
- 随机梯度下降：数据的随机子集
这些在机器学习中被大量使用。


#### 4. 凸优化

如果函数 ( f ) 满足以下条件，则它是凸函数：
$$
f(\lambda x + (1-\lambda)y) \le \lambda f(x) + (1-\lambda)f(y)
$$

这意味着任何局部最小值都是全局最小值。

示例：

- ( f(x) = x^2 ) （凸函数）
- ( f(x) = x^3 ) （非凸函数）
对于具有线性约束的凸函数，基于梯度的方法总是收敛到全局最优解。


#### A. 检查凸性

- 一维情况： ( f''(x) \ge 0 )
- 多维情况：海森矩阵 ( \nabla^2 f(x) ) 是半正定的

#### 5. 应用

- 线性规划（单纯形法）：物流、调度
- 二次规划：投资组合优化
- 梯度法：机器学习、曲线拟合
- 凸规划：控制系统、正则化

#### 6. 微型代码

使用简单梯度下降最小化 ( f(x,y)=x^2+y^2 )：

```c
double f(double x, double y) { return x*x + y*y; }
void grad(double x, double y, double *gx, double *gy) {
    *gx = 2*x; *gy = 2*y;
}

void optimize() {
    double x=5, y=3, lr=0.1;
    for(int i=0; i<100; i++){
        double gx, gy;
        grad(x, y, &gx, &gy);
        x -= lr * gx;
        y -= lr * gy;
    }
    printf("最小值在 (%.3f, %.3f)\n", x, y);
}
```


#### 7. 总结

| 算法           | 领域       | 复杂度                        | 备注                     |
| -------------- | ---------- | ----------------------------- | ------------------------ |
| 单纯形法       | 线性       | 多项式（平均情况）            | 线性规划求解器           |
| 梯度下降法     | 连续       | $O(k)$                        | 需要步长                 |
| 凸优化方法     | 凸         | $O(k \log \frac{1}{\varepsilon})$ | 保证全局最优解           |



#### 为何重要

优化将数学转化为决策。
从曲线拟合到资源规划，它将权衡和效率形式化。
这是计算与目的相遇的地方，在所有可能的世界中寻找最佳方案。

> "每个算法，本质上都是一个优化器，在寻找更好的东西。"


#### 亲自尝试

1.  使用单纯形法手动求解一个包含 2 个约束的线性规划问题。
2.  为 $f(x) = (x - 5)^2 + 2$ 实现梯度下降法。
3.  在你的梯度下降循环中添加动量。
4.  通过绘制 $f(x) = x^4 - 3x^2$ 的图像来检查其凸性。
5.  尝试不同的学习率：太小导致收敛缓慢；太大可能导致发散。

掌握优化意味着掌握算法如何逐步、迭代地自我改进。
### 60. 代数技巧与变换技术

在算法设计中，代数不仅仅是理论，它还是一个用于转换问题的工具箱。
通过以代数方式表达计算，我们可以简化、加速或推广解决方案。
本节将概述一些常见的代数技巧，它们能将难题转化为可处理的问题。

我们将探讨：

- 代数恒等式与因式分解
- 生成函数与变换
- 卷积技巧
- 多项式方法与 FFT 应用
- 用于加速的矩阵与线性变换

#### 1. 代数恒等式

这些恒等式允许你重写或分解表达式，以揭示其结构或降低复杂度。

经典形式：

- 平方差：
$$
a^2 - b^2 = (a-b)(a+b)
$$
- 立方和：
$$
a^3 + b^3 = (a+b)(a^2 - ab + b^2)
$$
- 和的平方：
$$
(a+b)^2 = a^2 + 2ab + b^2
$$

在简化递推项或约束条件时，用于动态规划、几何和优化问题。

示例：
转换 $(x+y)^2$ 可以让你高效地计算 $x^2 + y^2$ 以及交叉项。

#### 2. 生成函数

生成函数将一个序列 $a_0, a_1, a_2, \ldots$ 编码成一个形式幂级数：

$$
G(x) = a_0 + a_1x + a_2x^2 + \ldots
$$

它们将递推关系和计数问题转化为代数方程。

示例：斐波那契数列
$$
F(x) = F_0 + F_1x + F_2x^2 + \ldots
$$
其递推关系为 $F_n = F_{n-1} + F_{n-2}$

代数求解：
$$
F(x) = \frac{x}{1 - x - x^2}
$$

应用：组合数学、概率论、划分计数。

#### 3. 卷积技巧

卷积出现在序列组合中：
$$
(c_n) = (a * b)_n = \sum_{i=0}^{n} a_i b_{n-i}
$$

朴素计算：$O(n^2)$
使用快速傅里叶变换 (FFT)：$O(n \log n)$

示例：多项式乘法
令
$$
A(x) = a_0 + a_1x + a_2x^2, \quad B(x) = b_0 + b_1x + b_2x^2
$$
则 $C(x) = A(x)B(x)$ 的系数由卷积给出。

此技巧用于：

- 大整数乘法
- 模式匹配（互相关）
- 子集和加速

#### 4. 多项式方法

许多算法问题可以表示为多项式，其中系数编码了组合结构。

#### A. 多项式插值

给定 $n+1$ 个点，存在唯一的一个 $n$ 次多项式穿过这些点。

用于纠错、基于 FFT 的重建以及数论变换。

拉格朗日插值：
$$
P(x) = \sum_i y_i \prod_{j \ne i} \frac{x - x_j}{x_i - x_j}
$$

#### B. 根表示法

通过对多项式取模来求解方程或检查恒等式。
用于有限域和编码理论（例如，Reed-Solomon 码）。

#### 5. 变换技术

变换将问题转换到更简单的域，在该域中操作变得高效。

| 变换                         | 转换方向             | 关键属性                     | 用于                     |
| ---------------------------- | -------------------- | ---------------------------- | ----------------------- |
| FFT / NTT                    | 时域 ↔ 频域          | 卷积 → 乘法                  | 信号处理、多项式乘法     |
| Z 变换                       | 序列 ↔ 函数          | 递推求解                     | 数字信号处理、控制理论  |
| 拉普拉斯变换                 | 函数 ↔ 代数          | 微分方程 → 代数方程          | 连续系统                |
| Walsh-Hadamard 变换          | 布尔向量             | XOR 卷积                     | 子集和、SOS DP          |

示例：通过 FWT 进行子集卷积

对于所有子集 $S$：
$$
f'(S) = \sum_{T \subseteq S} f(T)
$$

使用快速 Walsh-Hadamard 变换 (FWHT) 在 $O(n2^n)$ 内计算，而不是 $O(3^n)$。

#### 6. 矩阵技巧

矩阵代数支持变换和紧凑的公式表示。

- 矩阵幂运算：在 $O(\log n)$ 内求解递推关系
- 对角化：$A = P D P^{-1}$，则 $A^k = P D^k P^{-1}$
- 快速幂：加速斐波那契数列、线性递推、马尔可夫链

示例：斐波那契数列

$$
\begin{bmatrix}
F_{n+1} \\
F_n
\end{bmatrix}
=
\begin{bmatrix}
1 & 1 \\
1 & 0
\end{bmatrix}^n
\begin{bmatrix}
1 \\
0
\end{bmatrix}
$$

#### 7. 精简代码

通过 FFT 进行多项式乘法（伪 C 代码）：

```c
// 使用复数 FFT 库的概要
fft(A, false);
fft(B, false);
for (int i = 0; i < n; i++)
    C[i] = A[i] * B[i];
fft(C, true); // 逆变换
```

矩阵幂（斐波那契）：

```c
void matmul(long long A[2][2], long long B[2][2]) {
    long long C[2][2] = {{0}};
    for (int i=0;i<2;i++)
        for (int j=0;j<2;j++)
            for (int k=0;k<2;k++)
                C[i][j] += A[i][k]*B[k][j];
    memcpy(A, C, sizeof(C));
}

void matpow(long long A[2][2], int n) {
    long long R[2][2] = {{1,0},{0,1}};
    while(n){
        if(n&1) matmul(R,A);
        matmul(A,A);
        n>>=1;
    }
    memcpy(A, R, sizeof(R));
}
```

#### 8. 总结

| 技巧                 | 目的                 | 加速效果                     |
| -------------------- | -------------------- | ---------------------------- |
| 代数恒等式           | 简化表达式           | 常数因子                     |
| 生成函数             | 求解递推关系         | 概念性                       |
| FFT / 卷积           | 快速组合序列         | $O(n^2) \to O(n \log n)$     |
| 多项式插值           | 重建                 | $O(n^2) \to O(n \log^2 n)$   |
| 矩阵技巧             | 加速递推关系         | $O(n) \to O(\log n)$         |

#### 为何重要

代数将计算转化为结构。
通过以代数形式重写问题，你可以揭示隐藏的对称性、利用快速变换并找到优雅的解决方案。
这不是魔法，而是性能背后的数学。

> "最聪明的代码往往是先在纸上自行求解的那一个。"

#### 亲自尝试

1.  使用 FFT 将两个多项式相乘。
2.  将斐波那契数列表示为矩阵并计算 $F_{100}$。
3.  使用生成函数计算硬币找零的方式数。
4.  通过 Walsh-Hadamard 变换实现子集和。
5.  推导一个递推关系并用代数方法求解。

理解代数技巧使你不仅是一名程序员，更是一名数学工程师，能够随心所欲地驾驭结构。

## 第 7 章. 字符串与文本算法
### 61. 字符串匹配（KMP、Z、Rabin-Karp、Boyer-Moore）

字符串匹配是计算机科学中最古老、最基本的问题之一：给定一个长度为 ( n ) 的文本 ( T ) 和一个长度为 ( m ) 的模式 ( P )，找出 ( P ) 在 ( T ) 中出现的所有位置。

本节将引导你了解从简单到高效的算法，从直接的暴力方法到优雅的线性时间解决方案（如 KMP 和 Z 算法），再到巧妙的启发式方法（如 Boyer-Moore 和 Rabin-Karp）。

#### 1. 问题设定

给定：
- 文本：$T = t_1 t_2 \ldots t_n$
- 模式：$P = p_1 p_2 \ldots p_m$

目标：找到所有满足下式的 ( i )
$$
T[i \ldots i+m-1] = P[1 \ldots m]
$$

朴素解法：将 ( P ) 与 ( T ) 的每个子串进行比较
时间复杂度：( O(nm) )

我们现在将看到如何将其降低到 ( O(n + m) ) 或接近这个复杂度。

#### 2. Knuth-Morris-Pratt (KMP)

KMP 通过预先计算模式内部的重叠部分来避免重新检查字符。

它构建一个前缀函数（也称为失效函数），该函数指示当发生不匹配时应该移动多少。

#### A. 前缀函数

对于每个位置 ( i )，计算 $\pi[i]$ = ( P[1..i] ) 的最长前缀的长度，该前缀同时也是其后缀。

示例：
模式 `ababc`

| i | P[i] | π[i] |
| - | ---- | ---- |
| 1 | a    | 0    |
| 2 | b    | 0    |
| 3 | a    | 1    |
| 4 | b    | 2    |
| 5 | c    | 0    |

#### B. 搜索阶段

使用 $\pi[]$ 来跳过文本中不匹配的前缀。

时间复杂度：( O(n + m) )
空间复杂度：( O(m) )

微型代码（C语言）

```c
void compute_pi(char *p, int m, int pi[]) {
    pi[0] = 0;
    for (int i = 1, k = 0; i < m; i++) {
        while (k > 0 && p[k] != p[i]) k = pi[k-1];
        if (p[k] == p[i]) k++;
        pi[i] = k;
    }
}

void kmp_search(char *t, char *p) {
    int n = strlen(t), m = strlen(p);
    int pi[m]; compute_pi(p, m, pi);
    for (int i = 0, k = 0; i < n; i++) {
        while (k > 0 && p[k] != t[i]) k = pi[k-1];
        if (p[k] == t[i]) k++;
        if (k == m) {
            printf("在 %d 处找到\n", i - m + 1);
            k = pi[k-1];
        }
    }
}
```

#### 3. Z 算法

Z 算法计算 Z 数组，
其中 $Z[i]$ = 从 $i$ 开始的最长子串的长度，该子串与 $P$ 的前缀匹配。

为了在 $T$ 中匹配 $P$，构建字符串：

$$
S = P + \# + T
$$

那么，每个满足 $Z[i] = |P|$ 的 $i$ 都对应一个匹配。

时间复杂度：$O(n + m)$
简单而优雅。

示例：

```
P = "aba", T = "ababa"
S = "aba#ababa"
Z = [0,0,1,0,3,0,1,0]
在索引 0, 2 处匹配
```

#### 4. Rabin-Karp（滚动哈希）

不是逐个字符地比较字符串，而是计算 ( T ) 中每个窗口的哈希值，然后比较哈希值。

$$
h(s_1s_2\ldots s_m) = (s_1b^{m-1} + s_2b^{m-2} + \ldots + s_m) \bmod M
$$

使用滚动哈希，每次移动以 ( O(1) ) 的时间更新。

时间复杂度：平均 ( O(n + m) )，最坏 ( O(nm) )
适用于多模式搜索。

微型代码（滚动哈希）

```c
#define B 256
#define M 101

void rabin_karp(char *t, char *p) {
    int n = strlen(t), m = strlen(p);
    int h = 1, pHash = 0, tHash = 0;
    for (int i = 0; i < m-1; i++) h = (h*B) % M;
    for (int i = 0; i < m; i++) {
        pHash = (B*pHash + p[i]) % M;
        tHash = (B*tHash + t[i]) % M;
    }
    for (int i = 0; i <= n-m; i++) {
        if (pHash == tHash && strncmp(&t[i], p, m) == 0)
            printf("在 %d 处找到\n", i);
        if (i < n-m)
            tHash = (B*(tHash - t[i]*h) + t[i+m]) % M;
        if (tHash < 0) tHash += M;
    }
}
```

#### 5. Boyer-Moore（启发式跳过）

Boyer-Moore 从右向左比较，并使用两种启发式方法：

1. 坏字符规则
   当在 ( j ) 处不匹配时，移动模式，使 ( P ) 中下一个出现的 ( T[i] ) 对齐。

2. 好后缀规则
   移动模式，使已匹配部分的后缀与另一个出现对齐。

时间复杂度：平均 ( O(n/m) )
实用且快速，尤其适用于英文文本。

#### 6. 总结

| 算法         | 时间复杂度     | 空间复杂度     | 核心思想           | 最佳适用场景   |
| ------------ | -------------- | -------------- | ------------------ | -------------- |
| 朴素算法     | (O(nm))        | (O(1))         | 直接比较           | 简单情况       |
| KMP          | (O(n+m))       | (O(m))         | 前缀重叠           | 通用用途       |
| Z 算法       | (O(n+m))       | (O(n+m))       | 前缀匹配           | 模式预处理     |
| Rabin-Karp   | (O(n+m)) 平均  | (O(1))         | 哈希               | 多模式搜索     |
| Boyer-Moore  | (O(n/m)) 平均  | (O$m+\sigma$)  | 从右向左跳过       | 长文本         |

#### 为何重要

字符串匹配为文本编辑器、DNA 搜索、垃圾邮件过滤器和搜索引擎提供动力。这些算法展示了结构和巧妙的预处理如何将暴力方法转化为优雅的解决方案。

> "寻找是人之常情，高效匹配则是神技。"

#### 亲自尝试

1.  实现 KMP 并打印句子中的所有匹配项。
2.  使用 Rabin-Karp 查找多个关键词。
3.  在大型文本文件上比较运行时间。
4.  修改 KMP 以实现不区分大小写的匹配。
5.  逐步可视化前缀函数的计算过程。

掌握这些算法，你将掌握模式发现的基础，即在符号流中寻找秩序的艺术。
### 62. 多模式搜索（Aho-Corasick）

到目前为止，我们都是在文本中匹配单个模式。但如果有很多模式，比如说一个关键词词典，而我们想在一次扫描中找到所有模式的所有出现位置呢？

这正是 Aho-Corasick 算法大显身手的地方。它构建一个带有失败链接的字典树，将多个模式转化为一个高效的状态机。可以把它看作是“一次处理多个单词的 KMP 算法”。

#### 1. 问题设定

给定：
- 一个长度为 \( n \) 的文本 \( T \)
- 一组模式 \( \{ P_1, P_2, \ldots, P_k \} \)，其总长度为 \( m = \sum |P_i| \)

目标：找到每个 \( P_i \) 在 \( T \) 中的所有出现位置。

朴素解法：
对每个模式运行 KMP 算法，时间复杂度 \( O(kn) \)。

更好的想法：
将所有模式合并到一个字典树中，并使用失败链接来处理失配情况。

Aho-Corasick 算法可以达到 \( O(n + m + z) \) 的时间复杂度，其中 \( z \) 是报告出的匹配总数。

#### 2. 字典树构建

将每个模式逐节点插入到字典树中。

示例模式：
```
he, she, his, hers
```

字典树：
```
(根节点)
 ├─ h ─ e*
 │   └─ r ─ s*
 ├─ s ─ h ─ e*
 └─ h ─ i ─ s*
```

每个节点可以标记一个输出（表示模式结束）。

#### 3. 失败链接

一个节点的失败链接指向字典树中最长的、同时也是某个模式前缀的**真后缀**。

这些链接让我们可以像 KMP 算法那样“回退”。

当发生失配时，沿着失败链接找到下一个可能的匹配位置。

#### 构建失败链接（广度优先搜索）

1. 根节点的失败链接 = null
2. 根节点的子节点 → 失败链接 = 根节点
3. 对节点进行广度优先搜索：
   - 对于每条边 \( (u, c) \rightarrow v \)：沿着 \( u \) 的失败链接向上查找，直到找到一个节点 \( f \)，它有一条标记为 \( c \) 的边，然后令 \( v.\text{fail} = f.c \)

#### 示例

对于模式 "he", "she", "his", "hers"：
- `fail("he") = 根节点`
- `fail("hers") = "rs"` 路径无效 → 回退到 `"s"`（如果存在）

因此，失败链接连接的是部分后缀。

#### 4. 匹配阶段

现在我们可以一次性处理文本：

```
state = root
for each character c in text:
    while state has no child c and state != root:
        state = state.fail
    if state has child c:
        state = state.child[c]
    else:
        state = root
    if state.output:
        report matches at this position
```

每次状态转移的摊还代价是 \( O(1) \)。无需回溯，完全是线性时间。

#### 5. 示例演练

模式：`he`, `she`, `his`, `hers`
文本：`ahishers`

在每个字符处的状态变化：
```
a → 根节点 (无匹配)
h → 转移到 h
i → 转移到 hi
s → 转移到 his → 输出 "his"
h → 回退 → h
e → he → 输出 "he"
r → her → 继续
s → hers → 输出 "hers"
```

输出：`"his"`, `"he"`, `"hers"`

#### 6. 精简代码（C 语言实现概览）

```c
#define ALPHA 26

typedef struct Node {
    struct Node *next[ALPHA];
    struct Node *fail;
    int out;
} Node;

Node* newNode() {
    Node *n = calloc(1, sizeof(Node));
    return n;
}

void insert(Node *root, char *p) {
    for (int i = 0; p[i]; i++) {
        int c = p[i] - 'a';
        if (!root->next[c]) root->next[c] = newNode();
        root = root->next[c];
    }
    root->out = 1;
}

void build_failures(Node *root) {
    Node *q[10000];
    int front=0, back=0;
    root->fail = root;
    q[back++] = root;
    while (front < back) {
        Node *u = q[front++];
        for (int c=0; c<ALPHA; c++) {
            Node *v = u->next[c];
            if (!v) continue;
            Node *f = u->fail;
            while (f != root && !f->next[c]) f = f->fail;
            if (f->next[c] && f->next[c] != v) v->fail = f->next[c];
            else v->fail = root;
            if (v->fail->out) v->out = 1;
            q[back++] = v;
        }
    }
}
```

#### 7. 复杂度

| 阶段         | 时间         | 空间    |
| ------------ | ------------ | ------- |
| 字典树构建   | \( O(m) \)   | \( O(m) \) |
| 失败链接构建 | \( O(m) \)   | \( O(m) \) |
| 搜索         | \( O(n + z) \) | \( O(1) \) |

总计：\( O(n + m + z) \)

#### 8. 总结

| 步骤       | 目的             |
| ---------- | ---------------- |
| 字典树     | 合并模式         |
| 失败链接   | 处理失配         |
| 输出       | 收集匹配         |
| 广度优先搜索 | 高效构建         |
| 一次扫描   | 匹配所有模式     |

#### 为何重要

Aho-Corasick 是以下应用的核心：
- 垃圾邮件过滤器
- 入侵检测系统（例如 Snort IDS）
- 编译器中的关键词搜索
- DNA 序列扫描器

它是将自动机理论与实际效率完美结合的典范。

> “当你的算法可以读取整个词典时，何必一次只搜索一个单词呢？”

#### 动手尝试

1.  为单词 {"he", "she", "hers"} 构建一个状态机并手动追踪。
2.  修改代码以支持大写字母。
3.  扩展功能以报告重叠的匹配。
4.  测量其运行时间并与朴素的多模式搜索进行比较。
5.  在图中可视化失败链接。

一旦你掌握了 Aho-Corasick 算法，你就会将模式搜索不再视为一个循环，而是一个能够读取和识别的机器。
### 63. 后缀结构（后缀数组、后缀树、LCP）

基于后缀的数据结构是字符串算法中最强大的工具之一。它们支持快速搜索、子串查询、模式匹配和字典序操作，所有这些都源于一个基本思想：

> 以结构化的形式表示字符串的所有后缀。

在本节中，我们将探讨三种关键结构：

- **后缀数组** - 按字典序排序的后缀索引
- **最长公共前缀数组** - 相邻后缀之间的共享前缀长度
- **后缀树** - 所有后缀的压缩字典树

它们共同为文本处理、生物信息学和压缩中的许多高级算法提供动力。

#### 1. 后缀数组

后缀数组按字典序存储字符串的所有后缀，用它们的起始索引表示。

示例：
字符串 `banana$`
所有后缀：

| 索引 | 后缀    |
| ---- | ------- |
| 0    | banana$ |
| 1    | anana$  |
| 2    | nana$   |
| 3    | ana$    |
| 4    | na$     |
| 5    | a$      |
| 6    | $       |

对它们进行排序：

| 排序顺序 | 后缀     | 索引 |
| -------- | -------- | ---- |
| 0        | `$`      | 6    |
| 1        | `a$`     | 5    |
| 2        | `ana$`   | 3    |
| 3        | `anana$` | 1    |
| 4        | `banana$`| 0    |
| 5        | `na$`    | 4    |
| 6        | `nana$`  | 2    |

后缀数组：`[6, 5, 3, 1, 0, 4, 2]`

#### 构造（前缀倍增法）

我们通过基数排序对后缀的排名对进行排序，迭代地按前 2ⁱ 个字符对后缀进行排序。

步骤：

1.  按字符分配初始排名。
2.  按 (rank[i], rank[i+k]) 排序。
3.  重复倍增 $k \leftarrow 2k$ 直到所有排名都不同。

时间复杂度：$O(n \log n)$
空间复杂度：$O(n)$

微型代码（C语言，草图）

```c
typedef struct { int idx, rank[2]; } Suffix;
int cmp(Suffix a, Suffix b) {
    return (a.rank[0]==b.rank[0]) ? (a.rank[1]-b.rank[1]) : (a.rank[0]-b.rank[0]);
}

void buildSA(char *s, int n, int sa[]) {
    Suffix suf[n];
    for (int i = 0; i < n; i++) {
        suf[i].idx = i;
        suf[i].rank[0] = s[i];
        suf[i].rank[1] = (i+1<n) ? s[i+1] : -1;
    }
    for (int k = 2; k < 2*n; k *= 2) {
        qsort(suf, n, sizeof(Suffix), cmp);
        int r = 0, rank[n]; rank[suf[0].idx]=0;
        for (int i=1;i<n;i++) {
            if (suf[i].rank[0]!=suf[i-1].rank[0] || suf[i].rank[1]!=suf[i-1].rank[1]) r++;
            rank[suf[i].idx]=r;
        }
        for (int i=0;i<n;i++){
            suf[i].rank[0] = rank[suf[i].idx];
            suf[i].rank[1] = (suf[i].idx+k/2<n)?rank[suf[i].idx+k/2]:-1;
        }
    }
    for (int i=0;i<n;i++) sa[i]=suf[i].idx;
}
```

#### 2. 最长公共前缀数组

LCP 数组存储了后缀数组中按顺序排列的相邻后缀之间的最长公共前缀的长度。

示例：`banana$`

| SA | 后缀    | LCP |
| -- | ------- | --- |
| 6  | $       | 0   |
| 5  | a$      | 0   |
| 3  | ana$    | 1   |
| 1  | anana$  | 3   |
| 0  | banana$ | 0   |
| 4  | na$     | 0   |
| 2  | nana$   | 2   |

所以 LCP = `[0,0,1,3,0,0,2]`

#### Kasai 算法（$O(n)$ 时间构建）

我们使用后缀数组的逆数组一次性计算 LCP：

```c
void buildLCP(char *s, int n, int sa[], int lcp[]) {
    int rank[n];
    for (int i=0;i<n;i++) rank[sa[i]]=i;
    int k=0;
    for (int i=0;i<n;i++) {
        if (rank[i]==n-1) { k=0; continue; }
        int j = sa[rank[i]+1];
        while (i+k<n && j+k<n && s[i+k]==s[j+k]) k++;
        lcp[rank[i]]=k;
        if (k>0) k--;
    }
}
```

时间复杂度：$O(n)$

#### 3. 后缀树

后缀树是所有后缀的压缩字典树。

每条边保存一个子串区间，而不是单个字符。
这带来了：

- $O(n)$ 时间的构造（Ukkonen 算法）
- $O(m)$ 时间的模式搜索
- 许多高级用途（例如，最长重复子串）

示例：
字符串：`banana$`
后缀树边：

```
(根节点)
 ├─ b[0:0] → ...
 ├─ a[1:1] → ...
 ├─ n[2:2] → ...
```

边将连续的字母压缩成类似 `[起始:结束]` 的区间。

#### 比较

| 结构         | 空间      | 构建时间         | 搜索             |
| ------------ | --------- | ---------------- | ---------------- |
| 后缀数组     | $O(n)$    | $O(n \log n)$    | $O(m \log n)$    |
| LCP 数组     | $O(n)$    | $O(n)$           | 范围查询         |
| 后缀树       | $O(n)$    | $O(n)$           | $O(m)$           |

后缀数组 + LCP ≈ 紧凑的后缀树。

#### 4. 应用

1.  子串搜索 - 在后缀数组中进行二分查找
2.  最长重复子串 - LCP 数组的最大值
3.  字典序 - 直接从后缀数组获得
4.  不同子串计数 = $n(n+1)/2 - \sum LCP[i]$
5.  模式频率 - 使用 LCP 在后缀数组中进行范围查询

#### 5. 微型代码（通过后缀数组搜索）

```c
int searchSA(char *t, int n, char *p, int sa[]) {
    int l=0, r=n-1, m=strlen(p);
    while (l <= r) {
        int mid = (l+r)/2;
        int cmp = strncmp(t+sa[mid], p, m);
        if (cmp==0) return sa[mid];
        else if (cmp<0) l=mid+1;
        else r=mid-1;
    }
    return -1;
}
```

#### 6. 总结

| 概念         | 目的                     | 复杂度           |
| ------------ | ------------------------ | ---------------- |
| 后缀数组     | 排序后的后缀索引         | $O(n \log n)$    |
| LCP 数组     | 相邻后缀的重叠部分       | $O(n)$           |
| 后缀树       | 后缀的压缩字典树         | $O(n)$           |

它们共同构成了高级字符串算法的核心。

#### 重要性

后缀结构揭示了字符串中隐藏的秩序。它们将原始文本转化为可搜索、可分析的数据，非常适合压缩、搜索引擎和 DNA 分析。

> "所有后缀，完美排序，文本的 DNA。"

#### 动手实践

1.  手动构建 `banana$` 的后缀数组。
2.  编写代码计算 LCP 和最长重复子串。
3.  使用后缀数组上的二分查找搜索多个模式。
4.  根据后缀数组和 LCP 计算不同子串的数量。
5.  比较基于后缀数组的搜索与基于树的搜索性能。

掌握后缀结构使你能够解决那些曾经对于暴力方法来说"太大"的问题，现在可以用优雅和秩序来解决。
### 64. 回文与周期性（马拉车算法）

回文是正读反读都相同的对称字符串，
例如 "level"、"racecar" 或 "madam"。
它们自然地出现在文本分析、生物信息学乃至数据压缩中。

本节介绍用于检测和分析字符串中回文结构与周期性的高效算法，包括传奇的马拉车算法，该算法能在线性时间内找到所有回文子串。

#### 1. 什么是回文？

如果字符串 ( S ) 满足以下条件，则它是一个回文：
$$
S[i] = S[n - i + 1] \quad \text{对所有 } i \text{ 成立}
$$

示例：

- `"abba"` 是偶数长度回文
- `"aba"` 是奇数长度回文

一个字符串可能包含许多回文子串，我们的目标是高效地找到所有中心。

#### 2. 朴素方法

对于每个中心（字符之间或字符处），
在字符匹配时向外扩展。

```c
for each center c:
    expand left, right while S[l] == S[r]
```

复杂度： ( O(n^2) ) ，对于长字符串来说太慢。

我们需要更快的算法，这就是马拉车算法登场的时候。

#### 3. 马拉车算法 (O(n))

马拉车算法在线性时间内找到以每个位置为中心的最长回文的半径。

它巧妙地利用镜像对称性和当前右边界来复用先前的计算结果。

#### 逐步解析

1.  预处理字符串以处理偶数长度回文：
    在字符之间插入 `#`。

    示例：

    ```
    S = "abba"
    T = "^#a#b#b#a#$"
    ```

    (`^` 和 `$` 是哨兵字符)

2.  维护：

    - `C`：最右回文的中心
    - `R`：右边界
    - `P[i]`：在 `i` 处的回文半径

3.  对于每个位置 `i`：

    - 镜像位置 `mirror = 2*C - i`
    - 初始化 `P[i] = min(R - i, P[mirror])`
    - 围绕 `i` 扩展，直到字符不匹配
    - 如果新回文延伸超过 `R`，则更新 `C` 和 `R`

4.  `P[i]` 的最大值给出最长回文。

#### 示例

```
S = "abba"
T = "^#a#b#b#a#$"
P = [0,0,1,0,3,0,3,0,1,0,0]
最长半径 = 3 → "abba"
```

#### 精简代码（C 语言实现）

```c
int manacher(char *s) {
    int n = strlen(s);
    char t[2*n + 3];
    int p[2*n + 3];
    int m = 0;
    t[m++] = '^';
    for (int i=0;i<n;i++) {
        t[m++] = '#';
        t[m++] = s[i];
    }
    t[m++] = '#'; t[m++] = '$';
    t[m] = '\0';
    
    int c = 0, r = 0, maxLen = 0;
    for (int i=1; i<m-1; i++) {
        int mirror = 2*c - i;
        if (i < r)
            p[i] = (r - i < p[mirror]) ? (r - i) : p[mirror];
        else p[i] = 0;
        while (t[i + 1 + p[i]] == t[i - 1 - p[i]])
            p[i]++;
        if (i + p[i] > r) {
            c = i;
            r = i + p[i];
        }
        if (p[i] > maxLen) maxLen = p[i];
    }
    return maxLen;
}
```

时间复杂度： ( O(n) )
空间复杂度： ( O(n) )

#### 4. 周期性与重复

如果字符串 ( S ) 满足以下条件，则它具有周期 ( p )：
$$
S[i] = S[i + p] \text{ 对所有有效的 } i \text{ 成立}
$$

示例：
`abcabcabc` 的周期是 3（`abc`）。

检查周期性：

1.  构建前缀函数（如 KMP 算法中所示）。
2.  令 ( n = |S| )，$p = n - \pi[n-1]$。
3.  如果 $n \mod p = 0$，则周期 = ( p )。

示例：

```
S = "ababab"
π = [0,0,1,2,3,4]
p = 6 - 4 = 2
6 mod 2 = 0 → 具有周期性
```

#### 精简代码（检查周期性）

```c
int period(char *s) {
    int n = strlen(s), pi[n];
    pi[0]=0;
    for(int i=1,k=0;i<n;i++){
        while(k>0 && s[k]!=s[i]) k=pi[k-1];
        if(s[k]==s[i]) k++;
        pi[i]=k;
    }
    int p = n - pi[n-1];
    return (n % p == 0) ? p : n;
}
```

#### 5. 应用

-   回文查询：子串 ( S[l:r] ) 是回文吗？ → 预计算半径
-   最长回文子串
-   DNA 对称性分析
-   模式压缩 / 周期检测
-   字符串规律性测试

#### 6. 总结

| 概念         | 用途                     | 时间复杂度 |
| ------------ | ------------------------ | ---------- |
| 朴素扩展     | 简单回文检查             | ( O(n^2) ) |
| 马拉车算法   | 最长回文子串             | ( O(n) )   |
| KMP 前缀函数 | 周期检测                 | ( O(n) )   |

#### 为何重要

回文揭示了隐藏的对称性。
马拉车算法是一颗明珠，它是一个基于镜像的线性时间解决方案，解决了一个原本是二次复杂度的问题。

> "在每个词语中，都可能隐藏着一个倒影。"

#### 动手实践

1.  在 `"abacdfgdcaba"` 上运行马拉车算法。
2.  修改代码以打印所有回文子串。
3.  使用前缀函数找到最小周期。
4.  结合两者以找到具有周期性的回文子串。
5.  与朴素扩展方法的运行时间进行比较。

理解回文和周期性，可以学习结构如何从重复中产生，这是所有算法设计中的一个核心主题。
### 65. 编辑距离与序列比对

编辑距离衡量两个字符串的差异程度，即一个字符串转换为另一个字符串所需的最少操作次数。它是拼写检查、DNA序列比对、抄袭检测和模糊搜索的基石。

最常见的形式是莱文斯坦距离（Levenshtein distance），使用以下操作：
- 插入（增加一个字符）
- 删除（移除一个字符）
- 替换（替换一个字符）

我们也将简要介绍序列比对，它通过自定义的得分和罚分来推广这一概念。

#### 1. 问题定义

给定两个字符串 ( A ) 和 ( B )，找到将 $A$ 转换为 $B$ 所需的最少编辑次数。

如果
( A = "kitten" )
( B = "sitting" )

一个最优操作序列是：

```
kitten → sitten (将 'k' 替换为 's')
sitten → sittin (将 'e' 替换为 'i')
sittin → sitting (插入 'g')
```

因此编辑距离 = 3。

#### 2. 动态规划解法

令 $dp[i][j]$ 为将 $A[0..i-1]$ 转换为 $B[0..j-1]$ 所需的最小编辑次数。

递推关系：
$$
dp[i][j] =
\begin{cases}
dp[i-1][j-1], & \text{如果 } A[i-1] = B[j-1], \\
1 + \min\big(dp[i-1][j],\, dp[i][j-1],\, dp[i-1][j-1]\big), & \text{否则}
\end{cases}
$$

其中：
- $dp[i-1][j]$：从 $A$ 中删除
- $dp[i][j-1]$：向 $A$ 中插入
- $dp[i-1][j-1]$：替换

基本情况：
$$
dp[0][j] = j,\quad dp[i][0] = i
$$

时间复杂度：$O(|A||B|)$

#### 示例

A = `kitten`, B = `sitting`

|    | "" | s | i | t | t | i | n | g |
| -- | -- | - | - | - | - | - | - | - |
| "" | 0  | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
| k  | 1  | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
| i  | 2  | 2 | 1 | 2 | 3 | 4 | 5 | 6 |
| t  | 3  | 3 | 2 | 1 | 2 | 3 | 4 | 5 |
| t  | 4  | 4 | 3 | 2 | 1 | 2 | 3 | 4 |
| e  | 5  | 5 | 4 | 3 | 2 | 2 | 3 | 4 |
| n  | 6  | 6 | 5 | 4 | 3 | 3 | 2 | 3 |

编辑距离 = 3

精简代码（C语言）

```c
#include <stdio.h>
#include <string.h>
#define MIN3(a,b,c) ((a<b)?((a<c)?a:c):((b<c)?b:c))

int edit_distance(char *A, char *B) {
    int n = strlen(A), m = strlen(B);
    int dp[n+1][m+1];
    for (int i=0;i<=n;i++) dp[i][0]=i;
    for (int j=0;j<=m;j++) dp[0][j]=j;
    for (int i=1;i<=n;i++)
        for (int j=1;j<=m;j++)
            if (A[i-1]==B[j-1])
                dp[i][j]=dp[i-1][j-1];
            else
                dp[i][j]=1+MIN3(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]);
    return dp[n][m];
}

int main() {
    printf("%d\n", edit_distance("kitten","sitting")); // 3
}
```

#### 3. 空间优化

我们只需要前一行来计算当前行。

因此，
空间复杂度：$O(\min(|A|, |B|))$

```c
int edit_distance_opt(char *A, char *B) {
    int n=strlen(A), m=strlen(B);
    int prev[m+1], curr[m+1];
    for(int j=0;j<=m;j++) prev[j]=j;
    for(int i=1;i<=n;i++){
        curr[0]=i;
        for(int j=1;j<=m;j++){
            if(A[i-1]==B[j-1]) curr[j]=prev[j-1];
            else curr[j]=1+MIN3(prev[j], curr[j-1], prev[j-1]);
        }
        memcpy(prev,curr,sizeof(curr));
    }
    return prev[m];
}
```

#### 4. 序列比对

序列比对展示两个字符串之间哪些字符是相互对应的。用于生物信息学（例如，DNA序列比对）。

每个操作都有一个代价：
- 匹配：0
- 不匹配：1
- 空缺（插入/删除）：1

我们以类似的方式填充DP表，但会跟踪选择以回溯比对结果。

#### 序列比对示例

```
A: kitten-
B: sitt-ing
```

我们可以通过回溯dp表来可视化转换路径。

#### 序列比对的评分（通用形式）

我们可以将其推广：
$$
dp[i][j] = \min \begin{cases}
dp[i-1][j-1] + cost(A_i,B_j) \\
dp[i-1][j] + gap \\
dp[i][j-1] + gap
\end{cases}
$$

用于尼德曼-翁施算法（全局比对）和史密斯-沃特曼算法（局部比对）。

#### 5. 变体

- 达梅劳-莱文斯坦距离：增加相邻字符交换操作
- 汉明距离：仅允许替换，且字符串长度必须相等
- 加权距离：不同操作有不同的代价
- 局部比对：仅寻找最佳匹配的子串

#### 6. 总结

| 方法                           | 操作                     | 时间复杂度 | 用途                       |
| -------------------------------- | ----------------------- | ------- | ------------------------- |
| 莱文斯坦距离                      | 插入、删除、替换 | $O(nm)$ | 拼写检查、模糊搜索 |
| 汉明距离                          | 仅替换       | $O(n)$  | DNA、二进制字符串       |
| 序列比对（尼德曼-翁施算法）     | 带评分            | $O(nm)$ | 生物信息学            |
| 局部比对（史密斯-沃特曼算法） | 最佳子串          | $O(nm)$ | DNA区域比对               |

#### 为何重要

编辑距离将"差异"转化为数据。它量化了两个字符串之间的距离，实现了灵活、鲁棒的比较。

> "相似性不是完美，而是变得相似所需的代价。"

#### 动手实践

1.  计算 "intention" 和 "execution" 之间的编辑距离。
2.  回溯操作以展示序列比对。
3.  修改代价（插入=2，删除=1，替换=2）并比较结果。
4.  为等长字符串实现汉明距离。
5.  探索史密斯-沃特曼算法以寻找最长公共子串。

一旦你掌握了编辑距离，你就能构建出理解拼写错误、比对基因组以及进行不完美但完美的搜索的工具。
### 66. 压缩（霍夫曼编码、算术编码、LZ77、BWT）

压缩算法让我们能够高效地编码信息，在不丢失意义的前提下减少存储或传输成本。它们将模式和冗余转换为更短的表示，这就是数据压缩的本质。

本节将介绍构成 ZIP、PNG 和 GZIP 等格式基础的关键无损压缩算法家族。

我们将探讨：

- 霍夫曼编码（前缀无关变长编码）
- 算术编码（分数区间编码）
- LZ77 / LZ78（基于字典的方法）
- 布劳斯-惠勒变换（BWT）（可逆排序变换）

#### 1. 霍夫曼编码

霍夫曼编码为频繁出现的符号分配较短的代码，为罕见的符号分配较长的代码，从而在前缀无关编码中实现最优压缩。

#### A. 算法

1.  统计所有符号的频率。
2.  构建一个节点 `(符号, 频率)` 的最小堆。
3.  当堆大小 > 1 时：
    - 提取两个最小的节点 `a`, `b`。
    - 创建一个新节点，其 `频率 = a.频率 + b.频率`。
    - 将新节点推回堆中。
4.  为左分支分配 `0`，为右分支分配 `1`。
5.  遍历树，收集编码。

每个符号获得一个唯一的前缀码（没有任何一个代码是另一个代码的前缀）。

#### B. 示例

文本：`ABRACADABRA`

频率：

| 符号 | 计数 |
| ---- | ---- |
| A    | 5    |
| B    | 2    |
| R    | 2    |
| C    | 1    |
| D    | 1    |

构建树得到类似以下的编码：

```
A: 0
B: 101
R: 100
C: 1110
D: 1111
```

编码后的文本：`0 101 100 0 1110 0 1111 0 101 100 0`
实现了压缩！

微型代码（C 语言，概念示意）

```c
typedef struct Node {
    char ch;
    int freq;
    struct Node *left, *right;
} Node;
```

使用最小堆（优先队列）来构建树。
递归遍历以打印码字。

复杂度：$O(n \log n)$

#### 2. 算术编码

算术编码不是将符号映射到比特串，而是将整个消息映射到 [0,1) 区间内的一个单一数字。

我们从区间 [0,1) 开始，然后根据符号概率迭代地缩小它。

#### 示例

符号：{A: 0.5, B: 0.3, C: 0.2}
消息：`ABC`

区间：

```
起始: [0, 1)
A → [0, 0.5)
B → [0.25, 0.4)
C → [0.34, 0.37)
```

最终编码 = [0.34, 0.37) 内的任意数字（例如 0.35）

解码过程与此相反。

优点：实现接近最优的熵压缩。
用于：JPEG2000, H.264

时间复杂度：$O(n)$

#### 3. LZ77（滑动窗口压缩）

LZ77 用指向滑动窗口内的反向引用 `(偏移量, 长度, 下一个字符)` 来替换重复的子串。

#### 示例

文本：`abcabcabcx`

窗口滑动；当 `abc` 重复时：

```
(0,0,'a'), (0,0,'b'), (0,0,'c'),
(3,3,'x')  // "abc" 从 3 个字符前开始重复
```

因此序列被压缩为对先前子串的引用。

用于：DEFLATE (ZIP, GZIP), PNG

时间：$O(n)$，空间：与窗口大小成正比。

#### 微型代码（概念性）

```c
struct Token { int offset, length; char next; };
```

在发出令牌之前，在之前的窗口中搜索最长匹配。

#### 4. LZ78（基于字典）

LZ78 不是使用滑动窗口，而是构建一个显式的子串字典。

算法：

- 从空字典开始。
- 读取输入，在字典中找到最长的前缀。
- 输出 `(索引, 下一个字符)` 并插入新条目。

示例：

```
输入: ABAABABAABAB
输出: (0,A), (0,B), (1,B), (2,A), (4,A), (3,B)
```

用于：LZW (GIF, TIFF)

#### 5. 布劳斯-惠勒变换（BWT）

BWT 本身不是压缩，它通过置换文本来聚集相似的字符，使其更容易通过游程编码或霍夫曼编码进行压缩。

#### 步骤

1.  生成字符串的所有旋转。
2.  按字典序对它们进行排序。
3.  取最后一列作为输出。

示例：`banana$`

| 旋转      | 排序后    |
| --------- | --------- |
| banana$   | $banana   |
| anana$b   | a$banna   |
| nana$ba   | ana$ban   |
| ana$ban   | anana$b   |
| na$bana   | banana$   |
| a$banan   | na$bana   |
| $banana   | nana$ba   |

最后一列：`annb$aa`
BWT("banana$") = "annb$aa"

通过原始行的索引可以逆变换。

用于：bzip2, FM-index（生物信息学）

#### 6. 总结

| 算法       | 思想                       | 复杂度          | 用途                 |
| ---------- | -------------------------- | --------------- | -------------------- |
| 霍夫曼编码 | 变长前缀码                 | $O(n \log n)$   | 通用压缩             |
| 算术编码   | 区间编码                   | $O(n)$          | 接近最优熵           |
| LZ77       | 滑动窗口匹配               | $O(n)$          | ZIP, PNG             |
| LZ78       | 字典构建                   | $O(n)$          | GIF, TIFF            |
| BWT        | 置换以聚集字符             | $O(n \log n)$   | bzip2                |

#### 为何重要

压缩算法揭示了数据中的结构，它们利用了人类无法看到的模式。它们也是了解信息论的一个窗口，展示了我们能够多么接近熵的极限。

> "压缩即是理解，节省的每一个比特都是一个被发现的模式。"

#### 动手尝试

1.  为 `MISSISSIPPI` 构建霍夫曼树。
2.  实现一个简单的 LZ77 编码器来处理重复模式。
3.  应用 BWT 并观察符号的聚集情况。
4.  比较相同输入下霍夫曼编码和算术编码的输出。
5.  探索结合了 LZ77 和霍夫曼编码的 DEFLATE 格式。

理解压缩意味着学会看到冗余，这是实现高效存储、传输乃至理解本身的关键。
### 67. 密码学哈希与校验和

在算法中，哈希帮助我们映射数据到固定大小的值。
但当用于安全和验证时，哈希就变成了一种密码学工具。
本节探讨密码学哈希和校验和，这些算法用于验证完整性、检测损坏和保护数据安全。

我们将了解：

- 简单校验和（奇偶校验、CRC）
- 密码学哈希函数（MD5、SHA 系列、BLAKE3）
- 抗碰撞性和原像抵抗性等特性
- 在验证、签名和存储中的实际应用

#### 1. 校验和

校验和是检测数据中意外错误的轻量级方法（不能防范攻击者）。
它们用于文件系统、网络和存储中以验证完整性。

#### A. 奇偶校验位

添加一个比特位，使总"1"的个数为偶数或奇数。
用于内存或通信中以检测单比特错误。

示例：
数据 = `1011` → 有三个"1"。添加奇偶校验位 `1` 使总数为 4（偶校验）。

局限性：只能检测奇数个比特错误。

#### B. 模和（简单校验和）

对所有字节求和（模 256 或 65536）。

微型代码（C）

```c
uint8_t checksum(uint8_t *data, int n) {
    uint32_t sum = 0;
    for (int i = 0; i < n; i++) sum += data[i];
    return (uint8_t)(sum % 256);
}
```

用途：简单的文件或数据包验证。

#### C. CRC（循环冗余校验）

CRC 将比特视为多项式的系数。
除以一个生成多项式，余数 = CRC 码。

用于以太网、ZIP 和 PNG。

示例：CRC-32，CRC-16。

有快速的硬件和基于查表的实现。

关键特性：

- 检测大多数突发错误
- 不具备密码学安全性

#### 2. 密码学哈希函数

密码学哈希函数 ( h(x) ) 将任意输入映射到固定大小的输出，满足：

1. 确定性：相同输入 → 相同输出
2. 快速计算
3. 原像抵抗性：给定 ( h(x) ) 难以找到 ( x )
4. 第二原像抵抗性：难以找到 $x' \neq x$ 使得 ( h(x') = h(x) )
5. 抗碰撞性：难以找到任何两个具有相同哈希的不同输入

| 算法     | 输出（比特） | 备注                     |
| -------- | ------------ | ------------------------ |
| MD5      | 128          | 已破解（已发现碰撞）     |
| SHA-1    | 160          | 已弃用                   |
| SHA-256  | 256          | 标准（SHA-2 家族）       |
| SHA-3    | 256          | 基于 Keccak 的海绵结构   |
| BLAKE3   | 256          | 快速、并行、现代         |

#### 示例

```text
h("hello") = 2cf24dba5fb0a... (SHA-256)
```

改变一个字母，哈希值完全改变（雪崩效应）：

```text
h("Hello") = 185f8db32271f...
```

即使微小的改动 → 巨大的差异。

#### 微型代码（C，使用伪接口）

```c
#include <openssl/sha.h>

unsigned char hash[SHA256_DIGEST_LENGTH];
SHA256((unsigned char*)"hello", 5, hash);
```

将哈希值打印为十六进制字符串进行验证。

#### 3. 应用

- 数据完整性：验证文件（例如，SHA256SUM）
- 数字签名：对哈希值签名，而非原始数据
- 密码存储：存储哈希值，而非明文
- 去重：通过哈希检测相同文件
- 区块链：用哈希指针链接区块
- Git：通过 SHA-1 标识符存储对象

#### 4. 哈希碰撞

当 $x \neq y$ 时 ( h(x) = h(y) ) 发生碰撞。
良好的密码学哈希函数使得这在计算上不可行。

根据生日悖论，对于一个 ( n ) 比特的哈希，大约在 $2^{n/2}$ 次操作后会出现碰撞。

因此，SHA-256 → 约需 $2^{128}$ 次尝试才能发生碰撞。

#### 5. 校验和 vs 哈希

| 特性       | 校验和       | 密码学哈希                 |
| ---------- | ------------ | -------------------------- |
| 目标       | 检测错误     | 确保完整性和真实性         |
| 抵抗力     | 低           | 高                         |
| 输出大小   | 小           | 128-512 比特               |
| 性能       | 非常快       | 快速但安全                 |
| 示例       | CRC32        | SHA-256, BLAKE3            |

#### 为何重要

校验和捕获意外损坏，哈希防范恶意篡改。
它们共同守护着数据的可信度，这是安全系统的基础。

> "完整性是隐形的，直到失去它。"

#### 动手实践

1.  计算一个文本文件的 CRC32，翻转一个比特，然后重新计算。
2.  使用 `sha256sum` 验证文件完整性。
3.  实验：改变输入中的一个字符，观察雪崩效应。
4.  比较 SHA-256 和 BLAKE3 的性能。
5.  研究 Git 如何使用 SHA-1 跟踪版本。

通过学习哈希，你掌握了安全的支柱之一，
证明某些东西没有改变，即使其他一切都变了。
### 68. 近似匹配与流式匹配

精确字符串匹配（如 KMP 或 Boyer-Moore）要求模式与文本完全对齐。但如果存在错误、噪声或不完整数据呢？

这就是近似匹配和流式匹配的用武之地。这些算法让你即使在以下情况下也能高效搜索：
- 模式可能包含拼写错误或变异
- 文本以数据流形式到达（太大而无法完全存储）
- 你希望匹配“足够接近”，而非“完全精确”

它们在搜索引擎、拼写检查器、生物信息学和实时监控系统中至关重要。

#### 1. 近似字符串匹配

近似字符串匹配允许在存在错配、插入或删除的情况下，在文本中查找模式的出现位置，通常用编辑距离来衡量。

#### A. 动态规划（Levenshtein 距离）

给定两个字符串 $A$ 和 $B$，编辑距离是将 $A$ 转换为 $B$ 所需的最少插入、删除或替换操作次数。

我们可以构建一个 DP 表 $dp[i][j]$：
- $dp[i][0] = i$（删除所有字符）
- $dp[0][j] = j$（插入所有字符）
- 如果 $A[i] = B[j]$，则 $dp[i][j] = dp[i-1][j-1]$
- 否则 $dp[i][j] = 1 + \min(dp[i-1][j],\, dp[i][j-1],\, dp[i-1][j-1])$

微型代码（C）
```c
int edit_distance(char *a, char *b) {
    int n = strlen(a), m = strlen(b);
    int dp[n+1][m+1];
    for (int i = 0; i <= n; i++) dp[i][0] = i;
    for (int j = 0; j <= m; j++) dp[0][j] = j;

    for (int i = 1; i <= n; i++)
        for (int j = 1; j <= m; j++)
            if (a[i-1] == b[j-1]) dp[i][j] = dp[i-1][j-1];
            else dp[i][j] = 1 + fmin(fmin(dp[i-1][j], dp[i][j-1]), dp[i-1][j-1]);
    return dp[n][m];
}
```
该算法以 $O(nm)$ 的时间复杂度计算 Levenshtein 距离。

#### B. Bitap 算法（Shift-Or）

当模式长度较小时，Bitap 使用位掩码来跟踪错配。
它高效地支持最多 k 个错误，并且对于小模式，运行时间接近线性。

用于 grep -E、ag 和模糊搜索系统。

思想：维护一个位掩码，其中 1 表示错配，0 表示匹配。在扫描文本时，对掩码进行移位和 OR 操作。

#### C. k-近似匹配

查找所有编辑距离 ≤ k 的位置。
对于小的 $k$（例如，拼写纠正：编辑距离 ≤ 2）非常高效。

应用：
- 容错搜索
- DNA 序列匹配
- 自动补全系统

#### 2. 流式匹配

在流式处理中，文本太大或无限，因此我们必须在线处理输入。
我们无法存储所有内容，只能存储摘要或草图。

#### A. 滚动哈希（Rabin-Karp 风格）

维护最近字符的移动哈希值。
当新字符到达时，以 $O(1)$ 的时间复杂度更新哈希值。
与模式的哈希值进行比较以寻找可能的匹配。

适用于滑动窗口匹配。

示例：
```c
hash = (base * (hash - old_char * base^(m-1)) + new_char) % mod;
```

#### B. 指纹（Karp-Rabin 指纹）

子串的紧凑表示。
如果指纹匹配，则进行完整验证（避免误报）。
用于流式算法和分块。

#### C. 基于草图的匹配

诸如 Count-Min Sketch 或 SimHash 之类的算法构建大型数据的摘要。
它们有助于近似计算流之间的相似性。

应用：
- 近似重复检测（Google 中的 SimHash）
- 网络异常检测
- 实时日志匹配

#### 3. 实践中的近似匹配

| 领域           | 用例                   | 算法                     |
| -------------- | ---------------------- | ------------------------ |
| 拼写检查       | "recieve" → "receive"  | 编辑距离                 |
| DNA 序列比对   | 查找相似序列           | Smith-Waterman           |
| 自动补全       | 建议接近的匹配项       | 模糊搜索                 |
| 日志与数据流   | 在线模式警报           | Streaming Bitap, Karp-Rabin |
| 近似重复       | 检测相似文本           | SimHash, MinHash         |

#### 4. 复杂度

| 算法           | 时间      | 空间     | 备注                     |
| -------------- | --------- | -------- | ------------------------ |
| Levenshtein DP | $O(nm)$   | $O(nm)$  | 精确距离                 |
| Bitap          | $O(n)$    | $O(1)$   | 适用于小模式             |
| 滚动哈希       | $O(n)$    | $O(1)$   | 概率性匹配               |
| SimHash        | $O(n)$    | $O(1)$   | 近似相似性               |

#### 为何重要

现实世界的数据是混乱的，存在拼写错误、噪声、丢失、损坏。
近似匹配让你能够构建能够容忍错误并适应数据流的算法。
它支撑着从搜索引擎到基因组学的方方面面，确保你的算法在不完美的世界中保持实用性。

#### 动手尝试

1.  计算 "kitten" 和 "sitting" 之间的编辑距离。
2.  实现一个模糊搜索，返回拼写错误 ≤1 的单词。
3.  使用滚动哈希检测数据流中重复的子串。
4.  使用 SimHash 实验比较文档相似性。
5.  观察小的拼写错误如何影响模糊搜索与精确搜索。
### 69. 生物信息学序列比对（Needleman-Wunsch， Smith-Waterman）

在生物信息学中，比较 DNA、RNA 或蛋白质序列就像比较字符串，但具有生物学意义。
每个序列由字母组成（DNA 为 A、C、G、T；蛋白质为氨基酸）。
为了分析相似性，科学家使用能够处理插入、删除和替换的序列比对算法。

主要有两种基本方法：

- Needleman-Wunsch 用于全局比对
- Smith-Waterman 用于局部比对

#### 1. 序列比对

比对意味着将两个序列并排放置，以最大化匹配并最小化空位或不匹配。

例如：

```
A C G T G A
| | |   | |
A C G A G A
```

这里可能会出现不匹配和空位，但比对会在一个计分体系下找到最佳的可能匹配。

#### 计分体系

比对使用分数，而不仅仅是计数。
典型的方案是：

- 匹配： +1
- 不匹配： -1
- 空位（插入或删除）： -2

你可以根据生物学背景调整权重。

#### 2. Needleman-Wunsch（全局比对）

当你想要从头到尾对齐整个序列时使用。

它使用动态规划来构建一个分数表（`dp[i][j]`），其中每个单元格代表对齐前缀 `A[1..i]` 和 `B[1..j]` 的最佳分数。

递推关系：

$$dp[i][j] = \max
\begin{cases}
dp[i-1][j-1] + \text{score}(A_i, B_j) \\
dp[i-1][j] + \text{gap penalty} \\
dp[i][j-1] + \text{gap penalty}
\end{cases}$$

基本情况：
$$
dp[0][j] = j \times \text{gap penalty}, \quad dp[i][0] = i \times \text{gap penalty}
$$

简短代码（C）

```c
int max3(int a, int b, int c) {
    return a > b ? (a > c ? a : c) : (b > c ? b : c);
}

int needleman_wunsch(char *A, char *B, int match, int mismatch, int gap) {
    int n = strlen(A), m = strlen(B);
    int dp[n+1][m+1];
    for (int i = 0; i <= n; i++) dp[i][0] = i * gap;
    for (int j = 0; j <= m; j++) dp[0][j] = j * gap;

    for (int i = 1; i <= n; i++) {
        for (int j = 1; j <= m; j++) {
            int s = (A[i-1] == B[j-1]) ? match : mismatch;
            dp[i][j] = max3(dp[i-1][j-1] + s, dp[i-1][j] + gap, dp[i][j-1] + gap);
        }
    }
    return dp[n][m];
}
```

示例：

```
A = "ACGT"
B = "AGT"
match = +1, mismatch = -1, gap = -2
```

产生最优比对：

```
A C G T
A - G T
```

#### 3. Smith-Waterman（局部比对）

当序列可能只有相似的片段，而非全长相似时使用。
非常适合寻找基序或保守区域。

递推关系类似，但增加了局部重置为零：

$$dp[i][j] = \max
\begin{cases}
0, \\
dp[i-1][j-1] + \text{score}(A_i, B_j), \\
dp[i-1][j] + \text{gap penalty}, \\
dp[i][j-1] + \text{gap penalty}
\end{cases}$$

最终答案 = 表中的最大值（不一定在末尾）。

它寻找最佳的子串比对。

#### 示例

```
A = "ACGTTG"
B = "CGT"
```

Smith-Waterman 找到最佳局部匹配：

```
A C G T
  | | |
  C G T
```

与全局比对不同，额外的前缀或后缀被忽略。

#### 4. 变体与扩展

| 算法               | 类型       | 备注                                           |
| ------------------ | ---------- | ---------------------------------------------- |
| Needleman-Wunsch   | 全局       | 对齐整个序列                                   |
| Smith-Waterman     | 局部       | 寻找相似的子序列                               |
| Gotoh 算法         | 全局       | 使用仿射空位罚分（起始 + 延伸）                |
| BLAST              | 启发式     | 加速大型数据库的搜索                           |

BLAST（基本局部比对搜索工具）使用词种子和延伸，以牺牲精确性换取速度，这对于大型基因组数据库至关重要。

#### 5. 复杂度

Needleman-Wunsch 和 Smith-Waterman 的运行复杂度均为：

- 时间：`O(nm)`
- 空间：`O(nm)`

但优化版本使用带状动态规划或 Hirschberg 算法将内存占用降至 `O(n + m)`。

#### 为何重要

序列比对连接了计算机科学和生物学。
我们通过它：

- 比较物种
- 识别基因
- 检测突变
- 追溯祖先
- 构建系统发育树

"最小编辑代价"的思想无处不在，从拼写检查器到 DNA 分析。

> "在生物学中，相似性是一个故事。比对是我们阅读它的方式。"

#### 动手实践

1.  为短 DNA 序列实现 Needleman-Wunsch 算法。
2.  改变空位罚分，观察比对结果如何变化。
3.  比较全局比对和局部比对的输出。
4.  使用 GenBank 的真实序列进行测试。
5.  在线探索 BLAST 并与精确比对结果进行比较。
### 70. 文本索引与搜索结构

当文本变得庞大时——想想书籍、数据库或整个互联网——朴素地搜索模式（O(nm)）就太慢了。
我们需要索引结构，让我们能够快速搜索，通常达到 O(m) 或 O(log n) 的时间复杂度。

本节涵盖搜索引擎和字符串处理的核心：

- 后缀数组
- 后缀树
- 倒排索引
- 字典树和前缀树
- 压缩索引，如 FM-Index（Burrows-Wheeler）

#### 1. 为什么需要索引？

文本索引就像一本书的目录——它不存储书的内容，但能让你直接跳转到你想要的部分。

如果你有一段长度为 ( n ) 的文本，并且需要运行多次查询，那么构建一个索引是值得的（即使构建成本是 ( O$n \log n$ )）。

没有索引：每次查询耗时 ( O(nm) )。
有索引：每次查询可以降至 ( O(m) ) 或更少。

#### 2. 后缀数组

后缀数组是一个字符串所有后缀的有序数组。

对于文本 `"banana"`，后缀是：

```
0: banana
1: anana
2: nana
3: ana
4: na
5: a
```

按字典序排序：

```
5: a
3: ana
1: anana
0: banana
4: na
2: nana
```

后缀数组 = `[5, 3, 1, 0, 4, 2]`

要搜索，可以在后缀数组上对模式进行二分查找——时间复杂度 ( O$m \log n$ )。

微型代码（C）（朴素构建）

```c
int cmp(const void *a, const void *b, void *txt) {
    int i = *(int*)a, j = *(int*)b;
    return strcmp((char*)txt + i, (char*)txt + j);
}

void build_suffix_array(char *txt, int n, int sa[]) {
    for (int i = 0; i < n; i++) sa[i] = i;
    qsort_r(sa, n, sizeof(int), cmp, txt);
}
```

现代方法如前缀倍增或基数排序可以在 ( O$n \log n$ ) 时间内构建它。

应用：

- 快速子串搜索
- 最长公共前缀（LCP）数组
- DNA 序列中的模式匹配
- 抄袭检测

#### 3. 后缀树

后缀树是所有后缀的压缩字典树——每条边存储多个字符。

对于 `"banana"`，你会构建一棵树，其中每个叶子节点对应一个后缀索引。

优点：

- 模式搜索时间复杂度 ( O(m) )
- 空间复杂度 ( O(n) )（经过压缩）

使用 Ukkonen 算法在 ( O(n) ) 时间内构建。

使用后缀数组 + LCP 作为节省空间的替代方案。

#### 4. FM-Index（Burrows-Wheeler 变换）

用于压缩全文搜索（例如，Bowtie, BWA）。
结合了：

- Burrows-Wheeler 变换（BWT）
- 秩/选择位向量

支持在 O(m) 时间内进行模式搜索，且内存占用极低。

核心思想：变换文本，使相似的子串聚集在一起，从而实现压缩和反向搜索。

应用：

- DNA 序列比对
- 大型文本档案
- 内存受限的搜索

#### 5. 倒排索引

用于搜索引擎。
它不是索引后缀，而是索引单词。

例如，文本语料库：

```
doc1: quick brown fox
doc2: quick red fox
```

倒排索引：

```
"quick" → [doc1, doc2]
"brown" → [doc1]
"red"   → [doc2]
"fox"   → [doc1, doc2]
```

现在搜索 "quick fox" 就变成了列表的集合交集操作。

与排名函数（TF-IDF, BM25）结合使用。

#### 6. 字典树和前缀树

字典树逐个字符存储字符串。
每个节点 = 一个前缀。

```c
typedef struct Node {
    struct Node *child[26];
    int end;
} Node;
```

非常适合：

- 自动补全
- 前缀搜索
- 拼写检查器

搜索：O(m)，其中 m = 模式长度。

压缩字典树（Patricia 树）可以减少空间占用。

#### 7. 结构对比

| 结构         | 搜索时间    | 构建时间    | 空间占用 | 备注           |
| ------------ | ----------- | ----------- | -------- | -------------- |
| 字典树       | O(m)        | O(n)        | 高       | 前缀查询       |
| 后缀数组     | O(m log n)  | O(n log n)  | 中       | 有序后缀       |
| 后缀树       | O(m)        | O(n)        | 高       | 结构丰富       |
| FM-Index     | O(m)        | O(n)        | 低       | 压缩           |
| 倒排索引     | O(k)        | O(N)        | 中       | 基于单词       |

#### 为何重要

文本索引是搜索引擎、DNA 序列比对和自动补全系统的支柱。
没有它，谷歌搜索、代码查找或基因组扫描将需要几分钟，而不是几毫秒。

> "索引将文本的海洋变成了可导航的地图。"

#### 动手实践

1.  为 "banana" 构建后缀数组，并对 "ana" 执行二分查找。
2.  为字典构建一棵字典树，并查询前缀。
3.  为几个文档编写一个简单的倒排索引。
4.  比较后缀树与后缀数组的内存使用情况。
5.  使用在线演示（如 BWT 浏览器）体验 FM-index。

## 第 8 章. 几何、图形与空间算法
### 71. 凸包（Graham、Andrew、Chan）

在计算几何中，一个点集的凸包是包含所有点的最小凸多边形。
直观地说，想象在一块木板上用橡皮筋套住一组钉子，橡皮筋形成的形状就是凸包。

凸包是许多几何算法的基础，例如最近点对、Voronoi 图和碰撞检测。

在本节中，我们将探讨三种经典算法：

- Graham 扫描法 - 优雅而简单 (O(n log n))
- Andrew 单调链法 - 稳健而实用 (O(n log n))
- Chan 算法 - 高级且最优 (O(n log h)，其中 h = 凸包顶点数)

#### 1. 定义

给定一个点集 $P = {p_1, p_2, ..., p_n}$，
凸包 ( \text{CH}(P) ) 是包含所有点的最小凸多边形。

形式化定义：
$$
\text{CH}(P) = \bigcap {C \subseteq \mathbb{R}^2 \mid C \text{ 是凸的且 } P \subseteq C }
$$

如果一个多边形中任意两点之间的线段都完全位于该多边形内部，则该多边形是凸的。

#### 2. 方向测试

所有凸包算法都依赖于使用叉积的方向测试：
给定三个点 ( a, b, c )：

$$
\text{cross}(a,b,c) = (b_x - a_x)(c_y - a_y) - (b_y - a_y)(c_x - a_x)
$$

- `> 0` → 逆时针转向
- `< 0` → 顺时针转向
- `= 0` → 共线

#### 3. Graham 扫描法

最早的凸包算法之一。

思路：

1.  选取最低点（若 y 坐标相同，则取最左边的点）。
2.  根据相对于该点的极角对所有其他点进行排序。
3.  遍历点并维护一个栈：
    - 添加点
    - 当最后三个点构成右转时，弹出中间的点
4.  栈中剩余的点按逆时针顺序构成凸包。

微型代码 (C)

```c
typedef struct { double x, y; } Point;

double cross(Point a, Point b, Point c) {
    return (b.x - a.x)*(c.y - a.y) - (b.y - a.y)*(c.x - a.x);
}

int cmp(const void *p1, const void *p2) {
    Point *a = (Point*)p1, *b = (Point*)p2;
    // 按极角或距离比较
    return (a->y != b->y) ? (a->y - b->y) : (a->x - b->x);
}

int graham_scan(Point pts[], int n, Point hull[]) {
    qsort(pts, n, sizeof(Point), cmp);
    int top = 0;
    for (int i = 0; i < n; i++) {
        while (top >= 2 && cross(hull[top-2], hull[top-1], pts[i]) <= 0)
            top--;
        hull[top++] = pts[i];
    }
    return top; // 凸包点数
}
```

复杂度：

- 排序: ( O$n \log n$ )
- 扫描: ( O(n) ) → 总计: O(n log n)

#### 示例

输入：

```
(0, 0), (1, 1), (2, 2), (2, 0), (0, 2)
```

凸包 (逆时针)：

```
(0,0) → (2,0) → (2,2) → (0,2)
```

#### 4. Andrew 单调链法

对于浮点坐标更简单、更稳健。
分别构建下凸包和上凸包。

步骤：

1.  按字典序对点排序（先 x 后 y）。
2.  构建下凸包（从左到右）
3.  构建上凸包（从右到左）
4.  连接（排除重复点）

微型代码 (C)

```c
int monotone_chain(Point pts[], int n, Point hull[]) {
    qsort(pts, n, sizeof(Point), cmp);
    int k = 0;
    // 下凸包
    for (int i = 0; i < n; i++) {
        while (k >= 2 && cross(hull[k-2], hull[k-1], pts[i]) <= 0) k--;
        hull[k++] = pts[i];
    }
    // 上凸包
    for (int i = n-2, t = k+1; i >= 0; i--) {
        while (k >= t && cross(hull[k-2], hull[k-1], pts[i]) <= 0) k--;
        hull[k++] = pts[i];
    }
    return k-1; // 最后一个点 == 第一个点
}
```

时间复杂度: ( O$n \log n$ )

#### 5. Chan 算法

当 $h \ll n$ 时，Chan 的方法达到 ( O$n \log h$ )：

1.  将点划分为大小为 ( m ) 的组。
2.  计算每组点的凸包（使用 Graham 扫描法）。
3.  使用 Jarvis March（礼品包装法）合并凸包。
4.  巧妙地选择 ( m )（例如 $m = 2^k$）以确保 ( O$n \log h$ )。

应用领域：大规模几何处理。

#### 6. 应用

| 领域             | 用途                     |
| ---------------- | ------------------------ |
| 计算机图形学     | 形状边界、碰撞检测框     |
| GIS / 地图绘制   | 区域边界                 |
| 机器人学         | 障碍物包络               |
| 聚类分析         | 离群点检测               |
| 数据分析         | 最小包围形状             |

#### 7. 复杂度总结

| 算法             | 时间复杂度      | 空间复杂度 | 备注           |
| ---------------- | --------------- | ---------- | -------------- |
| Graham 扫描法    | ( O$n \log n$ ) | ( O(n) )   | 简单、经典     |
| 单调链法         | ( O$n \log n$ ) | ( O(n) )   | 稳定、稳健     |
| Chan 算法        | ( O$n \log h$ ) | ( O(n) )   | 渐进最优       |

#### 重要性

凸包是计算几何的基石之一。
它们教授了排序、叉积和几何推理，并且是许多空间算法的基础。

> "每个散乱的点集都隐藏着一个简单的形状。凸包就是那种隐藏的简洁性。"

#### 动手实践

1.  为 10 个随机点实现 Graham 扫描法。
2.  绘制点并验证凸包。
3.  将结果与 Andrew 单调链法进行比较。
4.  使用共线和重复点进行测试。
5.  接下来探索 3D 凸包（QuickHull、礼品包装法）。
### 72. 最近点对与线段相交

几何问题常常会问：*两点之间的最短距离是多少？* 或者*这些线段相交吗？*
这些是计算几何中的经典构建模块，对于碰撞检测、图形学、聚类和路径规划至关重要。

本节涵盖两个基础问题：

- **最近点对** - 找出具有最小欧几里得距离的两个点
- **线段相交** - 判断两条线段是否（以及在何处）相交

#### 1. 最近点对

给定二维平面上的 ( n ) 个点，找出距离最小的一对点。
暴力解法的时间复杂度是 ( O(n^2) )，但使用分治法，我们可以在 O(n log n) 内解决它。

#### A. 分治算法

思路：

1. 按 x 坐标对点进行排序。
2. 分成左半部分和右半部分。
3. 递归地找出每半部分中的最近点对（距离记为 ( d )）。
4. 合并步骤：检查分割线两侧、距离在 ( d ) 范围内的点对。

在合并步骤中，根据几何打包原理，每个点最多只需要检查 6 个邻居点。

微型代码（C，示意）

```c
#include <math.h>
typedef struct { double x, y; } Point;

double dist(Point a, Point b) {
    double dx = a.x - b.x, dy = a.y - b.y;
    return sqrt(dx*dx + dy*dy);
}

double brute_force(Point pts[], int n) {
    double d = 1e9;
    for (int i = 0; i < n; i++)
        for (int j = i + 1; j < n; j++)
            d = fmin(d, dist(pts[i], pts[j]));
    return d;
}
```

递归分治与合并：

```c
double closest_pair(Point pts[], int n) {
    if (n <= 3) return brute_force(pts, n);
    int mid = n / 2;
    double d = fmin(closest_pair(pts, mid),
                    closest_pair(pts + mid, n - mid));
    // 合并步骤：检查距离在 d 范围内的带状区域内的点
    // 按 y 排序，检查邻居点
    return d;
}
```

时间复杂度：( O(n \log n) )

示例：

点集：

```
(2,3), (12,30), (40,50), (5,1), (12,10), (3,4)
```

最近点对：(2,3) 和 (3,4)，距离 = √2

#### B. 扫描线变体

另一种方法使用扫描线和平衡树来维护活动点。
当你从左向右移动时，维护一个包含最近 ( d ) 距离内点的窗口。

用于大规模空间系统。

#### 应用

| 领域         | 用途                     |
| ------------ | ------------------------ |
| 聚类         | 寻找最近邻               |
| 机器人学     | 避免碰撞                 |
| 地理信息系统 | 最近城市搜索             |
| 网络         | 传感器邻近性检测         |

#### 2. 线段相交

给定两条线段 ( AB ) 和 ( CD )，判断它们是否相交。
这是几何引擎和矢量图形系统的核心。

#### A. 方向测试

我们再次使用叉积（方向）测试。
两条线段 ( AB ) 和 ( CD ) 相交，当且仅当：

1. 线段相互跨越：
$$
\text{orient}(A, B, C) \neq \text{orient}(A, B, D)
$$

$$
\text{orient}(C, D, A) \neq \text{orient}(C, D, B)
$$
2. 处理共线点的特殊情况（检查边界框）。

微型代码（C）

```c
double cross(Point a, Point b, Point c) {
    return (b.x - a.x)*(c.y - a.y) - (b.y - a.y)*(c.x - a.x);
}

int on_segment(Point a, Point b, Point c) {
    return fmin(a.x, b.x) <= c.x && c.x <= fmax(a.x, b.x) &&
           fmin(a.y, b.y) <= c.y && c.y <= fmax(a.y, b.y);
}

int intersect(Point a, Point b, Point c, Point d) {
    double o1 = cross(a, b, c);
    double o2 = cross(a, b, d);
    double o3 = cross(c, d, a);
    double o4 = cross(c, d, b);
    if (o1*o2 < 0 && o3*o4 < 0) return 1; // 一般情况
    if (o1 == 0 && on_segment(a,b,c)) return 1;
    if (o2 == 0 && on_segment(a,b,d)) return 1;
    if (o3 == 0 && on_segment(c,d,a)) return 1;
    if (o4 == 0 && on_segment(c,d,b)) return 1;
    return 0;
}
```

#### B. 扫描线算法（Bentley-Ottmann）

对于多条线段，高效地检查所有交点。
算法：

1. 按 x 坐标对所有端点进行排序。
2. 从左到右扫描。
3. 维护活动线段集合（平衡二叉搜索树）。
4. 检查相邻线段是否相交。

时间复杂度：$O((n + k) \log n)$，其中 $k$ 是交点的数量。

用于 CAD、地图渲染和碰撞检测系统。

#### 3. 复杂度总结

| 问题               | 朴素方法      | 最优方法              | 技术             |
| ------------------ | ------------- | --------------------- | ---------------- |
| 最近点对           | $O(n^2)$      | $O(n \log n)$         | 分治法           |
| 线段相交（多条）   | $O(n^2)$      | $O((n + k) \log n)$   | 扫描线           |

#### 重要性

诸如此类的几何算法教会我们如何进行空间推理，融合了数学、排序和逻辑。
它们为现实世界中精度至关重要的系统提供动力：从自动驾驶汽车到游戏引擎。

> "每个点都有一个邻居；每条路径都可能与另一条相交，几何在空间中揭示真相。"

#### 动手实践

1.  使用分治法实现最近点对算法。
2.  可视化所有点对距离，观察哪些点对距离最小。
3.  在随机点对上测试线段相交。
4.  使用向量叉积修改算法以处理 3D 线段。
5.  尝试构建一个扫描线可视化工具，逐步捕捉交点。
### 73. 扫描线算法与平面扫描算法

扫描线（或平面扫描）技术是计算几何中最强大的范式之一。它通过一条线（或平面）扫过输入数据，并维护一组动态的活动元素，将复杂的空间问题转化为可管理的一维事件。

这种方法构成了许多几何算法的基础：

- **事件排序** → 按顺序处理事物
- **活动集维护** → 跟踪当前结构
- **更新与查询** → 随着扫描的进行做出响应

用于交集检测、最近点对、矩形并集、图形和地理信息系统中的计算几何。


#### 1. 核心思想

想象一条垂直线从左到右扫过平面。在每个“事件”处（例如一个点或线段端点），我们更新该线当前接触的对象集合，即活动集。

每个事件都可能触发查询、插入或删除操作。

这种方法之所以有效，是因为随着扫描的推进，几何问题通常只依赖于附近元素之间的局部关系。


#### A. 扫描线模板

通用结构如下所示：

```c
struct Event { double x; int type; Object *obj; };
sort(events.begin(), events.end());

ActiveSet S;

for (Event e : events) {
    if (e.type == START) S.insert(e.obj);
    else if (e.type == END) S.erase(e.obj);
    else if (e.type == QUERY) handle_query(S, e.obj);
}
```

排序确保事件按 x（或其他维度）递增的顺序处理。


#### 2. 经典应用

让我们探讨三个可通过扫描技术解决的基础问题。


#### A. 线段求交（Bentley-Ottmann 算法）

目标：检测 ( n ) 条线段之间的所有交点。

步骤：

1. 按 x 坐标对端点进行排序。
2. 从左到右扫描。
3. 维护一个有序的活动线段集合（按 y 排序）。
4. 当新线段开始时，检查与其上方和下方邻居的交点。
5. 当线段相交时，记录交点并在交点的 x 坐标处插入一个新事件。

复杂度：$O((n + k)\log n)$，其中 $k$ 是交点的数量。



#### B. 最近点对

扫描线版本按 x 排序，然后在滑动垂直线的同时，维护一个宽度为 ( d )（当前最小值）的条带内的活动点。只需检查条带内最多 6-8 个附近的点。

复杂度：$O(n \log n)$


#### C. 矩形并集面积

给定轴对齐的矩形，计算覆盖的总面积。

思路：

- 将垂直边视为事件（进入/离开矩形）。
- 扫描线沿 x 轴移动。
- 在活动集中维护 y 区间（使用线段树或区间树）。
- 在每一步，将当前宽度乘以活动区间并集的高度。

复杂度：$O(n \log n)$


微型代码草图（C）

```c
typedef struct { double x, y1, y2; int type; } Event;
Event events[MAX];
int n_events;

qsort(events, n_events, sizeof(Event), cmp_by_x);

double prev_x = events[0].x, area = 0;
SegmentTree T;

for (int i = 0; i < n_events; i++) {
    double dx = events[i].x - prev_x;
    area += dx * T.total_length(); // 当前并集高度
    if (events[i].type == START)
        T.insert(events[i].y1, events[i].y2);
    else
        T.remove(events[i].y1, events[i].y2);
    prev_x = events[i].x;
}
```


#### 3. 其他应用

| 问题             | 描述                         | 时间复杂度     |
| ---------------- | ---------------------------- | -------------- |
| K 最近点         | 在活动集中维护前 $k$ 个点     | $O(n \log n)$  |
| 矩形并集         | 计算覆盖面积                 | $O(n \log n)$  |
| 点定位           | 在平面细分中定位点           | $O(\log n)$    |
| 可见性图         | 跟踪可见边                   | $O(n \log n)$  |



#### 4. 平面扫描扩展

虽然线扫描在一个维度（x）上移动，但平面扫描处理二维或更高维的空间，其中：

- 事件是二维单元或区域。
- 扫描前沿是一个平面而不是一条线。

用于 3D 碰撞检测、计算拓扑和 CAD 系统。


#### 概念可视化

1.  按一个轴（例如 x）对事件排序。
2.  维护相交或活动元素的结构（集合、树或堆）。
3.  在每个事件处更新并记录所需的输出（交点、并集、覆盖范围）。

关键在于局部性原理：只有扫描结构中的邻居才能改变结果。


#### 5. 复杂度

| 阶段             | 复杂度         |
| ---------------- | -------------- |
| 事件排序         | $O(n \log n)$  |
| 处理事件         | $O(n \log n)$  |
| 总计             | $O(n \log n)$（典型） |



#### 为何重要

扫描线方法将几何混沌转化为有序，将空间关系转化为排序序列。它是几何与算法之间的桥梁，融合了结构与运动。

> “一条扫描线能看到一切，并非同时，而是恰逢其时。”


#### 动手尝试

1.  实现一个扫描线线段交点查找器。
2.  计算 3 个有重叠矩形的并集面积。
3.  制作扫描线动画以可视化事件处理过程。
4.  修改算法以处理圆形或多边形对象。
5.  探索扫描线逻辑如何应用于调度中基于时间的事件。
### 74. Delaunay 和 Voronoi 图

在几何学和空间计算中，Delaunay 三角剖分和 Voronoi 图是对偶的、优雅的结构，它们捕捉了点之间的邻近性、区域性和连通性。

它们被广泛应用：从网格生成、路径规划、地理空间分析到计算生物学。
本节将介绍两者、它们之间的关系以及高效构建它们的算法。

#### 1. Voronoi 图

给定一组站点（点）$P = {p_1, p_2, \ldots, p_n}$，
Voronoi 图将平面划分为多个区域，每个点对应一个区域，
使得一个区域内的每个位置都离其所属站点比离任何其他站点更近。

形式上，$p_i$ 的 Voronoi 单元定义为：
$$
V(p_i) = {x \in \mathbb{R}^2 \mid d(x, p_i) \le d(x, p_j), \forall j \neq i }
$$

每个区域都是凸的，边界由垂直平分线构成。

#### 示例

对于点 ( A, B, C )：

- 在每对点之间绘制平分线。
- 交点定义了 Voronoi 顶点。
- 生成的多边形覆盖平面，每个站点一个。

用于模拟最近邻区域，例如“哪个基站服务哪个区域？”

#### 性质

- 每个单元都是凸的。
- 相邻的单元共享边。
- 图的顶点是三个站点外接圆的圆心。
- 对偶图 = Delaunay 三角剖分。

#### 2. Delaunay 三角剖分

Delaunay 三角剖分连接点，使得没有任何点位于任何三角形的外接圆内部。

等价地，它是 Voronoi 图的对偶图。

它倾向于避免细长的三角形，最大化最小角，从而创建形状良好的网格。

#### 形式化定义

点集 ( P ) 的一个三角剖分 ( T ) 是 Delaunay 的，如果对于每个三角形 $\triangle abc \in T$，
没有任何点 $p \in P \setminus {a,b,c}$ 位于 $\triangle abc$ 的外接圆内部。

为何重要：

- 避免狭长的三角形。
- 用于有限元网格、地形建模和路径规划。
- 引出自热邻居插值和光滑曲面。

#### 3. 关系

Voronoi 图和 Delaunay 三角剖分是几何对偶：

| Voronoi 图                | Delaunay 三角剖分            |
| ------------------------- | ---------------------------- |
| 区域 = 邻近区域           | 三角形 = 邻居连接            |
| 边 = 平分线               | 边 = 邻居对                  |
| 顶点 = 外接圆心           | 面 = 外接圆                  |

连接相邻的 Voronoi 单元就得到 Delaunay 边。

#### 4. 算法

有几种算法可以高效地构建这些图。

#### A. 增量插入法

1.  从一个包含所有点的超级三角形开始。
2.  逐个插入点。
3.  移除其外接圆包含该点的三角形。
4.  对产生的多边形空洞进行重新三角剖分。

时间复杂度： ( O$n^2$ )，随机化后可改进到 ( O$n \log n$ )。

#### B. 分治法

1.  按 x 坐标对点排序。
2.  递归地为左半部分和右半部分构建 DT。
3.  通过寻找公切线进行合并。

时间复杂度： ( O$n \log n$ )
优雅、结构化且确定性的。

#### C. Fortune 扫描线算法

对于 Voronoi 图，Fortune 算法从上到下扫描一条线。
维护一个抛物线弧的海岸线和一个事件队列。

每个事件（站点或圆）更新结构，增量地构建 Voronoi 边。

时间复杂度： ( O$n \log n$ )

#### D. Bowyer-Watson 算法（通过外接圆测试的 Delaunay 算法）

一种广泛用于图形和仿真的实用增量版本。

步骤：

-   从超级三角形开始
-   插入点
-   移除所有外接圆包含该点的三角形
-   重新连接产生的空洞

概念性微代码

```c
typedef struct { double x, y; } Point;

typedef struct { Point a, b, c; } Triangle;

bool in_circle(Point a, Point b, Point c, Point p) {
    double A[3][3] = {
        {a.x - p.x, a.y - p.y, (a.x*a.x + a.y*a.y) - (p.x*p.x + p.y*p.y)},
        {b.x - p.x, b.y - p.y, (b.x*b.x + b.y*b.y) - (p.x*p.x + p.y*p.y)},
        {c.x - p.x, c.y - p.y, (c.x*c.x + c.y*c.y) - (p.x*p.x + p.y*p.y)}
    };
    return determinant(A) > 0;
}
```

此测试确保 Delaunay 性质。

#### 5. 应用

| 领域             | 应用                       |
| ---------------- | -------------------------- |
| 地理信息系统     | 最近设施、区域划分         |
| 网格生成         | 有限元方法                 |
| 机器人学         | 可见性图、导航             |
| 计算机图形学     | 地形三角剖分               |
| 聚类分析         | 空间邻居结构               |

#### 6. 复杂度总结

| 算法             | 类型       | 时间复杂度      | 备注         |
| ---------------- | ---------- | --------------- | ------------ |
| Fortune          | Voronoi    | ( O$n \log n$ ) | 扫描线       |
| Bowyer-Watson    | Delaunay   | ( O$n \log n$ ) | 增量式       |
| 分治法           | Delaunay   | ( O$n \log n$ ) | 递归         |

#### 为何重要

Voronoi 图和 Delaunay 三角剖分揭示了点集中的自然结构。
它们将距离转化为几何，展示了空间如何被划分和连接。
如果说几何是空间的形状，那么这些图就是它的骨架。

> “每个点都宣称其领土；每块领土都塑造其网络。”

#### 亲自尝试

1.  为 5 个随机点手工绘制 Voronoi 区域。
2.  构建 Delaunay 三角形（连接相邻站点）。
3.  验证空外接圆性质。
4.  使用库（CGAL / SciPy）可视化这两种结构。
5.  探索添加新点如何重塑这些图。
### 75. 点是否在多边形内与多边形三角剖分

几何学中经常提出两个基本问题：

1. 一个点是在多边形内部还是外部？
2. 如何将一个复杂多边形分解为三角形以便计算？

这些是空间分析、计算机图形学和计算几何的基础构件。

#### 1. 点是否在多边形内

给定一个由顶点 ( $x_1, y_1$, $x_2, y_2$, \ldots, $x_n, y_n$ ) 定义的多边形和一个测试点 ( (x, y) )，
我们想要确定该点位于多边形内部、边界上还是外部。

#### 方法

#### A. 射线投射算法

从该点向右水平发射一条射线。
计算它与多边形边相交的次数。

- 奇数次数 → 内部
- 偶数次数 → 外部
这是基于奇偶规则。

微型代码（C语言实现射线投射）

```c
bool point_in_polygon(Point p, Point poly[], int n) {
    bool inside = false;
    for (int i = 0, j = n - 1; i < n; j = i++) {
        if (((poly[i].y > p.y) != (poly[j].y > p.y)) &&
            (p.x < (poly[j].x - poly[i].x) * 
                   (p.y - poly[i].y) / 
                   (poly[j].y - poly[i].y) + poly[i].x))
            inside = !inside;
    }
    return inside;
}
```

每当发现一次交叉时，就切换 `inside` 的状态。

#### B. 环绕数算法

计算多边形围绕该点缠绕了多少次。

- 非零环绕数 → 内部
- 零 → 外部
对于有孔洞或自相交的复杂多边形，此方法更稳健。

| 方法         | 时间复杂度 | 稳健性                       |
| -------------- | --------------- | -------------------------------- |
| 射线投射    | (O(n))          | 简单，可能在边界情况下失败   |
| 环绕数 | (O(n))          | 对复杂形状更准确 |

#### 边界情况

处理：

- 点在边上或顶点上
- 水平边（需要特殊处理以避免重复计数）
数值精度是关键。

#### 应用

- 计算机图形学中的点击测试
- 地理信息系统空间查询
- 碰撞检测

#### 2. 多边形三角剖分

多边形三角剖分将一个多边形划分为互不重叠的三角形，
这些三角形的并集等于原多边形。

为什么要进行三角剖分？

- 三角形简单、稳定，并且对于渲染和计算非常高效。
- 用于图形管线、面积计算、物理模拟和网格生成。

#### A. 三角剖分基础

对于一个有 ( n ) 个顶点的简单多边形，

- 总是可能的
- 总是产生 ( n - 2 ) 个三角形
目标：高效且稳定地找到一种三角剖分。

#### B. 耳切法

一种直观且广泛使用的三角剖分方法。

#### 思路

1.  找到一个"耳"：由三个连续顶点 ( $v_{i-1}, v_i, v_{i+1}$ ) 形成的三角形，满足：
    - 它是凸的
    - 内部不包含任何其他顶点
2.  切掉这个"耳"（移除顶点 $v_i$）
3.  重复直到只剩下一个三角形

时间复杂度: ( O(n^2) )

微型代码（耳切法草图）

```c
while (n > 3) {
    for (i = 0; i < n; i++) {
        if (is_ear(i)) {
            add_triangle(i-1, i, i+1);
            remove_vertex(i);
            break;
        }
    }
}
```

辅助函数 `is_ear()` 检查凸性和空性。

#### C. 凸多边形的动态规划

如果多边形是凸的，使用动态规划进行三角剖分：

$$
dp[i][j] = \min_{k \in (i,j)} dp[i][k] + dp[k][j] + cost(i, j, k)
$$

代价：周长或面积（用于最小权重三角剖分）

时间复杂度: ( O(n^3) )
空间复杂度: ( O(n^2) )

#### D. 分治法

递归地分割多边形并对子多边形进行三角剖分。
适用于凸或近似凸的形状。

| 算法        | 时间复杂度     | 备注           |
| ---------------- | -------- | --------------- |
| 耳切法     | (O(n^2$)) | 简单多边形 |
| 动态规划三角剖分 | (O(n^3$)) | 加权代价   |
| 凸多边形   | (O(n))   | 直接明了 |

#### 3. 应用

| 领域                  | 用途                            |
| ----------------------- | -------------------------------- |
| 计算机图形学       | 渲染、光栅化         |
| 计算几何  | 面积计算、积分    |
| 有限元分析 | 网格细分                 |
| 机器人学                | 路径规划、地图分解 |

#### 为什么重要

点是否在多边形内回答了你"在哪里"。
三角剖分告诉你"空间是如何构建的"。
它们共同构成了几何推理的基础。

> "从一个点到一千个三角形，几何学将空间转化为结构。"

#### 亲自尝试

1.  画一个非凸多边形，并使用射线投射规则测试随机点。
2.  为一个简单多边形实现耳切法。
3.  可视化每一步如何移除一个"耳"并简化形状。
4.  比较凸形和凹形的三角剖分结果。
### 76. 空间数据结构（KD 树、R 树）

在处理几何数据（点、矩形或多边形）时，高效的查找和组织至关重要。空间数据结构旨在回答以下类型的查询：

- 哪些对象在给定点附近？
- 哪些形状与某个区域相交？
- 最近邻是什么？

它们是计算几何、计算机图形学、地理信息系统（GIS）和搜索系统的基石。

#### 1. 动机

暴力检查每个对象的方法具有 ( O(n) ) 或更差的性能。
像 KD 树和 R 树这样的空间索引结构，能够实现高效的范围查询、最近邻搜索和空间连接。

#### 2. KD 树（k 维树）

KD 树是一种二叉树，它使用轴对齐的超平面递归地划分空间。

每个节点按一个坐标轴（x, y, z, ...）分割数据。

#### 结构

- 每个节点代表一个点。
- 每一层按不同的轴（x, y, x, y, ...）分割。
- 左子节点包含坐标值较小的点。
- 右子节点包含坐标值较大的点。

微型代码（二维 KD 树构建）

```c
typedef struct {
    double x, y;
} Point;

int axis; // 0 表示 x 轴，1 表示 y 轴

KDNode* build(Point points[], int n, int depth) {
    if (n == 0) return NULL;
    axis = depth % 2;
    int mid = n / 2;
    nth_element(points, points + mid, points + n, compare_by_axis);
    KDNode* node = new_node(points[mid]);
    node->left  = build(points, mid, depth + 1);
    node->right = build(points + mid + 1, n - mid - 1, depth + 1);
    return node;
}
```

搜索复杂度：

- 平均： ( O$\log n$ )
- 最坏情况： ( O(n) )

#### 查询

- 范围查询：查找区域内的点。
- 最近邻：搜索可能包含更近点的分支。
- K 最近邻：使用优先队列。

#### 优缺点

| 优点                     | 缺点                         |
| ------------------------ | ---------------------------- |
| 对静态数据高效           | 更新代价高                   |
| 适用于低维度             | 在高维度下性能下降           |

#### 应用

- 机器学习中的最近邻
- 碰撞检测
- 聚类（例如，加速 k-means）

#### 3. R 树（矩形树）

R 树是一种用于矩形边界框的高度平衡树。它是 B 树在空间上的类比。

#### 思想

- 将对象或边界框存储在叶节点中。
- 内部节点存储覆盖其子节点框的 MBR（最小边界矩形）。
- 通过遍历重叠的 MBR 进行查询。

微型代码（R 树节点草图）

```c
typedef struct {
    Rectangle mbr;
    Node* children[MAX_CHILDREN];
    int count;
} Node;
```

插入操作会选择 MBR 扩展最少以容纳新条目的子节点。

#### 操作

- 插入：选择子树 → 插入 → 调整 MBR
- 搜索：下降到其 MBR 与查询相交的节点
- 分裂：当节点满时，使用启发式方法（线性、二次、R* 树）

复杂度：

- 查询： ( O$\log n$ )
- 插入/删除： 平均 ( O$\log n$ )

#### 优缺点

| 优点                 | 缺点                             |
| -------------------- | -------------------------------- |
| 支持动态数据         | 重叠可能降低性能                 |
| 适用于矩形           | 分裂规则复杂                     |

#### 变体

- R* 树：优化的重新插入，更好的打包
- R+ 树：非重叠分区
- Hilbert R 树：使用空间填充曲线

#### 4. 比较

| 特性         | KD 树              | R 树                             |
| ------------ | ------------------ | -------------------------------- |
| 数据类型     | 点                 | 矩形 / 区域                      |
| 维度         | 低（2-10）         | 中等                             |
| 用例         | 最近邻、范围查询   | 空间连接、重叠查询               |
| 更新         | 代价高             | 支持动态更新                     |
| 平衡         | 递归中值           | 类似 B 树                        |

#### 5. 其他空间结构

| 结构       | 描述                                   |
| ---------- | -------------------------------------- |
| 四叉树     | 递归地将二维空间细分为四个象限         |
| 八叉树     | 四叉树的三维类比                       |
| BSP 树     | 使用任意平面进行二叉划分               |
| 网格索引   | 将空间划分为均匀的网格单元             |

#### 6. 应用

| 领域       | 用途                                 |
| ---------- | ------------------------------------ |
| GIS        | 区域查询、地图相交                   |
| 图形学     | 光线追踪加速                         |
| 机器人学   | 碰撞检测和路径规划                   |
| 机器学习   | 最近邻搜索                           |
| 数据库     | 空间索引                             |

#### 为何重要

空间结构将几何数据转化为可搜索的数据。
它们为"在哪里"和"附近有什么"提供了高效的算法，这对实时系统至关重要。

> "明智地划分空间，查询就会变成低语，而非呐喊。"

#### 动手实践

1.  为 10 个随机的二维点构建一个 KD 树。
2.  实现最近邻搜索。
3.  将矩形插入一个简单的 R 树，并查询与某个边界框的相交情况。
4.  比较查询时间与暴力方法。
### 77. 光栅化与扫描线技术

当你在屏幕上绘制形状——三角形、多边形、圆形时，它们必须被转换成像素。这种转换过程就叫做光栅化。它是连接几何数学与可见图像之间的桥梁。

光栅化与扫描线算法是计算机图形学、游戏引擎和渲染管线的基础。


#### 1. 什么是光栅化？

光栅化将矢量形状（连续的线条和曲面）转换为网格上的离散像素。

例如，一个由顶点 `(x1, y1), (x2, y2), (x3, y3)` 定义的三角形，必须逐个像素地进行填充。


#### 2. 核心思想

每个形状（直线、多边形、圆形）都在一个网格上进行采样。
算法决定哪些像素在形状内部、边界上或外部。

光栅化器回答以下问题：

- 哪些像素应该被点亮？
- 每个像素应该具有什么颜色或深度值？

#### 3. 直线光栅化（Bresenham 算法）

一种使用整数运算绘制直线的经典方法。

关键思想：从一个像素移动到下一个像素，选择最接近真实直线路径的像素。

```c
void draw_line(int x0, int y0, int x1, int y1) {
    int dx = abs(x1 - x0), dy = abs(y1 - y0);
    int sx = (x0 < x1) ? 1 : -1;
    int sy = (y0 < y1) ? 1 : -1;
    int err = dx - dy;
    while (true) {
        plot(x0, y0); // 绘制像素
        if (x0 == x1 && y0 == y1) break;
        int e2 = 2 * err;
        if (e2 > -dy) { err -= dy; x0 += sx; }
        if (e2 < dx) { err += dx; y0 += sy; }
    }
}
```

原理：Bresenham 算法避免了浮点运算，并保持了直线的视觉连续性。


#### 4. 多边形光栅化

为了填充形状，我们需要扫描线算法。它们沿着形状水平（y轴）扫描，并填充边缘之间的像素。

#### 扫描线填充步骤

1. 按 y 坐标对边进行排序。
2. 扫描每条线（y）。
3. 找出与多边形边的交点。
4. 在交点对之间进行填充。

这保证了凸多边形和凹多边形的正确填充。


#### 示例（简单三角形光栅化）

```c
for (int y = y_min; y <= y_max; y++) {
    找出与多边形边的所有 x 交点；
    对 x 交点进行排序；
    for (int i = 0; i < count; i += 2)
        draw_line(x[i], y, x[i+1], y);
}
```


#### 5. 圆形光栅化（中点算法）

利用对称性，一个圆在 8 个象限中是对称的。

每一步计算误差项，以决定是水平移动还是对角线移动。

```c
void draw_circle(int xc, int yc, int r) {
    int x = 0, y = r, d = 3 - 2 * r;
    while (y >= x) {
        plot_circle_points(xc, yc, x, y);
        x++;
        if (d > 0) { y--; d += 4 * (x - y) + 10; }
        else d += 4 * x + 6;
    }
}
```


#### 6. 深度与着色

在 3D 图形中，光栅化包括深度测试（Z 缓冲）和颜色插值。
每个像素存储其深度值；只有当新像素更近时才会覆盖旧像素。

插值着色（Gouraud，Phong）计算多边形上平滑的颜色过渡。


#### 7. 硬件光栅化

现代 GPU 并行执行光栅化：

- 顶点着色器 → 投影
- 光栅化器 → 像素网格
- 片元着色器 → 颜色与深度
每个像素都在片元着色器中处理光照、纹理和特效。


#### 8. 优化技术

| 技术                | 目的                           |
| ------------------- | ------------------------------ |
| 边界框裁剪          | 跳过屏幕外区域                 |
| 早期 Z 剔除         | 提前丢弃隐藏像素               |
| 边函数              | 对三角形进行快速内部测试       |
| 重心坐标            | 平滑地插值深度/颜色            |


#### 9. 为何重要

光栅化将数学转化为图像。
它是所有视觉计算、渲染器、CAD、游戏和 GUI 的基础。
即使光线追踪正在兴起，光栅化在实时渲染领域仍然占据主导地位。

> "你看到的每个像素都始于数学，它只是被光线描绘的几何图形。"


#### 10. 动手实践

1.  实现 Bresenham 直线算法。
2.  为三角形编写扫描线多边形填充算法。
3.  使用重心坐标进行颜色插值来扩展它。
4.  与暴力方法（遍历所有像素）比较性能。
### 78. 计算机视觉（Canny、Hough、SIFT）

计算机视觉是算法学习“看”，从图像中提取结构、形状和意义的领域。
在每个物体检测器、边缘图和关键点匹配器的背后，都有一系列强大的几何算法。

在本节中，我们将探讨经典视觉的四大支柱：Canny 边缘检测、Hough 变换和 SIFT（尺度不变特征变换）。

#### 1. 视觉处理流程

大多数视觉算法遵循一个简单的模式：

1.  **输入**：原始像素（灰度或彩色）
2.  **预处理**：平滑或滤波
3.  **特征提取**：边缘、角点、斑点
4.  **检测或匹配**：形状、关键点
5.  **解释**：物体识别、跟踪

Canny、Hough 和 SIFT 属于特征提取和检测阶段。

#### 2. Canny 边缘检测器

边缘标记了强度发生急剧变化的位置，即物体的轮廓。
Canny 算法（1986）是最鲁棒、应用最广泛的边缘检测器之一。

#### 步骤

1.  **平滑**：应用高斯模糊以减少噪声。
2.  **梯度计算**：
    *   通过 Sobel 滤波器计算 $G_x$ 和 $G_y$
    *   梯度幅值：$G = \sqrt{G_x^2 + G_y^2}$
    *   梯度方向：$\theta = \tan^{-1}\frac{G_y}{G_x}$
3.  **非极大值抑制**：
    *   仅保留沿梯度方向的局部最大值
4.  **双阈值处理**：
    *   强边缘（高梯度）
    *   弱边缘（与强边缘相连）
5.  **滞后边缘跟踪**：
    *   连接与强边缘相连的弱边缘

#### 微型代码（伪代码）

```c
Image canny(Image input) {
    Image smoothed = gaussian_blur(input);
    Gradient grad = sobel(smoothed);
    Image suppressed = non_max_suppression(grad);
    Image edges = hysteresis_threshold(suppressed, low, high);
    return edges;
}
```

#### Canny 为何有效

Canny 最大化三个标准：

1.  良好的检测（低漏检率）
2.  良好的定位（边缘接近真实边缘）
3.  单一响应（无重复边缘）

它在敏感性和稳定性之间取得了精心的平衡。

#### 3. Hough 变换

Canny 找到边缘点，Hough 将它们连接成形状。

Hough 变换通过在参数空间中进行投票，来检测直线、圆和其他参数化形状。

#### 直线检测

直线方程：
$$
\rho = x\cos\theta + y\sin\theta
$$

每个边缘点为其可能属于的所有 ($\rho, \theta$) 组合投票。
累加器数组中的峰值对应强直线。

微型代码（Hough 变换）

```c
for each edge point (x, y):
  for theta in [0, 180):
    rho = x*cos(theta) + y*sin(theta);
    accumulator[rho, theta]++;
```

然后选择投票数最高的 ($\rho, \theta$)。

#### 圆检测

使用三维累加器 $center_x, center_y, radius$。
每个边缘像素为可能的圆心投票。

#### 应用

*   自动驾驶中的车道检测
*   形状识别（圆形、椭圆形）
*   文档分析（线条、网格）

#### 4. SIFT（尺度不变特征变换）

SIFT 寻找在尺度、旋转和光照变化下保持稳定的关键点。

它广泛应用于图像匹配、全景图、3D 重建和物体识别。

#### 步骤

1.  **尺度空间极值检测**
    *   跨尺度使用高斯差分 (DoG)。
    *   在空间-尺度邻域中检测极大值/极小值。
2.  **关键点定位**
    *   精炼关键点位置并丢弃不稳定的点。
3.  **方向分配**
    *   分配主导梯度方向。
4.  **描述符生成**
    *   在局部图像块中构建梯度方向的 128 维直方图。

微型代码（概述）

```c
for each octave:
  build scale-space pyramid
  find DoG extrema
  localize keypoints
  assign orientations
  compute 128D descriptor
```

#### 特性

| 特性 | 描述 |
| :--- | :--- |
| 尺度不变性 | 在多个尺度上检测特征 |
| 旋转不变性 | 使用局部方向 |
| 鲁棒性 | 处理光照、噪声、仿射变换 |

#### 5. 比较

| 算法 | 目的 | 输出 | 鲁棒性 |
| :--- | :--- | :--- | :--- |
| Canny | 边缘检测 | 二值边缘图 | 对阈值敏感 |
| Hough | 形状检测 | 直线、圆 | 需要清晰的边缘 |
| SIFT | 特征检测 | 关键点、描述符 | 非常鲁棒 |

#### 6. 应用

| 领域 | 用例 |
| :--- | :--- |
| 机器人 | 视觉 SLAM、定位 |
| AR / VR | 标记跟踪 |
| 搜索 | 图像匹配 |
| 医学 | 边缘分割 |
| 工业 | 质量检测 |

#### 7. 现代继任者

*   **ORB (FAST + BRIEF)**：高效，适用于实时应用
*   **SURF**：更快的 SIFT 替代方案
*   **Harris / FAST**：角点检测器
*   **深度特征**：基于 CNN 的描述符

#### 为何重要

这些算法赋予了机器最初的“眼睛”，在深度学习之前，它们是计算机识别结构的方式。
即使在今天，它们仍被用于预处理、嵌入式系统和混合处理流程中。

> “在神经网络能够‘做梦’之前，视觉始于梯度、几何和投票。”

#### 动手尝试

1.  使用 Sobel 和滞后阈值实现 Canny。
2.  使用 Hough 变换在合成图像中检测直线。
3.  尝试使用 OpenCV SIFT 来匹配两幅旋转图像之间的关键点。
4.  比较高斯模糊前后的边缘图。
### 79. 空间路径规划 (A*, RRT, PRM)

无论是穿越迷宫、驾驶自动驾驶汽车，还是移动机械臂，核心问题都是一样的：
我们如何高效且安全地找到一条从起点到目标的路径？

路径规划算法回答了这个问题，它需要在最优性、速度和适应性之间取得平衡。本节我们将探讨三个基础的算法家族：

- **A***：在网格和图上的启发式搜索
- **RRT (快速探索随机树)**：基于采样的探索
- **PRM (概率路线图)**：预计算的导航网络

#### 1. 路径规划问题

给定：
- 一个空间（网格、图或连续空间）
- 一个起始节点和目标节点
- 一个代价函数（距离、时间、能耗）
- （可选的）障碍物

寻找一条无碰撞、低成本的路径。

#### 2. A* (A-star) 搜索

A* 结合了 Dijkstra 算法和一个用于估计剩余代价的启发函数。
它是最流行的基于图的路径规划算法。

#### 核心思想

每个节点 ( n ) 具有：
$$
f(n) = g(n) + h(n)
$$

- ( g(n) )：从起点到当前节点的实际代价
- ( h(n) )：从当前节点到目标的估计代价
- ( f(n) )：总的估计代价

#### 算法

1. 用起始节点初始化优先队列
2. 当队列非空时：
   - 弹出具有最小 ( f(n) ) 的节点
   - 如果到达目标 → 返回路径
   - 对于每个邻居节点：
     - 计算新的 ( g ) 和 ( f )
     - 如果更优，则更新队列

#### 微型代码（网格 A*）

```c
typedef struct { int x, y; double g, f; } Node;

double heuristic(Node a, Node b) {
    return fabs(a.x - b.x) + fabs(a.y - b.y); // 曼哈顿距离
}

void a_star(Node start, Node goal) {
    PriorityQueue open;
    push(open, start);
    while (!empty(open)) {
        Node cur = pop_min(open);
        if (cur == goal) return reconstruct_path();
        for (Node n : neighbors(cur)) {
            double tentative_g = cur.g + dist(cur, n);
            if (tentative_g < n.g) {
                n.g = tentative_g;
                n.f = n.g + heuristic(n, goal);
                push(open, n);
            }
        }
    }
}
```

#### 复杂度

- 时间：( O$E \log V$ )
- 空间：( O(V) )
- 如果 ( h(n) ) 是可采纳的（从不高估），则算法是最优的

#### 变体

| 变体               | 描述                           |
| ------------------ | ------------------------------ |
| Dijkstra           | 令 ( h(n) = 0 ) 的 A*          |
| 贪心最佳优先搜索   | 仅使用 ( h(n) )                |
| 加权 A*            | 以牺牲最优性为代价提高速度     |
| 跳点搜索           | 针对均匀网格优化               |

#### 3. RRT (快速探索随机树)

A* 在连续或高维空间（例如机械臂）中表现不佳。
RRT 通过随机化探索来解决这个问题。

#### 核心思想

- 从起点开始，通过随机采样点来生长一棵树。
- 以步长 ( $\epsilon$ ) 向每个采样点方向扩展树。
- 当接近目标时停止。

#### 微型代码（RRT 草图）

```c
Tree T = {start};
for (int i = 0; i < MAX_ITERS; i++) {
    Point q_rand = random_point();
    Point q_near = nearest(T, q_rand);
    Point q_new = steer(q_near, q_rand, step_size);
    if (collision_free(q_near, q_new))
        add_edge(T, q_near, q_new);
    if (distance(q_new, goal) < eps)
        return path;
}
```

#### 优缺点

| 优点                     | 缺点                                 |
| ------------------------ | ------------------------------------ |
| 适用于连续空间           | 路径是次优的                         |
| 能处理高维空间           | 随机性可能错过狭窄通道               |
| 简单快速                 | 需要后处理（平滑）                   |

#### 变体

| 变体           | 描述                   |
| -------------- | ---------------------- |
| RRT*           | 渐进最优               |
| 双向 RRT       | 从起点和目标同时生长   |
| Informed RRT*  | 聚焦于有希望的区域     |

#### 4. PRM (概率路线图)

PRM 构建一个可行配置的图（即路线图），然后在该图上进行搜索。

#### 步骤

1. 在自由空间中采样随机点
2. 用无碰撞的边连接附近的点
3. 在路线图上搜索（例如，使用 A*）

#### 微型代码（PRM 草图）

```c
Graph G = {};
for (int i = 0; i < N; i++) {
    Point p = random_free_point();
    G.add_vertex(p);
}
for each p in G:
    for each q near p:
        if (collision_free(p, q))
            G.add_edge(p, q);
path = a_star(G, start, goal);
```

#### 优缺点

| 优点                         | 缺点                            |
| ---------------------------- | ------------------------------- |
| 预计算可重用的路线图         | 需要大量采样点以获得良好覆盖    |
| 适用于多次查询               | 对单次查询规划效果不佳          |
| 适用于高维空间               | 可能需要后处理平滑              |

#### 5. 对比

| 算法 | 空间类型   | 性质         | 最优性             | 适用场景                 |
| ---- | ---------- | ------------ | ------------------ | ------------------------ |
| A*   | 离散       | 确定性       | 是                 | 网格、图                 |
| RRT  | 连续       | 随机化       | 否（RRT* = 是）    | 机器人、运动规划         |
| PRM  | 连续       | 随机化       | 近似               | 多查询规划               |

#### 6. 应用领域

| 领域               | 应用场景                           |
| ------------------ | ---------------------------------- |
| 机器人学           | 机械臂运动、移动机器人导航         |
| 游戏               | NPC 寻路、AI 导航网格              |
| 自动驾驶汽车       | 路线规划                           |
| 航空航天           | 无人机和航天器轨迹规划             |
| 物流               | 仓库机器人移动                     |

#### 为何重要

路径规划是空间中的决策过程，它赋予智能体移动、探索和有目的行动的能力。
从吃豆人到火星车，每一次旅程都始于一个算法。

> "要有目的地移动，必须先看清可能的路径。"

#### 动手实践

1.  在一个有墙壁的 2D 网格上实现 A*。
2.  在一个 2D 障碍物场中生成一个 RRT。
3.  为一个连续空间构建一个 PRM，并在路线图上运行 A*。
4.  比较不同方法的速度和路径平滑度。
### 80. 计算几何的变体与应用

计算几何是研究几何数据、点、线、多边形、圆以及高维形状上的算法的学科。
到目前为止，你已经看到了核心构建模块：凸包、交点、最近邻、三角剖分和空间索引。

这最后一节将通过变体、推广和实际应用将它们结合起来，展示几何学如何默默地支撑着现代计算。


#### 1. 超越平面

到目前为止，大多数例子都假设是二维几何。但真实系统通常存在于三维或 N 维空间中。

| 维度 | 示例问题                                     | 典型用途               |
| ---- | -------------------------------------------- | ---------------------- |
| 2D   | 凸包、多边形面积、扫描线                     | GIS、CAD、地图绘制     |
| 2D   | 凸包、多边形面积、扫描线                     | GIS、CAD、地图绘制     |
| 3D   | 凸多面体、网格相交、可见性                   | 图形学、仿真           |
| N-D  | 高维 Voronoi 图、KD 树、优化                 | 机器学习、机器人、数据科学 |

更高的维度增加了复杂性（有时甚至是不可能的）：

- 精确几何常常被近似方法所取代。
- 体积、距离和相交测试变得更加昂贵。

#### 2. 近似与鲁棒几何

现实世界的几何面临数值误差（浮点数）和退化情况（共线、重叠）。
为了处理这些问题，算法采用了鲁棒性和近似策略。

- Epsilon 比较：将容差范围内的值视为相等
- 方向测试：通过叉积鲁棒地计算转向方向
- 精确算术：有理数或符号计算
- 网格吸附：量化空间以获得稳定性

近似几何接受小的误差以换取大的速度提升，这在图形学和机器学习中至关重要。


#### 3. 几何对偶性

一种推理问题的强大工具：将点映射到线，线映射到点。
例如：

- 点 `( (a, b) )` 映射到直线 `y = ax - b`。
- 直线 `y = mx + c` 映射到点 `( (m, -c) )`。

应用：

- 将线相交问题转化为点定位问题
- 简化半平面相交
- 实现计算几何中的排列算法

对偶性是一个常用技巧：将几何问题颠倒过来使其更简单。


#### 4. 几何数据结构

核心空间结构及其最擅长领域的回顾：

| 结构                 | 存储内容         | 查询类型       | 用例                 |
| -------------------- | ---------------- | -------------- | -------------------- |
| KD 树                | 点               | 最近邻、范围   | 低维搜索             |
| R 树                 | 矩形             | 重叠           | 空间数据库           |
| 四叉树/八叉树        | 空间分区         | 点查找         | 图形学、GIS          |
| BSP 树               | 多边形           | 可见性         | 渲染                 |
| Delaunay 三角剖分    | 点               | 邻居           | 网格生成             |
| 线段树               | 区间             | 范围           | 扫描线事件           |


#### 5. 随机化几何

随机性简化了确定性几何：

- 随机增量构造（凸包，Delaunay）
- 用于近似的随机采样（ε-网，VC 维）
- 用于概率相交和覆盖的蒙特卡洛几何

示例：随机增量凸包算法以优雅的证明构建出期望 `O$n \log n$` 的结构。


#### 6. 计算拓扑学

几何之外是形状的连通性，由拓扑学研究。
算法计算连通分量、孔洞、同调和贝蒂数。

应用包括：

- 3D 打印（水密性）
- 数据分析（持久同调）
- 机器人学（自由空间拓扑）

几何与拓扑在 alpha 形状、单纯复形和流形重建中相遇。


#### 7. 几何与机器学习相遇

许多机器学习方法本质上是几何的：

- 最近邻 → Voronoi 图
- 支持向量机 → 超平面分离
- K 均值 → Voronoi 划分
- 流形学习 → 低维几何
- 凸优化 → 几何可行性

可视化工具（t-SNE，UMAP）依赖于空间嵌入和距离几何。


#### 8. 跨领域应用

| 领域         | 应用                 | 几何核心                     |
| ------------ | -------------------- | ---------------------------- |
| 图形学       | 渲染、碰撞           | 三角剖分、光线追踪           |
| GIS          | 地图、道路           | 多边形、点是否在区域内       |
| 机器人学     | 路径规划             | 障碍物、构型空间             |
| 建筑学       | 建模                 | 网格操作                     |
| 视觉         | 物体边界             | 轮廓、凸性                   |
| 人工智能     | 聚类、相似性         | 距离度量                     |
| 物理学       | 仿真                 | 粒子碰撞                     |
| 数据库       | 空间连接             | R 树、索引                   |

几何学是结构、位置和关系的基础，是空间推理的支柱。


#### 9. 复杂性与开放问题

一些问题仍然挑战着高效的解决方案：

- 动态环境中的点定位
- 复杂多边形中的可见性图
- 高维空间中的运动规划
- 几何中位数/中心问题
- 鲁棒环境中的近似保证

这些仍然是计算几何研究中的活跃领域。


#### 微型代码（通过射线投射判断点是否在多边形内）

```c
bool inside(Point p, Polygon poly) {
    int cnt = 0;
    for (int i = 0; i < poly.n; i++) {
        Point a = poly[i], b = poly[(i + 1) % poly.n];
        if (intersect_ray(p, a, b)) cnt++;
    }
    return cnt % 2 == 1; // 奇数次交叉 = 内部
}
```

这个小例程无处不在，地图、游戏、GUI 和物理引擎中都有它的身影。


#### 10. 为何重要

计算几何不仅仅是形状，它是空间的数学，驱动着视觉计算、空间数据和智能系统。
任何有物体移动、碰撞、绘制地图或识别形态的地方，几何学都是引导它的无形之手。

> "所有的计算都存在于某处，而几何学是我们理解'何处'的方式。"


#### 动手试试

1.  实现点是否在多边形内的判断，并在凸形和凹形上进行测试。
2.  可视化一个 Delaunay 三角剖分及其 Voronoi 对偶。
3.  尝试使用 KD 树进行最近邻查询。
4.  使用增量插入法编写一个小的三维凸包算法。
5.  在几何地图上绘制一个 RRT 路径。

## 第 9 章. 系统、数据库与分布式算法
### 81. 并发控制 (2PL, MVCC, OCC)

在多用户或多线程系统中，许多操作希望同时读取或写入共享数据。如果没有规范，这将导致混乱、更新丢失、脏读，甚至状态不一致。

并发控制确保并行环境下的正确性，使得结果如同每个事务单独运行一样（这一特性称为可串行化）。

本节介绍三种基础技术：

- 2PL - 两阶段锁
- MVCC - 多版本并发控制
- OCC - 乐观并发控制

#### 1. 目标：可串行化

我们希望事务的行为如同按某种串行顺序执行一样，即使它们实际上是交错执行的。

如果一个调度产生的结果与事务的某种串行顺序执行的结果相同，则该调度是*可串行化*的。

并发控制可以防止以下问题：

- 丢失更新：两个写入操作相互覆盖。
- 脏读：读取未提交的数据。
- 不可重复读：事务执行期间数据发生变化。
- 幻读：查询后出现新的行。

#### 2. 两阶段锁 (2PL)

思想：使用锁来协调访问。每个事务包含两个阶段：

1. 增长阶段：获取锁（共享锁或排他锁）
2. 收缩阶段：释放锁（释放锁后不允许再获取新锁）

这确保了冲突可串行化。

#### 锁类型

| 类型            | 操作   | 共享？ | 排他？ |
| --------------- | ------ | ------ | ------ |
| 共享锁 (S)      | 读     | 是     | 否     |
| 排他锁 (X)      | 写     | 否     | 否     |

如果事务需要读：请求 S 锁。
如果事务需要写：请求 X 锁。

微型代码（锁管理器草图）

```c
void acquire_lock(Transaction *T, Item *X, LockType type) {
    while (conflict_exists(X, type))
        wait();
    add_lock(X, T, type);
}

void release_all(Transaction *T) {
    for (Lock *l in T->locks)
        unlock(l);
}
```

#### 示例

```
T1: read(A); write(A)
T2: read(A); write(A)
```

无锁 → 竞态条件。
使用 2PL → 一个事务必须等待 → 一致。

#### 变体

| 变体                | 描述                                           |
| ------------------- | ---------------------------------------------- |
| 严格 2PL            | 持有所有锁直到提交 → 避免级联中止               |
| 严格 2PL            | 与严格 2PL 相同，所有锁在事务结束时释放         |
| 保守 2PL            | 在执行前获取所有锁                             |

#### 优缺点

| 优点                         | 缺点                     |
| ---------------------------- | ------------------------ |
| 保证可串行化                 | 可能导致死锁             |
| 概念简单                     | 负载下会阻塞，存在竞争   |

#### 3. 多版本并发控制 (MVCC)

思想：读者不阻塞写者，写者也不阻塞读者。
每次写入都会创建一个带时间戳的数据新版本。

事务根据其开始时间从一个一致的快照中读取数据。

#### 快照隔离

- 读者看到事务开始时已提交的最新版本。
- 写者产生新版本；在提交时检测冲突。
每条记录存储：

- `value`
- `created_at`
- `deleted_at`（如果适用）

微型代码（版本链）

```c
struct Version {
    int value;
    Timestamp created;
    Timestamp deleted;
    Version *next;
};
```

读取时查找满足 `created <= tx.start && deleted > tx.start` 的版本。

#### 优缺点

| 优点                   | 缺点                           |
| ---------------------- | ------------------------------ |
| 无需读锁               | 内存占用更高（多个版本）       |
| 读者从不阻塞           | 提交时检测写冲突               |
| 非常适合 OLTP 系统     | 需要垃圾回收旧版本             |

#### 使用场景

- PostgreSQL
- Oracle
- MySQL (InnoDB)
- Spanner

#### 4. 乐观并发控制 (OCC)

思想：假设冲突很少发生。让事务在没有锁的情况下运行。
在提交时进行验证，如果存在冲突，则回滚。

#### 阶段

1. 读阶段 - 执行，读取数据，缓冲写操作。
2. 验证阶段 - 检查是否发生冲突。
3. 写阶段 - 如果有效则应用更改，否则中止。

微型代码（OCC 验证）

```c
bool validate(Transaction *T) {
    for (Transaction *U in committed_since(T.start))
        if (conflict(T, U))
            return false;
    return true;
}
```

#### 优缺点

| 优点                               | 缺点                         |
| ---------------------------------- | ---------------------------- |
| 无锁 → 无死锁                       | 高竞争下中止率高             |
| 非常适合低冲突工作负载             | 中止时浪费工作               |

#### 使用场景

- 内存数据库
- 分布式系统
- STM（软件事务内存）

#### 5. 选择策略

| 系统类型              | 首选控制方式 |
| --------------------- | ------------ |
| OLTP（读写频繁）      | MVCC         |
| OLAP（读密集型）      | MVCC 或 OCC  |
| 实时系统              | 2PL（可预测）|
| 低竞争                | OCC          |
| 高竞争                | 2PL / MVCC   |

#### 6. 为何重要

并发控制是数据库、分布式系统乃至多线程程序中一致性的支柱。
它在混乱中强制执行正确性，确保您的数据不会被悄无声息地破坏。

> "没有秩序，并行就是噪音。并发控制是其指挥者。"

#### 动手实践

1.  用两个更新共享数据的事务模拟 2PL。
2.  实现一个带有版本链的玩具 MVCC 表。
3.  为三个并发事务编写一个 OCC 验证器。
4.  实验：在高冲突下，哪种模型表现最佳？
### 82. 日志、恢复与提交协议

无论你的算法多么优雅，存储速度多快，故障总会发生。断电、崩溃和网络分区都是不可避免的。关键在于恢复，即将系统恢复到一致状态而不丢失已提交的工作。

日志、恢复和提交协议构成了可靠事务系统的支柱，确保在崩溃时系统的持久性和正确性。

#### 1. 问题

我们需要保证 ACID 属性：

- 原子性 - 要么全做，要么全不做
- 一致性 - 事务前后状态均有效
- 隔离性 - 事务间互不干扰
- 持久性 - 一旦提交，永远安全

如果事务中途发生崩溃，我们如何正确地进行回滚或重做？

答案：记录一切，然后在故障后重放或撤销。

#### 2. 预写日志

黄金法则：

> "在修改数据库之前，先写日志条目。"

每个操作都被记录在磁盘上的顺序日志中，确保系统可以重建状态。

#### 日志记录格式

每个日志条目通常包括：

- `LSN`（日志序列号）
- `事务 ID`
- `操作`（更新、插入、删除）
- `前像`（旧值）
- `后像`（新值）

```c
struct LogEntry {
    int lsn;
    int tx_id;
    char op[10];
    Value before, after;
};
```

当一个事务提交时，系统首先将日志刷新到磁盘（`fsync`）。只有在此之后，才确认提交。

#### 3. 恢复操作

当系统重启时，它会读取日志并应用恢复算法。

#### 三个阶段（ARIES 模型）

1. 分析 - 确定崩溃时的状态（活跃 vs 已提交）
2. 重做 - 从最后一个检查点开始重复所有操作
3. 撤销 - 回滚未完成的事务

ARIES（利用语义进行恢复和隔离的算法）是最广泛使用的方法（IBM DB2、PostgreSQL、SQL Server）。

#### 重做规则

如果系统在崩溃前已提交 → 重做所有更新以保留数据。

#### 撤销规则

如果系统未提交 → 撤销操作以保持原子性。

微型代码（简化恢复流程）

```c
void recover(Log log) {
    for (Entry e : log) {
        if (e.committed)
            apply(e.after);
        else
            apply(e.before);
    }
}
```

#### 4. 检查点

为了避免重放整个日志，系统会设置检查点，即定期标记安全状态的快照。

| 类型                 | 描述                                     |
| -------------------- | ----------------------------------------------- |
| 精确检查点 | 短暂停止所有事务，刷新数据 + 日志 |
| 模糊检查点 | 标记一致的 LSN；继续运行           |

检查点减少了恢复时间：只需重放最后一个检查点之后的操作。

#### 5. 提交协议

在分布式系统中，多个节点必须一致同意提交或中止。
这由原子提交协议处理。

#### 两阶段提交

目标：所有参与者要么一起提交，要么一起中止。

步骤：

1. 准备阶段（投票）：
   - 协调者询问所有参与者是否“准备就绪”
   - 每个参与者回复是/否
2. 提交阶段（决策）：
   - 如果全部同意 → 提交
   - 否则 → 中止

```text
协调者: PREPARE  
参与者: VOTE YES / NO  
协调者: COMMIT / ABORT
```

如果协调者在准备阶段后崩溃，参与者必须等待 → 阻塞协议。

微型代码（2PC 伪代码）

```c
bool two_phase_commit(Participants P) {
    for each p in P:
        if (!p.prepare()) return abort_all();
    for each p in P:
        p.commit();
    return true;
}
```

#### 三阶段提交

在 2PC 基础上改进，增加了一个中间阶段以避免无限期阻塞。
更复杂，用于具有可靠故障检测的系统。

#### 6. 分布式系统中的日志记录

每个参与者维护自己的 WAL。
为了进行全局恢复：

- 使用协调检查点
- 维护全局提交日志
- 基于共识的协议（Paxos Commit、Raft）可以替代 2PC 以实现高可用性

#### 7. 示例时间线

| 步骤                | 操作                        |
| ------------------- | ----------------------------- |
| T1 更新记录 A | 写入 WAL 条目             |
| T1 更新记录 B | 写入 WAL 条目             |
| T1 提交          | 刷新 WAL，写入提交记录      |
| 崩溃！              | 磁盘可能处于不一致状态      |
| 重启             | 恢复扫描日志，重做 T1 |

#### 8. 优缺点

| 方法        | 优点         | 缺点        |
| --------------- | ---------------- | --------------- |
| WAL             | 简单，持久  | 写入开销  |
| 检查点   | 恢复更快  | I/O 峰值      |
| 2PC             | 全局原子性 | 阻塞        |
| 3PC / 共识 | 非阻塞     | 复杂，较慢 |

#### 9. 实际系统

| 系统         | 策略                 |
| -------------- | ------------------------ |
| PostgreSQL     | WAL + ARIES + 检查点 |
| MySQL (InnoDB) | WAL + 模糊检查点   |
| Spanner        | WAL + 2PC + TrueTime     |
| Kafka          | WAL 用于持久性       |
| RocksDB        | WAL + LSM 检查点    |

#### 10. 为何重要

日志和提交协议使数据在崩溃后得以幸存，并在多台机器间保持一致。
没有它们，每次故障都可能导致数据损坏。

> "持久性不在于永不失败，而在于记住如何重新站起来。"

#### 动手试试

1.  编写一个在写操作前记录日志的玩具 WAL 系统。
2.  模拟事务中途崩溃并重放日志。
3.  实现一个包含两个参与者的简单 2PC 协调者。
4.  比较使用检查点和不使用检查点的恢复时间。
### 83. 调度（轮转调度、最早截止时间优先、单调速率调度）

在操作系统和实时系统中，调度决定了任务或进程运行的顺序。由于像 CPU 时间这样的资源是有限的，一个好的调度器旨在平衡公平性、效率和响应性。

#### 1. 调度的目标

每个系统都有任务在竞争 CPU。调度决定：

- 接下来运行哪个任务
- 它运行多长时间
- 它何时让出或被抢占

不同领域有不同的目标：

| 领域                   | 目标                   |
| ------------------------ | --------------------------- |
| 通用操作系统       | 公平性、响应性    |
| 实时系统        | 满足截止时间           |
| 嵌入式系统         | 可预测性              |
| 高性能服务器 | 吞吐量、延迟平衡 |

调度器的策略可以是抢占式（中断任务）或非抢占式（等待自愿让出）。

#### 2. 轮转调度

轮转调度是最简单的抢占式调度器之一。
每个进程获得一个固定的时间片，并在一个循环队列中运行。

如果一个进程没有完成，它会被放回队列的末尾。

#### 微型代码：轮转调度（伪代码）

```c
queue processes;
while (!empty(processes)) {
    process = dequeue(processes);
    run_for_quantum(process);
    if (!process.finished)
        enqueue(processes, process);
}
```

#### 特点

- 公平：每个进程都能获得 CPU 时间。
- 响应性好：短任务不会饿死。
- 缺点：如果时间片太小，上下文切换开销大。

#### 示例

| 进程 | 执行时间 |   |
| ------- | ---------- | - |
| P1      | 4          |   |
| P2      | 3          |   |
| P3      | 2          |   |

时间片 = 1
顺序：P1, P2, P3, P1, P2, P3, P1, P2 → 所有进程都公平地完成。

#### 3. 优先级调度

每个任务都有一个优先级。调度器总是选择优先级最高的就绪任务。

- 抢占式：更高优先级的任务可以中断较低优先级的任务。
- 非抢占式：CPU 被自愿释放。

#### 问题

- 饥饿：低优先级任务可能永远无法运行。
- 解决方案：老化——逐渐增加等待任务的优先级。

#### 4. 最早截止时间优先调度

最早截止时间优先调度是用于实时系统的动态优先级调度器。
每个任务都有一个截止时间，截止时间最早的任务最先运行。

#### 规则

在任何时候，运行截止时间最近的就绪任务。

#### 示例

| 任务 | 执行时间 | 截止时间 |
| ---- | -------------- | -------- |
| T1   | 1              | 3        |
| T2   | 2              | 5        |
| T3   | 1              | 2        |

顺序：T3 → T1 → T2

对于单处理器上独立任务的抢占式调度，EDF 是最优的。

#### 5. 单调速率调度

在周期性实时系统中，任务以固定的时间间隔重复。
RMS 将更高的优先级分配给周期更短的任务。

| 任务 | 周期 | 优先级 |
| ---- | ------ | -------- |
| T1   | 2 ms   | 高     |
| T2   | 5 ms   | 中   |
| T3   | 10 ms  | 低      |

它是静态的（优先级不变），并且在固定优先级调度器中是最优的。

#### 利用率界限

对于 n 个任务，如果满足以下条件，则 RMS 保证可调度：

$$
U = \sum_{i=1}^{n} \frac{C_i}{T_i} \le n(2^{1/n} - 1)
$$

例如，对于 3 个任务，$U \le 0.78$。

#### 6. 最短作业优先调度

首先运行执行时间最短的任务。

- 非抢占式 SJF：一旦开始，运行到完成。
- 抢占式 SJF（最短剩余时间优先）：如果更短的任务到达，则抢占。

优点：最小化平均等待时间。
缺点：需要知道未来作业的长度。

#### 7. 多级队列调度

将进程划分为不同的类别（交互式、批处理、系统）。
每个类别都有自己的队列和自己的策略，例如：

- 队列 1：系统 → RR（时间片 = 10ms）
- 队列 2：交互式 → RR（时间片 = 50ms）
- 队列 3：批处理 → FCFS（先来先服务）

CPU 根据队列优先级进行分配。

#### 8. 多级反馈队列调度

进程根据其行为在队列之间移动。

- CPU 密集型 → 向下移动（更低优先级）
- I/O 密集型 → 向上移动（更高优先级）

目标：自适应调度，奖励交互式任务。

用于现代操作系统内核（Linux、Windows）。

#### 9. 调度指标

| 指标              | 含义                              |
| ------------------- | ------------------------------------ |
| 周转时间 | 完成时间 − 到达时间                 |
| 等待时间    | 在就绪队列中花费的时间            |
| 响应时间   | 从到达时间到首次执行的时间 |
| 吞吐量      | 单位时间内完成的任务数        |
| CPU 利用率 | CPU 繁忙的时间百分比                |

调度器根据设计目标来平衡这些指标。

#### 10. 为什么它很重要

调度器决定了系统在响应性、效率和公平性方面的感受。
在操作系统中，它们管理多任务处理。
在实时系统中，它们确保截止时间得到满足。
在服务器中，它们保持低延迟和高吞吐量。

> "调度不仅仅是关于时间。它关乎公平、远见和流程。"

#### 动手实践

1.  模拟时间片为 2 的轮转调度，比较平均等待时间。
2.  为一组具有截止时间的周期性任务实现 EDF。
3.  检查 3 个周期性任务在 RMS 下的可调度性。
4.  探索 Linux CFS（完全公平调度器）源代码。
5.  比较 SJF 和 RR 对于 CPU 密集型与 I/O 密集型工作负载的表现。
### 84. 缓存与替换策略（LRU、LFU、CLOCK）

缓存是一门通过记住过去来加速未来的艺术。在计算领域，缓存存储最近使用或频繁访问的数据，以减少延迟并减轻较慢存储（如磁盘或网络）的负载。挑战在于：缓存容量有限，因此当缓存已满时，我们必须决定驱逐什么。这就是替换策略发挥作用的地方。

#### 1. 缓存的需求

缓存无处不在：

- CPU：L1、L2、L3 缓存加速内存访问
- 数据库：查询结果或索引页
- Web 浏览器 / CDN：最近获取的页面
- 操作系统：磁盘块的页缓存

指导所有缓存的原则是局部性：

- 时间局部性：最近使用的项目很可能很快再次被使用
- 空间局部性：邻近的项目很可能接下来被需要

#### 2. 缓存替换问题

当缓存已满时，我们应该移除哪个项目？

我们希望最小化缓存未命中（在缓存中未找到的请求）。

形式化描述：

> 给定一个访问序列，找到一个能最小化未命中的替换策略。

理论上的最优策略（OPT）：总是驱逐未来最久不会被使用的项目。
但 OPT 需要预知未来，因此我们依赖启发式方法，如 LRU、LFU、CLOCK。

#### 3. 最近最少使用（LRU）

LRU 驱逐最近最少访问的项目。
它假设最近被使用的项目很可能再次被使用。

#### 实现方法

- 栈（列表）：访问时将项目移到顶部
- 哈希映射 + 双向链表：`O(1)` 的插入、删除、查找

#### 微型代码：LRU（简化版）

```c
typedef struct Node {
    int key;
    struct Node *prev, *next;
} Node;

HashMap cache;
List lru_list;

void access(int key) {
    if (in_cache(key)) move_to_front(key);
    else {
        if (cache_full()) remove_lru();
        insert_front(key);
    }
}
```

#### 优点

- 对于具有强时间局部性的工作负载效果良好

#### 缺点

- 在硬件或大规模缓存中成本高昂（元数据开销）

#### 4. 最不经常使用（LFU）

LFU 驱逐访问频率最低的项目。

跟踪每个项目的使用计数：

- 每次访问时递增计数
- 驱逐计数最低的项目

#### 示例

| 项目 | 访问次数 | 频率 |
| ---- | -------- | --------- |
| A    | 3        | 3         |
| B    | 1        | 1         |
| C    | 2        | 2         |

驱逐 B。

#### 变体

- 带老化的 LFU：逐渐减少计数以适应新趋势
- 近似 LFU：使用范围计数器（以提高内存效率）

#### 优点

- 对于稳定、重复的工作负载效果极佳

#### 缺点

- 对于流行度变化的工作负载效果不佳（适应慢）

#### 5. 先进先出（FIFO）

简单但朴素：

- 驱逐最旧的项目，忽略使用情况

用于简单的硬件缓存。
当访问模式是循环时效果良好，否则效果不佳。

#### 6. 随机替换（RR）

驱逐一个随机条目。

在某些高并发系统中出人意料地有竞争力，
并且实现起来微不足道。
在 memcached 中使用（作为一个选项）。

#### 7. CLOCK 算法

LRU 的一种实用近似，广泛用于操作系统页面替换。

每个页面有一个引用位（R）。页面形成一个循环列表。

算法：

1. 时钟指针扫描页面。
2. 如果 `R = 0`，驱逐页面。
3. 如果 `R = 1`，设置 `R = 0` 并跳过。

这以 O(1) 成本和低开销模拟了 LRU。

#### 8. 二次机会和增强型 CLOCK

二次机会：在驱逐之前给最近使用的页面一个“第二次机会”。
增强型 CLOCK：还使用修改位（M）来优先选择干净页面。

用于 Linux 的页面替换（使用活动/非活动列表）。

#### 9. 自适应算法

现代系统使用混合或自适应策略：

- ARC（自适应替换缓存）- 平衡最近性和频率
- CAR（带自适应替换的 CLOCK）- CLOCK 风格的自适应
- TinyLFU - 频率草图 + 准入策略
- 双曲缓存 - 用于大规模系统的流行度衰减

这些策略能动态适应变化的工作负载。

#### 10. 为何重要

缓存是系统速度的支柱：

- 操作系统将其用于分页
- 数据库用于缓冲池
- CPU 用于内存层次结构
- CDN 用于全球加速

选择合适的驱逐策略可能意味着延迟和吞吐量数量级的提升。

> “一个好的缓存记住重要的东西，并忘记不再重要的东西。”

#### 动手尝试

1.  模拟一个大小为 3 的缓存，访问序列为：A B C A B D A B C D
    比较 LRU、LFU 和 FIFO 的未命中次数。
2.  在 C 语言中使用双向链表和哈希映射实现 LRU。
3.  尝试使用引用位实现 CLOCK，模拟一次扫描。
4.  针对动态工作负载，实验 ARC 和 TinyLFU。
5.  测量不同访问模式（顺序、随机、循环）的命中率。
### 85. 网络（路由、拥塞控制）

网络算法确保数据在庞大、互联的系统中高效、可靠且公平地找到其传输路径。网络算法的两大核心支柱是路由（决定数据包*去向何处*）和拥塞控制（决定*以多快速度*发送它们）。

它们共同确保了互联网在高负载、动态拓扑和不可预测需求下的正常运行。

#### 1. 网络算法的目标

- 正确性：如果路径存在，则所有目的地均可到达
- 效率：使用最少的资源（带宽、延迟、跳数）
- 可扩展性：支持大型、动态的网络
- 鲁棒性：从故障中恢复
- 公平性：避免流饥饿

#### 2. 路由的类型

路由决定数据包在类似图的网络中应遵循的路径。

#### 静态路由 vs 动态路由

- 静态路由：固定路由，手动配置（适用于小型网络）
- 动态路由：随着拓扑变化自动调整路由（互联网规模）

#### 单播、组播、广播

- 单播：一对一（大多数流量）
- 组播：一对多（视频流、游戏）
- 广播：一对所有（本地网络）

#### 3. 最短路径路由

大多数路由依赖于最短路径算法：

#### Dijkstra（迪杰斯特拉）算法

- 从一个源点构建最短路径
- 复杂度：使用优先队列时为 `O(E log V)`

用于：
- OSPF（开放最短路径优先）
- IS-IS（中间系统到中间系统）

#### Bellman-Ford（贝尔曼-福特）算法

- 处理负权边
- 距离向量路由（RIP）的基础

#### 微型代码：用于路由的 Dijkstra 算法

```c
#define INF 1e9
int dist[MAX], visited[MAX];
vector<pair<int,int>> adj[MAX];

void dijkstra(int s, int n) {
    for (int i = 0; i < n; i++) dist[i] = INF;
    dist[s] = 0;
    priority_queue<pair<int,int>> pq;
    pq.push({0, s});
    while (!pq.empty()) {
        int u = pq.top().second; pq.pop();
        if (visited[u]) continue;
        visited[u] = 1;
        for (auto [v, w]: adj[u]) {
            if (dist[v] > dist[u] + w) {
                dist[v] = dist[u] + w;
                pq.push({-dist[v], v});
            }
        }
    }
}
```

#### 4. 距离向量 vs 链路状态

| 特性         | 距离向量 (RIP)          | 链路状态 (OSPF)         |
| ------------ | ----------------------- | ---------------------- |
| 共享信息     | 到邻居的距离            | 完整的拓扑图           |
| 收敛速度     | 较慢（可能出现环路）    | 快（SPF 计算）         |
| 复杂度       | 较低                    | 较高                   |
| 示例         | RIP, BGP（概念上）      | OSPF, IS-IS            |

RIP 使用 Bellman-Ford 算法。
OSPF 泛洪链路状态更新，在每个节点运行 Dijkstra 算法。

#### 5. 分层路由

大规模网络（如互联网）使用分层路由：

- 路由器分组为自治系统
- AS 内部路由：OSPF, IS-IS
- AS 间路由：BGP（边界网关协议）

BGP 交换可达性信息，而非最短路径，并优先采用基于策略的路由（例如，成本、合同、对等关系）。

#### 6. 拥塞控制

即使有好的路由，也不能淹没链路。
拥塞控制确保带宽的公平和高效使用。

主要在传输层（TCP）实现。

#### TCP 拥塞控制

关键组成部分：
- 加性增、乘性减
- 慢启动：探测容量
- 拥塞避免：谨慎增长
- 快速重传 / 快速恢复

现代变体：
- TCP Reno：经典的 AIMD
- TCP Cubic：针对高速网络的非线性增长
- BBR（瓶颈带宽 + RTT）：基于模型的控制

#### 算法概览（AIMD）

```text
收到 ACK 时：cwnd += 1/cwnd  // 缓慢增加
发生丢包时：cwnd /= 2        // 窗口减半
```

#### 7. 队列管理

路由器维护队列。
队列过满？=> 数据包丢失、延迟尖峰、队尾丢弃。

解决方案：
- RED（随机早期检测） - 提前丢弃数据包
- CoDel（受控延迟） - 监控队列延迟，自适应丢弃

这些方法防止缓冲区膨胀，改善实时流量的延迟。

#### 8. 流量控制 vs 拥塞控制

- 流量控制：防止发送方压垮接收方
- 拥塞控制：防止发送方压垮网络

TCP 两者都用：接收窗口和拥塞窗口。
实际发送速率 = `min(rwnd, cwnd)`。

#### 9. 数据平面 vs 控制平面

- 控制平面：决定路由（OSPF, BGP）
- 数据平面：转发数据包（快速路径）

现代网络（例如 SDN，软件定义网络）将两者分离：
- 控制器计算路由
- 交换机根据流规则行动

#### 10. 为何重要

路由和拥塞控制塑造了以下方面的性能：
- 互联网骨干网
- 数据中心网络（带有负载均衡）
- 云服务和微服务网格
- 内容分发网络

每个数据包的旅程，从你的笔记本电脑到全球数据中心，都依赖于这些思想。

> "网络不是魔法。它是算法在时间和空间中移动数据。"

#### 动手尝试

1.  为一个小型网络图实现 Dijkstra 算法。
2.  模拟 RIP（距离向量）：每个节点从邻居处更新信息。
3.  模拟 TCP AIMD 窗口增长；用 Python 可视化。
4.  尝试 RED：当队列长度 > 阈值时丢弃数据包。
5.  在模拟中比较 TCP Reno、Cubic、BBR 的吞吐量。
### 86. 分布式共识（Paxos、Raft、PBFT）

在一个分布式系统中，多个节点必须就某个单一值达成一致，例如日志的状态、数据库条目或区块链区块。这个达成一致的过程被称为共识。

共识算法使得分布式系统能够像一个可靠的系统一样运作，即使某些节点发生故障、崩溃或说谎（拜占庭故障）。

#### 1. 为什么需要共识？

想象一个管理共享日志的集群（例如在数据库或 Raft 中）。
每个节点可能：

- 看到不同的请求，
- 发生故障并恢复，
- 通过不可靠的链路进行通信。

我们需要所有非故障节点就相同的操作顺序达成一致。

一个有效的共识算法必须满足：

- **一致性**：所有正确的节点选择相同的值
- **有效性**：被选中的值是由某个节点提出的
- **终止性**：每个正确的节点最终都会做出决定
- **容错性**：在出现故障时仍能工作

#### 2. FLP 不可能性

FLP 定理（Fischer, Lynch, Paterson, 1985）指出：

> 在一个异步系统中，即使只有一个故障进程，也没有确定性算法能够保证达成共识。

因此，实际算法采用：

- 随机化，或
- 部分同步性（超时、重试）

#### 3. Paxos：经典算法

由 Leslie Lamport 提出的 Paxos 是分布式共识的理论基础。

它围绕三个角色展开：

- **提议者**：提议值
- **接受者**：对提议进行投票
- **学习者**：学习最终决定

共识过程分为两个阶段。

#### 阶段 1（准备）

1.  提议者选择一个提议编号 `n`，并向接受者发送 `(Prepare, n)`。
2.  接受者回复它们已接受的最高编号的提议（如果有的话）。

#### 阶段 2（接受）

1.  如果提议者收到多数派的响应，则发送带有值 `v`（所见最高值或新值）的 `(Accept, n, v)`。
2.  如果接受者没有承诺过更高的 `n`，则接受该提议。

当多数派接受时，值 `v` 即被选定。

#### 保证

- **安全性**：不会选定两个不同的值
- **活性**：在稳定的领导权下可能达成

#### 缺点

- 正确实现复杂
- 消息开销高

> "Paxos 是为理论家准备的；Raft 是为工程师准备的。"

#### 4. Raft：易于理解的共识

Raft 的设计目标是比 Paxos 更简单、更实用，专注于复制日志。

#### 角色

- **领导者**：协调所有变更
- **跟随者**：复制领导者的日志
- **候选人**：在选举期间

#### 工作流程

1.  **领导者选举**
    - 超时触发候选人选举。
    - 每个跟随者投票；获得多数票者获胜。
2.  **日志复制**
    - 领导者追加条目，发送 `AppendEntries` RPC。
    - 跟随者确认；当获得多数确认时，领导者提交条目。
3.  **安全性**
    - 多数派节点间的日志保持一致。
    - 跟随者只接受有效的前缀。

Raft 确保：

- 每个任期最多一个领导者
- 已提交的条目永不丢失
- 日志保持一致

#### 伪代码概览

```c
// 超时时 -> 成为候选人()
on timeout -> become_candidate()
// 发送请求投票(任期, id)
send RequestVote(term, id)
// 如果获得多数票 -> 成为领导者()
if majority_votes -> become_leader()

// 当收到追加条目(任期, 条目)时：
on AppendEntries(term, entries):
    // 如果任期 >= 当前任期：
    if term >= current_term:
        // 追加(条目)
        append(entries)
        // 回复成功
        reply success
```

#### 5. PBFT：拜占庭容错

Paxos 和 Raft 假设的是崩溃故障（节点停止，不说谎）。
对于拜占庭故障（任意行为），我们使用 PBFT（实用拜占庭容错算法）。

在总共 `3f + 1` 个节点中，最多容忍 `f` 个故障节点。

#### 阶段

1.  **预准备**：领导者提议值
2.  **准备**：节点广播提议的哈希值
3.  **提交**：节点通过 2f+1 票确认收到

用于区块链和关键系统（航天、金融）。

#### 6. 法定人数概念

共识通常依赖于法定人数（多数派）：

- 两个法定人数总是相交，确保了一致性。
- 写法定人数 + 读法定人数 ≥ 总节点数。

在 Raft/Paxos 中：

- 多数派 = `N/2 + 1`
- 即使某些节点故障，也能保证重叠。

#### 7. 日志复制与状态机

共识是复制状态机的基础：

- 每个节点以相同的顺序应用相同的命令。
- 保证了确定性的、相同的状态。

该模型支撑着：

- 数据库（etcd、Spanner、TiKV）
- 协调系统（ZooKeeper、Consul）
- Kubernetes 控制平面

#### 8. 领导者选举

所有实用的共识系统都需要领导者：

- 简化协调
- 减少冲突
- 心跳检测故障
- 新选举恢复进展

算法：

- Raft 选举（随机超时）
- 霸道算法
- Chang-Roberts 环选举

#### 9. 性能与优化

- **批处理**：分摊 RPC 开销
- **流水线**：并行追加
- **只读优化**：从跟随者提供（可能读取旧数据）
- **见证节点**：参与法定人数但不存储完整数据

高级技术：

- **Multi-Paxos**：重用领导者，减少轮次
- **Fast Paxos**：跳过某些阶段
- **Viewstamped Replication**：类似 Paxos 的日志复制

#### 10. 为什么它很重要

共识是现代分布式系统中可靠性的支柱。
每一个一致的数据库、服务注册中心或区块链都依赖于它。

使用共识的系统：

- etcd、Consul、ZooKeeper - 集群协调
- Kubernetes 中的 Raft - 领导者选举
- 区块链中的 PBFT - 容错账本
- Spanner、TiDB - 一致性数据库

> "共识是机器学会达成一致和建立信任的方式。"

#### 动手实践

1.  用 C 或 Python 实现 Raft 领导者选举。
2.  在 5 个节点上模拟 Paxos，并加入消息丢失。
3.  探索 PBFT：尝试节点故障和拜占庭行为。
4.  在负载下比较 Raft 与 Paxos 的性能。
5.  用 Raft 构建一个复制的键值存储。
### 87. 负载均衡与速率限制

当系统扩展时，没有单个服务器能够独自处理所有请求。负载均衡将传入的流量分发到多个服务器，以提高吞吐量、降低延迟并防止过载。同时，速率限制通过控制请求的允许频率来保护系统，确保公平性、稳定性和安全性。

这两种思想——分散负载和控制流量——是现代分布式系统和 API 的基石。

#### 1. 为什么负载均衡很重要

想象一个每秒接收数千个请求的 Web 服务。如果每个请求都发往一台机器，它将会崩溃。负载均衡器充当流量导向器，将请求分散到多个后端服务器。

目标：

- **效率** - 充分利用服务器
- **可靠性** - 无单点故障
- **可扩展性** - 处理不断增长的工作负载
- **灵活性** - 动态添加/移除服务器

#### 2. 负载均衡器的类型

#### 1. 第 4 层（传输层）

基于 IP 和端口进行均衡。
速度快且与协议无关（适用于 TCP/UDP）。

示例：Linux IPVS, Envoy, HAProxy

#### 2. 第 7 层（应用层）

理解 HTTP 等协议。
可以根据 URL 路径、请求头、Cookie 进行路由。

示例：Nginx, Envoy, AWS ALB

#### 3. 负载均衡算法

#### 轮询

按顺序循环遍历后端服务器。

```text
请求1 → 服务器A  
请求2 → 服务器B  
请求3 → 服务器C
```

简单、公平（如果所有节点相同）。

#### 加权轮询

分配权重以反映容量。
示例：服务器A(2倍), 服务器B(1倍)

#### 最少连接

将请求发送给活跃连接数最少的服务器。

#### 最短响应时间

选择延迟最低的后端（动态监控）。

#### 基于哈希（一致性哈希）

基于请求键（如用户 ID）确定性地路由。

- 保持缓存局部性
- 用于 CDN、分布式缓存（例如 memcached）

#### 随机

随机选择一个后端，在均匀负载下出人意料地有效。

#### 4. 一致性哈希（深入探讨）

用于分片和粘性会话。

核心思想：

- 将服务器映射到一个哈希环
- 请求的键被哈希到环上
- 分配给顺时针方向的下一个服务器

当服务器加入/离开时，只有一小部分键需要移动。

用于：

- CDN
- 分布式缓存（Redis Cluster, DynamoDB）
- 负载感知系统

#### 5. 健康检查与故障转移

智能负载均衡器监控每台服务器的健康状况：

- 心跳探测（HTTP/TCP）
- 自动移除不健康的服务器
- 即时重新平衡流量

示例：
如果服务器B 故障，将其从轮换中移除：

```
健康服务器: [服务器A, 服务器C]
```

还支持主备故障转移：当活动服务器故障时，热备服务器接管。

#### 6. 全局负载均衡

跨区域或数据中心：

- **GeoDNS**：路由到最近的区域
- **任播**：在全球范围内通告相同的 IP；路由选择最近的
- **基于延迟的路由**：测量并选择最低 RTT

被 CDN、云服务、多区域应用程序使用

#### 7. 速率限制：另一面

如果说负载均衡是分散工作，那么速率限制就是保持总工作量在合理范围内。

它防止：

- **滥用**（机器人、DDoS）
- **过载**（请求过多）
- **公平性问题**（没有用户独占资源）

策略：

- 按用户、按 IP、按 API 密钥
- 全局或按端点

#### 8. 速率限制算法

#### 令牌桶

- 桶持有令牌（容量 = 突发限制）
- 每个请求消耗 1 个令牌
- 令牌以恒定速率补充（速率限制）
- 如果为空 → 拒绝或延迟

适用于突发流量。

```c
if (tokens > 0) {
    tokens--;
    allow();
} else reject();
```

#### 漏桶

- 请求流入一个桶，以固定速率流出
- 过量 = 溢出 = 丢弃

平滑突发；用于流量整形。

#### 固定窗口计数器

- 在固定间隔内（例如 1 秒）计数请求
- 每个窗口重置
- 简单，但在边界附近不公平

#### 滑动窗口日志 / 滑动窗口计数器

- 维护请求的时间戳
- 移除超出时间窗口的旧请求
- 更准确和公平

#### 9. 结合两者

一个完整的系统可能：

- 对每个用户或服务使用速率限制
- 跨节点使用负载均衡
- 当过载持续时应用断路器

它们共同构成了弹性架构，即使在流量高峰下也能保持在线。

#### 10. 为什么这很重要

这些技术使大规模系统能够：

- **可扩展** - 处理数百万用户
- **稳定** - 防止级联故障
- **公平** - 每个客户端获得公平份额
- **弹性** - 从容应对流量高峰或节点丢失

用于：

- **API 网关**（Kong, Envoy, Nginx）
- **云负载均衡器**（AWS ALB, GCP LB）
- **Kubernetes Ingress 和服务网格**
- **分布式缓存和数据库**

> "均衡让系统存活。限制让系统保持理智。"

#### 动手实践

1.  模拟在 3 台服务器上进行轮询和最少连接负载均衡。
2.  用 C 或 Python 实现一个令牌桶速率限制器。
3.  测试突发流量，观察丢弃或延迟情况。
4.  将一致性哈希与令牌桶结合，实现用户级控制。
5.  可视化负载均衡 + 速率限制如何保持系统低延迟。
### 88. 搜索与索引（倒排索引、BM25、WAND）

搜索引擎，无论是像 Google 这样的网络级引擎，还是像 SQLite 的 FTS 这样的本地引擎，都依赖于高效的索引和排序来快速响应查询。它们使用索引（结构化查找表）来快速找到相关匹配，而不是扫描所有文档。

本节探讨倒排索引、排序算法（TF-IDF、BM25）以及像 WAND 这样的高效检索技术。

#### 1. 搜索问题

给定：
- 一个文档语料库
- 一个查询（例如，"machine learning algorithms"）

我们希望返回：
- 相关文档
- 按重要性和相似度排序

朴素搜索 → O(N × M) 次比较
倒排索引 → O(K log N)，其中 K = 查询中的词项数

#### 2. 倒排索引：搜索的核心

倒排索引将词项映射到包含它们的文档。

#### 示例

| 词项        | 倒排记录表 |
| ----------- | ------------- |
| "data"      | [1, 4, 5]     |
| "algorithm" | [2, 3, 5]     |
| "machine"   | [1, 2]        |

每条倒排记录可能包含：
- 文档 ID
- 词项频率 (tf)
- 位置信息（用于短语搜索）

#### 构建步骤

1.  文档分词 → 单词
2.  规范化（小写、词干提取、停用词移除）
3.  构建倒排记录：词项 → [文档ID, tf, 位置]
4.  排序和压缩以提高存储效率

使用者：
- Elasticsearch, Lucene, Whoosh, Solr

#### 3. 布尔检索

最简单的模型：
- 查询 = 布尔表达式，例如 `(machine AND learning) OR AI`

对倒排记录表使用集合操作：
- AND → 交集
- OR → 并集
- NOT → 差集

快速交集操作在有序列表上使用合并算法。

```c
void intersect(int A[], int B[], int n, int m) {
    int i = 0, j = 0;
    while (i < n && j < m) {
        if (A[i] == B[j]) { print(A[i]); i++; j++; }
        else if (A[i] < B[j]) i++;
        else j++;
    }
}
```

但是布尔搜索不排序结果，所以我们需要评分模型。

#### 4. 向量空间模型

将文档和查询表示为词项向量。
每个维度 = 词项权重 (tf-idf)。

- tf: 词项在文档中的频率
- idf: 逆文档频率 $idf = \log\frac{N}{df_t}$

余弦相似度衡量相关性：
$$
\text{score}(q, d) = \frac{q \cdot d}{|q| |d|}
$$

简单、可解释，构成了 BM25 和现代嵌入模型的基础。

#### 5. BM25：经典的排序函数

BM25（Best Match 25）是信息检索领域事实上的标准。

$$
\text{score}(q, d) = \sum_{t \in q} IDF(t) \cdot \frac{f(t, d) \cdot (k_1 + 1)}{f(t, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{avgdl})}
$$

其中：
- $f(t, d)$: 词项频率
- $|d|$: 文档长度
- $avgdl$: 平均文档长度
- $k_1, b$: 可调参数（典型值 1.2-2.0, 0.75）

#### 优势

- 平衡词项频率、文档长度和稀有度
- 快速有效的基线方法
- 仍被 Elasticsearch、Lucene、OpenSearch 使用

#### 6. 效率技巧：WAND、Block-Max WAND

排序涉及合并多个倒排记录表。
我们可以使用 WAND（Weak AND）提前跳过不相关的文档。

#### WAND 原理

- 每个词项有一个上限分数
- 在每个倒排记录表中维护指针
- 计算潜在最大分数
- 如果最大值 < 当前阈值，则跳过该文档

提高了 top-k 检索的延迟。

变体：
- BMW (Block-Max WAND) - 使用块级分数边界
- MaxScore - 更简单的阈值处理
- 动态剪枝 - 跳过无希望的候选文档

#### 7. 索引压缩

倒排记录表很长，压缩至关重要。

常用方案：
- 差值编码：存储文档 ID 之间的间隔
- 可变字节 (VB) 或 Gamma 编码
- 参考帧 (FOR) 和 SIMD-BP128 用于向量化解码

目标：更小的存储空间 + 更快的解压缩速度

#### 8. 高级检索

#### 邻近搜索

要求单词彼此靠近出现。
使用位置索引。

#### 短语搜索

使用位置信息匹配精确序列：
"machine learning" ≠ "learning machine"

#### 模糊 / 近似搜索

允许拼写错误：
使用 Levenshtein 自动机、n-gram 或 k-近似匹配

#### 字段化搜索

按字段评分（标题、正文、标签）
加权组合

#### 9. 学习排序与语义搜索

现代搜索增加了基于机器学习的重排序：
- 学习排序 (LTR)：使用特征（tf、idf、BM25、点击率）
- 神经重排序：使用 BERT 风格的嵌入进行语义相似度计算
- 混合检索：结合 BM25 + 稠密向量（例如 ColBERT、RRF）

此外：用于基于向量搜索的 ANN（近似最近邻）。

#### 10. 为何重要

高效搜索驱动着：
- 网络搜索引擎
- IDE 符号查找
- 日志搜索、代码搜索
- 数据库全文搜索
- AI 检索管道 (RAG)

这是算法与语言和规模交汇的地方。

> "搜索是我们将意义与记忆连接起来的方式。"

#### 动手实践

1.  用 C 或 Python 构建一个微型倒排索引。
2.  实现布尔 AND 和 OR 查询。
3.  为一个小型数据集计算 TF-IDF 和 BM25 分数。
4.  为 top-k 检索添加 WAND 剪枝。
5.  比较 BM25 与语义嵌入在相关性上的表现。
### 89. 系统中的压缩与编码

压缩与编码算法是计算领域默默无闻的“老黄牛”，它们通过缩减数据来节省空间、带宽和时间。它们使得系统能够存储更多、传输更快、处理更高效。从文件、数据库到网络和日志，压缩技术几乎塑造了系统设计的每一个层面。

#### 1. 为何压缩至关重要

压缩无处不在：

*   **数据库** - 列存储、索引、日志
*   **网络** - HTTP、TCP、QUIC 载荷
*   **文件系统** - ZFS、NTFS、btrfs 压缩
*   **流媒体** - 视频/音频编解码器
*   **日志与遥测** - 减少 I/O 和存储成本

**益处：**

*   数据更小 = I/O 更快
*   存储更少 = 成本更低
*   传输更少 = 吞吐量更高

**权衡取舍：**

*   CPU 开销（压缩/解压缩）
*   延迟（尤其对于小数据）
*   适用性（取决于熵和结构）

#### 2. 核心概念

##### 熵

表示数据所需的最小比特数（香农理论）。熵值高 → 可压缩性低。

##### 冗余

压缩利用了数据的重复和模式。

##### 无损压缩 vs 有损压缩

*   **无损压缩**：可逆的（ZIP、PNG、LZ4）
*   **有损压缩**：近似的（JPEG、MP3、H.264）

在系统环境中，无损压缩占主导地位。

#### 3. 常见的无损压缩家族

##### 霍夫曼编码

*   前缀无关的变长编码
*   高频符号 = 短编码
*   在符号级模型下是最优的

**用于**：DEFLATE、JPEG、MP3

##### 算术编码

*   将序列编码为分数区间
*   对于偏态分布，比霍夫曼编码更高效
*   **用于**：H.264、bzip2、AV1

##### 基于字典的编码（LZ77， LZ78）

*   用引用替换重复的子字符串
*   ZIP、gzip、zlib、LZMA、Snappy 的核心

##### LZ77 算法概览

```c
while (not EOF) {
    find longest match in sliding window;
    output (offset, length, next_char);
}
```

**变体：**

*   **LZ4** - 速度快，压缩比相对较低
*   **Snappy** - 为速度优化
*   **Zstandard (Zstd)** - 可调节速度/压缩比，支持字典

##### 伯罗斯-惠勒变换

*   重新排列数据以分组相似符号
*   后接“移至前端”变换 + 霍夫曼编码
*   **用于** bzip2、基于 BWT 的压缩器

##### 游程编码

*   用（符号， 计数）替换连续的重复项
*   非常适合结构化或稀疏数据

**示例**：`AAAAABBBCC` → `(A,5)(B,3)(C,2)`

#### 4. 系统中的专用压缩

##### 列式数据库

按列压缩：

*   **字典编码** - 将字符串映射为整数
*   **游程编码** - 适用于已排序列
*   **差值编码** - 存储差值（时间序列）
*   **位打包** - 以最小比特数存储固定宽度整数

结合多种技术以获得最佳压缩比。

**示例（时间差值）：**

```
[100, 102, 103, 107] → [100, +2, +1, +4]
```

##### 日志与遥测压缩

*   结构化格式 → 按字段编码
*   通常使用 Snappy 或 LZ4 以实现快速解码
*   聚合器（Fluentd、Loki、Kafka）严重依赖它们

##### 数据湖与文件

*   Parquet、ORC、Arrow → 列式 + 压缩
*   按列选择编解码器：LZ4 用于速度，Zstd 用于压缩比

#### 5. 流式与分块压缩

大数据通常分块处理：

*   支持随机访问和并行处理
*   网络流、分布式文件所需

**示例**：`zlib` 块、`Zstd` 帧、`gzip` 块

**用于：**

*   HTTP 分块传输编码
*   Kafka 日志分段
*   MapReduce 洗牌

#### 6. 编码方案

压缩 ≠ 编码。编码确保安全传输。

##### Base64

*   将 3 字节映射为 4 个字符
*   33% 的开销
*   **用于** 二进制数据 → 文本（电子邮件、JSON API）

##### URL 编码

*   用 `%xx` 转义不安全字符

##### 差值编码

*   存储差值，而非完整值

##### 可变长度整数 / Zigzag 编码

*   紧凑的整数表示（例如 protobufs）
*   数字越小 → 字节数越少

**示例：**

```c
while (x >= 0x80) { emit((x & 0x7F) | 0x80); x >>= 7; }
emit(x);
```

#### 7. 自适应与上下文模型

现代压缩器能适应局部模式：

*   PPM（部分匹配预测）
*   上下文混合
*   Zstd 使用 FSE（有限状态熵）编码

在速度、内存和压缩比之间取得平衡。

#### 8. 硬件加速

压缩可以卸载到：

*   支持 SIMD（AVX2、SSE4.2）的 CPU
*   GPU（并行编码/解码）
*   网卡 / 智能网卡
*   专用集成电路（例如，Intel QAT）

对于高吞吐量数据库、网络设备和存储系统至关重要。

#### 9. 设计权衡

| 目标             | 最佳选择             |
| ---------------- | -------------------- |
| 最大速度         | LZ4, Snappy          |
| 最大压缩比       | Zstd, LZMA           |
| 平衡             | Zstd（可调）         |
| 列存储           | RLE, Delta, Dict     |
| 日志 / 遥测      | Snappy, LZ4          |
| 归档             | bzip2, xz            |
| 实时             | LZ4, Brotli（快速模式） |

根据 CPU 预算、I/O 成本、延迟容忍度进行选择。

#### 10. 为何这很重要

压缩是一流的优化手段：

*   在数据中心节省 PB 级数据
*   提升跨网络的吞吐量
*   为云存储（S3、BigQuery、Snowflake）提供动力
*   实现高效的分析和机器学习流水线

> “节省的每一个字节，都是赢得的时间。”

#### 动手实践

1.  使用霍夫曼编码压缩文本（构建频率表）。
2.  在 1GB 数据集上比较 gzip、Snappy 和 Zstd。
3.  为数值数据实现差值编码和游程编码。
4.  在重复字符串上尝试字典编码。
5.  测量压缩比、速度和 CPU 使用率之间的权衡。
### 90. 容错与复制

现代系统必须在硬件崩溃、网络分区或数据丢失的情况下继续运行而不停止。容错确保系统即使在部分组件发生故障时仍能继续运行。复制是这种弹性的基础，通过跨多个节点复制数据或计算来实现冗余、性能提升和一致性。

两者共同构成了分布式系统可靠性的支柱。

#### 1. 为什么需要容错？

没有系统是完美的：

- 服务器崩溃
- 磁盘故障
- 网络分区
- 断电

问题不在于故障*是否*会发生，而在于*何时*发生。
容错系统能够自动检测、隔离故障并从故障中恢复。

目标：

- 可用性 - 持续处理请求
- 持久性 - 永不丢失数据
- 一致性 - 在副本间保持正确性

#### 2. 故障模型

#### 崩溃故障

节点停止响应但行为正常。
通过重启或复制（Raft, Paxos）处理。

#### 遗漏故障

消息丢失或更新被丢弃。
通过重试和确认机制处理。

#### 拜占庭故障

任意/恶意行为。
通过拜占庭容错（PBFT）处理，成本高但鲁棒性强。

#### 3. 冗余：核心策略

容错 = 冗余 + 检测 + 恢复

冗余类型：

- 硬件：多个电源、磁盘（RAID）
- 软件：复制服务、重试
- 数据：多个副本、纠删码
- 时间：重试或检查点与重放

#### 4. 复制模型

#### 1. 主动复制

所有副本并行（同步）处理请求。
结果必须匹配。
用于实时系统和拜占庭容错系统。

#### 2. 被动复制（主-备）

一个领导者（主节点）处理请求。
备份节点复制日志，在故障时接管。
用于 Raft、ZooKeeper、PostgreSQL 流复制。

#### 3. 法定人数复制

写入和读取需要联系大多数副本。
确保操作重叠 → 一致性。
用于 Cassandra、DynamoDB、Etcd。

#### 5. 一致性模型

复制引入了**一致性**和**可用性**之间的权衡（CAP 定理）。

#### 强一致性

所有客户端立即看到相同的值。
示例：Raft、Etcd、Spanner。

#### 最终一致性

副本最终会收敛到一致状态。
示例：DynamoDB、Cassandra。

#### 因果一致性

保持事件的因果顺序。
示例：向量时钟、CRDTs。

选择取决于工作负载：

- 银行业务 → 强一致性
- 社交信息流 → 最终一致性
- 协同编辑 → 因果一致性

#### 6. 检查点与恢复

为了在崩溃后恢复：

- 定期对状态进行**检查点**保存
- 重启时，**重放**错过的日志事件

示例：
数据库 → 预写日志（WAL）
流处理系统 → Kafka 检查点

```text
1. 将状态保存到磁盘
2. 记录最新的日志位置
3. 重启时 → 重新加载 + 重放
```

#### 7. 纠删码

不存储完整副本，而是存储编码后的数据块。
对于 ( k ) 个数据块，使用 ( m ) 个奇偶校验块 → 可容忍 ( m ) 个故障。

示例：里德-所罗门码（用于 HDFS、Ceph）

| k | m | 总数 | 容错能力 |
| - | - | ---- | -------- |
| 4 | 2 | 6    | 2 个故障 |

比 3 倍复制具有更好的存储效率。

#### 8. 故障检测

在分布式系统中检测故障很棘手（因为存在延迟）。
常用技术：

- 心跳 - 周期性的"我还活着"消息
- 超时 - 如果没有收到心跳则怀疑节点故障
- 流言协议 - 在对等节点间共享故障信息

用于 Consul、Cassandra、Kubernetes 健康检查。

#### 9. 自愈系统

故障发生后：

1. 检测故障
2. 隔离故障组件
3. 替换或重启
4. 重新平衡负载或重新复制数据

模式：

- 监督树（Erlang/Elixir）
- 自愈集群（Kubernetes）
- 重新平衡（Cassandra 环修复）

"永远不要信任单台机器，要信任系统。"

#### 10. 为什么重要

容错将脆弱的基础设施转变为可靠的服务。

用于：

- 数据库（复制 + WAL）
- 分布式存储（HDFS、Ceph、S3）
- 编排系统（Kubernetes 控制器）
- 流处理系统（Kafka、Flink）

如果没有复制和容错，大规模系统将在故障下崩溃。

> "弹性是构建出来的，不是假设出来的。"

#### 动手实践

1.  构建一个主-备键值存储：领导者写入，跟随者复制。
2.  添加心跳 + 超时检测以触发故障转移。
3.  模拟网络分区：探索在强一致性与最终一致性下的行为。
4.  为一个小型应用实现检查点 + 重放恢复。
5.  比较 3 倍复制与里德-所罗门码（4+2）在空间和可靠性上的差异。

## 第 10 章. 人工智能、机器学习与优化
### 91. 经典机器学习（k-means、朴素贝叶斯、SVM、决策树）

经典机器学习建立在可解释的数学和坚实的优化基础之上。在深度学习出现之前很久，这些算法就为搜索引擎、垃圾邮件过滤器和推荐系统提供了动力。它们至今仍在使用，具有快速、可解释和易于部署的特点。

本节涵盖经典机器学习的四大支柱：

- k-means - 无监督聚类
- 朴素贝叶斯 - 概率分类
- SVM - 基于间隔的分类
- 决策树 - 基于规则的学习

#### 1. 经典机器学习的本质

经典机器学习是关于使用统计原理从数据中学习，通常不需要巨大的计算量。
给定数据集 ( D = {$x_i, y_i$} )，任务是：

- 根据 ( x ) 预测 ( y )
- 泛化到未见过的数据
- 平衡偏差和方差

#### 2. k-means 聚类

目标：将数据划分为 ( k ) 个组（簇），使得簇内距离最小化。

#### 目标函数

$$
\min_{C} \sum_{i=1}^k \sum_{x \in C_i} |x - \mu_i|^2
$$
其中 $\mu_i$ = 簇 ( i ) 的质心。

#### 算法

1. 选择 ( k ) 个随机质心
2. 将每个点分配到最近的质心
3. 重新计算质心
4. 重复直到稳定

#### 微型代码（C语言风格）

```c
for (iter = 0; iter < max_iter; iter++) {
    assign_points_to_clusters(); // 将点分配到簇
    recompute_centroids(); // 重新计算质心
}
```

#### 优点

- 简单，快速 (( O(nkd) ))
- 适用于球形簇

#### 缺点

- 需要指定 ( k )
- 对初始化和异常值敏感

变体：

- k-means++（更好的初始化）
- 小批量 k-means（可扩展）

#### 3. 朴素贝叶斯分类器

一种在独立性假设下使用贝叶斯定理的概率模型。

$$
P(y|x) \propto P(y) \prod_{i=1}^n P(x_i | y)
$$

#### 算法

1. 计算先验概率 ( P(y) )
2. 计算似然 ( P$x_i | y$ )
3. 预测具有最大后验概率的类别

#### 类型

- 多项式朴素贝叶斯 - 文本（词袋模型）
- 高斯朴素贝叶斯 - 连续特征
- 伯努利朴素贝叶斯 - 二元特征

#### 示例（垃圾邮件检测）

```
P(垃圾邮件 | "赢钱") ∝ P(垃圾邮件) * P("赢"|垃圾邮件) * P("钱"|垃圾邮件)
```

#### 优点

- 快速，适用于文本
- 需要的数据量少
- 概率解释

#### 缺点

- 假设特征独立
- 对相关特征效果差

#### 4. 支持向量机（SVM）

SVM 寻找将类别分开的最大间隔超平面。

#### 目标函数

最大化间隔 = 边界与最近点之间的距离。

$$
\min_{w, b} \frac{1}{2} |w|^2 \quad \text{s.t.} \quad y_i(w \cdot x_i + b) \ge 1
$$

可以通过二次规划求解。

#### 直观理解

- 每个数据点 → 向量
- 超平面：$w \cdot x + b = 0$
- 支持向量 = 边界点

#### 核技巧

通过核函数 ( K$x_i, x_j$ = \phi$x_i$ \cdot \phi$x_j$ ) 变换输入：

- 线性核：点积
- 多项式核：( $x_i \cdot x_j + c$^d )
- RBF核：$e^{-\gamma |x_i - x_j|^2}$

#### 优点

- 在高维空间中有效
- 可以建模非线性边界
- 超参数少

#### 缺点

- 在大数据上速度慢
- 核参数更难调整

#### 5. 决策树

用于分类/回归的 if-else 结构。

在每个节点：

- 选择特征 ( f ) 和阈值 ( t )
- 分割以最大化信息增益

#### 度量指标

- 熵：$H = -\sum p_i \log p_i$
- 基尼系数：$G = 1 - \sum p_i^2$

#### 伪代码

```c
if (feature < threshold) // 如果特征小于阈值
    go left; // 向左走
else
    go right; // 向右走
```

递归构建直到满足以下条件：

- 最大深度
- 叶节点最小样本数
- 纯节点

#### 优点

- 可解释性强
- 处理混合类型数据
- 无需特征缩放

#### 缺点

- 容易过拟合
- 不稳定（数据微小变化可能导致不同结果）

改进方法：

- 剪枝（减少深度）
- 集成方法：随机森林、梯度提升

#### 6. 偏差-方差权衡

| 算法         | 偏差 | 方差 |
| ------------ | ---- | ---- |
| k-means       | 高   | 低   |
| 朴素贝叶斯   | 高   | 低   |
| SVM           | 低   | 中   |
| 决策树       | 低   | 高   |

平衡两者 = 良好的泛化能力。

#### 7. 评估指标

对于分类：

- 准确率、精确率、召回率、F1分数
- ROC-AUC、混淆矩阵

对于聚类：

- 惯性、轮廓系数

始终使用训练/测试集划分或交叉验证。

#### 8. 扩展到大数据

技术：

- 小批量训练
- 在线更新（随机梯度下降）
- 降维（主成分分析）
- 近似方法（随机投影）

库：

- scikit-learn（Python）
- liblinear, libsvm（C/C++）
- MLlib（Spark）

#### 9. 何时使用何种算法

| 任务                     | 推荐算法               |
| ------------------------ | ---------------------- |
| 文本分类                 | 朴素贝叶斯             |
| 聚类                     | k-means                |
| 非线性分类               | SVM（RBF核）           |
| 表格数据                 | 决策树                 |
| 快速基线模型             | 逻辑回归 / 朴素贝叶斯  |

#### 10. 为何重要

这些算法快速、可解释，并且是现代机器学习的理论基础。
它们仍然是以下情况的首选：

- 中小型数据集
- 实时分类
- 可解释的人工智能

> "经典机器学习是一门用你仍然可以在白板上写出的数学来解决问题的艺术。"

#### 动手实践

1.  使用 k-means 对二维点进行聚类，并绘制质心。
2.  在垃圾邮件/正常邮件数据集上训练朴素贝叶斯模型。
3.  使用 SVM 对线性可分数据进行分类。
4.  从零开始构建决策树（使用熵、基尼系数）。
5.  比较模型的准确率和可解释性。
### 92. 集成方法（装袋法、提升法、随机森林）

集成方法结合多个弱学习器来构建一个强预测器。
与其依赖单一模型，集成方法通过投票、平均或提升多个模型来提高稳定性和准确性。

它们是连接经典与现代机器学习的桥梁，简单的模型通过巧妙组合变得强大。

#### 1. 核心思想

> "多个弱学习器组合起来，可以胜过单个强学习器。"

从数学上讲，如果 $f_1, f_2, \ldots, f_k$ 是弱学习器，
那么集成预测器为：

$$
F(x) = \frac{1}{k}\sum_{i=1}^k f_i(x)
$$

对于分类任务，通过多数投票进行组合。
对于回归任务，通过平均进行组合。

#### 2. 装袋法（Bootstrap Aggregating）

装袋法通过在数据集的不同样本上训练模型来减少方差。

#### 步骤

1. 从数据集 ( D ) 中抽取 ( B ) 个自助样本。
2. 在每个样本上训练一个模型。
3. 通过平均或投票来聚合预测结果。

$$
\hat{f}*{bag}(x) = \frac{1}{B} \sum*{b=1}^B f_b(x)
$$

每个 $f_b$ 都在一个随机子集（有放回抽样）上训练。

#### 示例

- 基础学习器：决策树
- 集成方法：装袋树
- 著名实例：随机森林

#### 微型代码（C风格伪代码）

```c
for (int b = 0; b < B; b++) {
    D_b = bootstrap_sample(D); // 从D中抽取自助样本
    model[b] = train_tree(D_b); // 在样本上训练决策树
}
prediction = average_predictions(model, x); // 对预测结果取平均
```

#### 优点

- 减少方差
- 与高方差学习器配合良好
- 可并行化

#### 缺点

- 增加计算量
- 不能减少偏差

#### 3. 随机森林

一种基于装袋法的决策树集成方法，引入了特征随机性。

#### 关键思想

- 每棵树在自助样本上训练。
- 在每次分裂时，考虑特征的随机子集。
- 最终预测 = 多数投票或平均。

这降低了树之间的相关性，从而提高了泛化能力。

$$
F(x) = \frac{1}{B} \sum_{b=1}^{B} T_b(x)
$$

#### 优点

- 能处理大量特征
- 过拟合程度低
- 是表格数据的良好默认选择

#### 缺点

- 可解释性较差
- 在超大数据集上速度较慢

袋外误差 = 来自未使用样本的内部验证。

#### 4. 提升法

提升法侧重于通过顺序训练模型来减少偏差，
每个模型都纠正前一个模型的错误。

#### 步骤

1. 从弱学习器 ( f_1(x) ) 开始
2. 在残差/误差上训练下一个学习器 ( f_2(x) )
3. 通过加权和进行组合

$$
F_m(x) = F_{m-1}(x) + \alpha_m f_m(x)
$$

权重 $\alpha_m$ 侧重于难以预测的样本。

#### 微型代码（概念性）

```c
F = 0; // 初始化集成模型
for (int m = 0; m < M; m++) {
    residual = y - predict(F, x); // 计算残差
    f_m = train_weak_learner(x, residual); // 在残差上训练弱学习器
    F += alpha[m] * f_m; // 加权添加到集成模型中
}
```

#### 5. AdaBoost（自适应提升）

AdaBoost 在每次迭代后自适应地调整样本权重。

#### 算法

1. 初始化权重：$w_i = \frac{1}{n}$
2. 训练弱分类器 $f_t$
3. 计算误差：$\epsilon_t$
4. 更新权重：
$$
w_i \leftarrow w_i \cdot e^{\alpha_t \cdot I(y_i \ne f_t(x_i))}
$$
其中 $\alpha_t = \frac{1}{2} \ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)$
5. 归一化权重

最终分类器：
$$
F(x) = \text{sign}\left( \sum_t \alpha_t f_t(x) \right)
$$

#### 优点

- 在干净数据上准确率高
- 权重简单且可解释

#### 缺点

- 对异常值敏感
- 顺序执行 → 不易并行化

#### 6. 梯度提升

一种现代版本的提升法，在损失函数上使用梯度下降。

在每一步，将新模型拟合到损失函数的负梯度。

#### 目标

$$
F_m(x) = F_{m-1}(x) + \gamma_m h_m(x)
$$

其中 $h_m(x) \approx -\frac{\partial L(y, F(x))}{\partial F(x)}$

#### 常用库

- XGBoost
- LightGBM
- CatBoost

#### 优点

- 在表格数据上性能高
- 灵活（可自定义损失函数）
- 能处理混合特征类型

#### 缺点

- 训练速度较慢
- 对超参数敏感

#### 7. 堆叠法（堆叠泛化）

通过一个元模型来组合多个模型（基础学习器）。

#### 步骤

1. 训练基础模型（SVM、决策树、朴素贝叶斯等）
2. 收集它们的预测结果
3. 在这些输出上训练元模型（例如逻辑回归）

$$
\hat{y} = f_{meta}(f_1(x), f_2(x), \ldots, f_k(x))
$$

#### 8. 装袋法 vs 提升法

| 特性       | 装袋法           | 提升法            |
| ---------- | ---------------- | ----------------- |
| 策略       | 并行             | 顺序              |
| 目标       | 减少方差         | 减少偏差          |
| 加权方式   | 均匀             | 自适应            |
| 示例       | 随机森林         | AdaBoost, XGBoost |

#### 9. 偏差-方差行为

- 装袋法：↓ 方差
- 提升法：↓ 偏差
- 随机森林：平衡
- 堆叠法：灵活但复杂

#### 10. 为何重要

集成方法是经典机器学习竞赛和现实世界表格问题的主力军。
它们融合了可解释性、灵活性和预测能力。

> "独木易折，森林难摧。"

#### 动手实践

1.  在 Iris 数据集上训练一个随机森林。
2.  使用决策树桩从头实现 AdaBoost。
3.  比较装袋法与提升法的准确率。
4.  尝试使用不同学习率的 XGBoost。
5.  可视化不同模型的特征重要性。
### 93. 梯度方法 (SGD, Adam, RMSProp)

基于梯度的优化是机器学习的心脏。
这些方法通过沿着损失函数的负梯度方向迭代更新参数。
它们为从线性回归到深度神经网络的一切提供动力。

#### 1. 核心思想

我们希望最小化损失函数 $L(\theta)$。
从某个初始参数 $\theta_0$ 开始，我们沿着梯度的反方向移动：

$$
\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta L(\theta_t)
$$

其中 $\eta$ 是学习率（步长）。

梯度告诉我们函数增长最快的方向，我们则向相反方向移动。

#### 2. 批量梯度下降

使用整个数据集来计算梯度。

$$
\nabla_\theta L(\theta) = \frac{1}{N} \sum_{i=1}^N \nabla_\theta \ell_i(\theta)
$$

- 准确，但对于大的 $N$ 来说很慢
- 每次更新代价高昂

微型代码

```c
for (int t = 0; t < T; t++) {
    grad = compute_full_gradient(data, theta); // 计算完整梯度
    theta = theta - eta * grad;
}
```

适用于：小数据集或凸优化问题

#### 3. 随机梯度下降 (SGD)

不使用全部数据，而是每一步使用一个随机样本。

$$
\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta \ell_i(\theta_t)
$$

- 更新有噪声但更快
- 可以逃离局部最小值
- 非常适合在线学习

微型代码

```c
for each sample (x_i, y_i): // 对每个样本
    grad = grad_loss(theta, x_i, y_i); // 计算梯度
    theta -= eta * grad;
```

优点

- 收敛快
- 适用于大数据集

缺点

- 更新有噪声
- 需要调整学习率

#### 4. 小批量梯度下降

批量与随机之间的折中方案。

使用一小部分样本（小批量）：

$$
\theta_{t+1} = \theta_t - \eta \cdot \frac{1}{m} \sum_{i=1}^m \nabla_\theta \ell_i(\theta_t)
$$

通常批量大小为 32 或 64。
更新更快、更稳定。

#### 5. 动量

添加速度项以平滑振荡。

$$
v_t = \beta v_{t-1} + (1 - \beta) \nabla_\theta L(\theta_t)
$$

$$
\theta_{t+1} = \theta_t - \eta v_t
$$

这累积了过去的梯度，以加速在一致方向上的移动。

可以把它想象成一个从山上滚下的重球。

#### 6. Nesterov 加速梯度 (NAG)

通过向前看改进了动量：

$$
v_t = \beta v_{t-1} + \eta \nabla_\theta L(\theta_t - \beta v_{t-1})
$$

它在计算梯度之前预判了未来的位置。

在凸优化场景下收敛更快。

#### 7. RMSProp

使用梯度平方的指数移动平均来调整每个参数的学习率：

$$
E[g^2]_t = \rho E[g^2]_{t-1} + (1 - \rho) g_t^2
$$

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t
$$

这在梯度大小变化很大时很有帮助。

适用于：非平稳目标、深度网络

#### 8. Adam (自适应矩估计)

结合了动量 + RMSProp：

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
$$

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
$$

偏差校正估计：

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

更新规则：

$$
\theta_{t+1} = \theta_t - \frac{\eta \cdot \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

微型代码（概念性）

```c
m = 0; v = 0;
for (int t = 1; t <= T; t++) {
    g = grad(theta); // 计算梯度
    m = beta1 * m + (1 - beta1) * g;
    v = beta2 * v + (1 - beta2) * g * g;
    m_hat = m / (1 - pow(beta1, t));
    v_hat = v / (1 - pow(beta2, t));
    theta -= eta * m_hat / (sqrt(v_hat) + eps);
}
```

优点

- 开箱即用效果好
- 自适应学习率
- 非常适合深度学习

缺点

- 可能无法精确收敛
- 需要衰减计划以保证稳定性

#### 9. 学习率调度

随时间控制 $\eta$：

- 阶梯衰减：$\eta_t = \eta_0 \cdot \gamma^{\lfloor t/s \rfloor}$
- 指数衰减：$\eta_t = \eta_0 e^{-\lambda t}$
- 余弦退火：平滑周期性衰减
- 热重启：周期性重置学习率

#### 10. 为何重要

所有现代深度学习都建立在梯度之上。
选择合适的优化器可能意味着更快的训练和更好的准确率。

| 优化器         | 自适应 | 动量   | 常见用途           |
| -------------- | ------ | ------ | ------------------ |
| SGD            | 否     | 可选   | 简单任务           |
| SGD + Momentum | 否     | 是     | 卷积神经网络       |
| RMSProp        | 是     | 否     | 循环神经网络       |
| Adam           | 是     | 是     | Transformer, 深度神经网络 |

> "优化是一门多次向正确方向迈出小步的艺术。"

#### 动手尝试

1.  在线性回归任务上实现 SGD 和 Adam。
2.  比较 SGD、Momentum、RMSProp 和 Adam 的训练曲线。
3.  尝试不同的学习率调度策略。
4.  在二维等高线图上可视化优化路径。
### 94. 深度学习（反向传播、Dropout、归一化）

深度学习是关于堆叠计算层，以便网络能够学习层次化的表示。
从原始像素到抽象特征，深度网络通过函数的组合来构建意义。

这个过程的核心是三个思想：反向传播、正则化（Dropout）和归一化。


#### 1. 深度学习的本质

神经网络是一系列函数的链：

$$
f(x; \theta) = f_L(f_{L-1}(\cdots f_1(x)))
$$

每一层都转换其输入并将其传递下去。

训练涉及找到最小化损失 $L(f(x; \theta), y)$ 的参数 $\theta$。


#### 2. 反向传播

反向传播是训练神经网络的算法。

它使用微积分的链式法则来逐层高效地计算梯度。

对于每一层 $i$：

$$
\frac{\partial L}{\partial \theta_i} = \frac{\partial L}{\partial a_i} \cdot \frac{\partial a_i}{\partial \theta_i}
$$

并向后传播：

$$
\frac{\partial L}{\partial a_{i-1}} = \frac{\partial L}{\partial a_i} \cdot \frac{\partial a_i}{\partial a_{i-1}}
$$

因此，每个神经元都学习了它对误差的贡献程度。


微型代码

```c
// 2层网络的伪代码
前向传播:
    z1 = W1*x + b1;
    a1 = relu(z1);
    z2 = W2*a1 + b2;
    y_hat = softmax(z2);
    loss = cross_entropy(y_hat, y);

反向传播:
    dz2 = y_hat - y;
    dW2 = dz2 * a1.T;
    db2 = sum(dz2);
    da1 = W2.T * dz2;
    dz1 = da1 * relu_grad(z1);
    dW1 = dz1 * x.T;
    db1 = sum(dz1);
```

每个梯度都通过局部微分计算并乘回。


#### 3. 激活函数

非线性激活函数使网络能够逼近非线性函数。

| 函数名   | 公式                         | 用途                   |
| -------- | ---------------------------- | ---------------------- |
| ReLU     | $\max(0, x)$                 | 默认选择，速度快       |
| Sigmoid  | $\frac{1}{1 + e^{-x}}$       | 概率                   |
| Tanh     | $\tanh(x)$                   | 中心化的激活           |
| GELU     | $x \, \Phi(x)$               | 现代 Transformer 模型  |


如果没有非线性，堆叠层就只是一个大的线性变换。


#### 4. Dropout

Dropout 是一种防止过拟合的正则化技术。
在训练期间，随机关闭神经元：

$$
\tilde{a}_i = a_i \cdot m_i, \quad m_i \sim \text{Bernoulli}(p)
$$

在推理时，将激活按保留概率 $p$ 进行缩放。

它迫使网络不依赖任何单一路径。

微型代码

```c
for (int i = 0; i < n; i++) {
    if (rand_uniform() < p) a[i] = 0;
    else a[i] /= p; // 缩放
}
```


#### 5. 归一化

归一化通过减少内部协变量偏移来稳定和加速训练。

#### 批归一化

对每个批次的激活进行归一化：

$$
\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$

$$
y = \gamma \hat{x} + \beta
$$

可学习的参数 $\gamma, \beta$ 恢复了灵活性。

优点：

- 平滑梯度
- 允许更高的学习率
- 起到正则化作用

#### 层归一化

用于 Transformer 模型（跨特征归一化，而不是批次）。


#### 6. 初始化

适当的初始化有助于梯度流动。

| 方案     | 公式                              | 用途   |
| -------- | --------------------------------- | ------ |
| Xavier   | $\text{Var}(W) = \frac{1}{n_{in}}$ | Tanh   |
| He       | $\text{Var}(W) = \frac{2}{n_{in}}$ | ReLU   |


糟糕的初始化可能导致梯度消失或爆炸。


#### 7. 训练流程

1. 初始化权重
2. 前向传播
3. 计算损失
4. 反向传播
5. 更新权重（例如使用 Adam）

重复直到收敛。


#### 8. 深度架构

| 模型         | 核心理念           | 典型用途           |
| ------------ | ------------------ | ------------------ |
| MLP          | 全连接             | 表格数据           |
| CNN          | 卷积               | 图像               |
| RNN          | 序列递归           | 时间序列、文本     |
| Transformer  | 自注意力           | 语言、视觉         |

每种架构都以不同的方式堆叠线性运算和非线性运算。


#### 9. 过拟合与正则化

常见解决方法：

- Dropout
- 权重衰减（$L_2$ 正则化）
- 数据增强
- 早停

关键在于提高泛化能力，而不仅仅是最小化训练损失。


#### 10. 为何重要

反向传播将神经网络从理论变为实践。
归一化使它们训练得更快。
Dropout 使它们泛化得更好。

它们共同开启了深度学习革命。

> "深度赋予力量，但梯度赋予生命。"


#### 动手实践

1.  实现一个带有 ReLU 和 softmax 的 2 层网络。
2.  添加 Dropout 和批归一化。
3.  可视化使用和不使用 Dropout 的训练过程。
4.  比较在 MNIST 数据集上使用和不使用归一化的性能。
### 95. 序列模型（Viterbi、Beam Search、CTC）

序列模型处理顺序重要的数据，例如文本、语音、DNA、时间序列。
它们捕捉跨位置的依赖关系，根据上下文预测下一步。

本节探讨三个基本工具：Viterbi（维特比）、Beam Search（束搜索）和 CTC（连接时序分类）。

#### 1. 序列数据的本质

序列数据具有时间或结构顺序。
每个元素 $x_t$ 依赖于过去的输入 $x_{1:t-1}$。

常见的序列任务：

- 标注（词性标注、命名实体识别）
- 转录（语音 → 文本）
- 解码（翻译、路径重建）

为了处理这类问题，我们需要能够记忆的模型。

#### 2. 隐马尔可夫模型

隐马尔可夫模型假设：

- 一个隐藏状态序列 $z_1, z_2, \ldots, z_T$
- 每个状态生成一个观测值 $x_t$
- 转移概率和发射概率控制该过程

$$
P(z_t | z_{t-1}) = A_{z_{t-1}, z_t}, \quad P(x_t | z_t) = B_{z_t}(x_t)
$$

目标：在给定观测序列的情况下，找到最可能的隐藏状态序列。

#### 3. Viterbi 算法

Viterbi 是一种动态规划算法，用于解码最可能的路径：

$$
\delta_t(i) = \max_{z_{1:t-1}} P(z_{1:t-1}, z_t = i, x_{1:t})
$$

递推关系：

$$
\delta_t(i) = \max_j \big( \delta_{t-1}(j) \cdot A_{j,i} \big) \cdot B_i(x_t)
$$

跟踪回溯指针以重建最佳序列。

时间复杂度：$O(T \cdot N^2)$，
其中 $N$ = 状态数，$T$ = 序列长度。

微型代码

```c
for (t = 1; t < T; t++) {
    for (i = 0; i < N; i++) {
        double best = -INF;
        int argmax = -1;
        for (j = 0; j < N; j++) {
            double score = delta[t-1][j] * A[j][i];
            if (score > best) { best = score; argmax = j; }
        }
        delta[t][i] = best * B[i][x[t]];
        backptr[t][i] = argmax;
    }
}
```

使用 `backptr` 来回溯最优路径。

#### 4. Beam Search（束搜索）

对于许多序列模型（例如神经机器翻译），穷举搜索是不可能的。
束搜索在每一步只保留 top-k 个最佳假设。

算法：

1.  从一个空序列和分数 0 开始
2.  在每一步，用所有可能的下一个标记扩展每个候选序列
3.  只保留 k 个最佳序列（束宽）
4.  当所有序列结束或达到最大长度时停止

束宽控制权衡：

- 束宽越大 → 精度越高，速度越慢
- 束宽越小 → 速度越快，风险越大

微型代码

```c
for (step = 0; step < max_len; step++) {
    vector<Candidate> new_beam;
    for (c in beam) {
        probs = model_next(c.seq);
        for (token, p in probs)
            new_beam.push({c.seq + token, c.score + log(p)});
    }
    beam = top_k(new_beam, k);
}
```

使用对数概率以避免下溢。

#### 5. 连接时序分类

用于输入和输出长度不同的语音识别和手写识别任务。

CTC 学习在没有显式对齐的情况下将输入帧与输出符号对齐。

添加一个特殊的空白符号（∅）以允许灵活对齐。

示例（CTC 解码）：

| 帧       | 输出    | 折叠后      |
| -------- | ------- | ----------- |
| A ∅ A A  | A ∅ A   | A A         |
| H ∅ ∅ H  | H H     | H           |

损失：
$$
P(y | x) = \sum_{\pi \in \text{Align}(x, y)} P(\pi | x)
$$
其中 $\pi$ 是所有能缩减为 $y$ 的对齐路径。

CTC 使用动态规划计算前向-后向概率。

#### 6. 方法比较

| 方法         | 用于                | 核心思想             | 处理对齐？ |
| ------------ | ------------------- | -------------------- | ---------- |
| Viterbi      | HMM                 | 最可能的状态路径     | 是         |
| Beam Search  | 神经解码器          | 近似搜索             | 隐式       |
| CTC          | 语音 / seq2seq 模型 | 对所有对齐路径求和   | 是         |

#### 7. 使用场景

- Viterbi：词性标注、语音解码
- Beam Search：翻译、文本生成
- CTC：自动语音识别、光学字符识别、手势识别

#### 8. 实现技巧

- 在概率计算中使用对数空间
- 在束搜索中，应用长度归一化
- 在 CTC 中，使用动态规划表
- 结合 CTC 和束搜索进行语音解码

#### 9. 常见陷阱

- Viterbi 假设马尔可夫性质（有限记忆）
- Beam Search 可能错过全局最优解
- CTC 在没有空白符时可能混淆重复字符

#### 10. 为何重要

序列模型是结构与时间之间的桥梁。
它们展示了如何解码有序数据中的隐藏含义。

从解码摩尔斯电码到转录语音，这些算法赋予了机器理解序列的能力。

#### 动手实践

1.  为 3 状态 HMM 实现 Viterbi 算法。
2.  在玩具语言模型上比较贪婪解码与束搜索。
3.  为短序列（如 "HELLO"）构建 CTC 损失表。
### 96. 元启发式算法（GA、SA、PSO、ACO）

元启发式算法是通用的优化策略，当精确方法太慢或不可行时，它们可以在广阔、复杂的空间中进行搜索。
它们不保证找到完美答案，但通常能快速找到足够好的解。

本节涵盖四种经典算法：

- GA（遗传算法）- SA（模拟退火）- PSO（粒子群优化）- ACO（蚁群优化）

#### 1. 元启发式算法的哲学

元启发式算法从自然和物理学中汲取灵感。
它们结合了探索（广泛搜索）和利用（改进有希望的区域）。

它们适用于：

- NP难问题（TSP、调度问题）- 连续优化（参数调优）- 黑盒函数（无梯度）
它们用数学上的保证换取实际求解能力。

#### 2. 遗传算法（GA）

灵感来源于自然选择，GA 通过演化一个解的种群来工作。

#### 核心步骤

1. 随机初始化种群
2. 评估每个个体的适应度
3. 选择父代
4. 交叉产生子代
5. 变异以增加多样性
6. 用新候选解替换最差的个体

重复直到收敛。

微型代码

```c
for (gen = 0; gen < max_gen; gen++) {
    evaluate(pop);
    parents = select_best(pop);
    offspring = crossover(parents);
    mutate(offspring);
    pop = select_survivors(pop, offspring);
}
```

算子

- 选择：锦标赛选择、轮盘赌选择- 交叉：单点交叉、均匀交叉- 变异：位翻转、高斯变异
优点：全局搜索，探索多样性强
缺点：可能收敛缓慢

#### 3. 模拟退火（SA）

模仿金属冷却过程，开始时温度高（随机性强），然后缓慢降温。

每一步：

1. 提出一个随机邻居
2. 如果更好则接受
3. 如果更差，则以概率接受
$$
P = e^{-\frac{\Delta E}{T}}
$$
4. 逐渐降低温度 ( T )

微型代码

```c
T = T_init;
state = random_state();
while (T > T_min) {
    next = neighbor(state);
    dE = cost(next) - cost(state);
    if (dE < 0 || exp(-dE/T) > rand_uniform())
        state = next;
    T *= alpha; // 冷却速率
}
```

优点：能够逃离局部极小值
缺点：对冷却计划敏感

#### 4. 粒子群优化（PSO）

灵感来源于鸟群行为。
每个粒子根据以下因素调整速度：

- 它自身找到的最佳位置- 整个群体找到的全局最佳位置
$$
v_i \leftarrow w v_i + c_1 r_1 (p_i - x_i) + c_2 r_2 (g - x_i)
$$

$$
x_i \leftarrow x_i + v_i
$$

微型代码

```c
for each particle i:
    v[i] = w*v[i] + c1*r1*(pbest[i]-x[i]) + c2*r2*(gbest-x[i]);
    x[i] += v[i];
    update_best(i);
```

优点：适用于连续域，简单易实现
缺点：容易过早收敛

#### 5. 蚁群优化（ACO）

灵感来源于蚂蚁觅食，蚂蚁在路径上留下信息素。
路径上的信息素越强，其他蚂蚁跟随的可能性越大。

步骤：

1. 在所有边上初始化信息素
2. 每只蚂蚁构建一个解（概率与信息素成正比）
3. 评估路径
4. 信息素挥发
5. 增强优秀路径的信息素

$$
\tau_{ij} \leftarrow (1 - \rho)\tau_{ij} + \sum_k \Delta\tau_{ij}^k
$$

微型代码

```c
for each iteration:
    for each ant:
        path = build_solution(pheromone);
        score = evaluate(path);
    evaporate(pheromone);
    deposit(pheromone, best_paths);
```

优点：适用于组合优化问题（如 TSP）
缺点：参数调优困难，收敛较慢

#### 6. 四种算法比较

| 方法 | 灵感来源       | 最适用场景         | 核心思想                       |
| ---- | -------------- | ------------------ | ------------------------------ |
| GA   | 进化论         | 离散搜索           | 选择、交叉、变异               |
| SA   | 热力学         | 逃离局部最优       | 冷却 + 随机性                  |
| PSO  | 群体行为       | 连续搜索           | 局部 + 全局吸引                |
| ACO  | 蚂蚁觅食       | 图路径问题         | 信息素增强                     |

#### 7. 设计模式

常见的元启发式算法模式：

- 表示解- 定义适应度/代价函数- 定义邻居/变异算子- 平衡随机性和贪婪性
参数调优通常比公式本身更重要。

#### 8. 混合元启发式算法

结合优势：

- GA + SA：演化种群，局部精细调优- PSO + DE：使用粒子群 + 差分进化- ACO + 局部搜索：用爬山法增强
这些混合方法通常优于单一方法。

#### 9. 常见陷阱

- 表示方式不佳 → 搜索能力弱- 过度利用 → 陷入局部最优- 参数设置不当 → 行为混乱或停滞不前
始终要可视化进度（适应度随时间变化）。

#### 10. 为何重要

元启发式算法赋予我们自适应的智能，能够在没有梯度、方程或完整知识的情况下进行搜索。
它们反映了自然界解决复杂难题的方式：迭代、适应、生存。

> "优化不在于追求完美，而在于由好奇心引导的进步。"

#### 动手实践

1.  为旅行商问题实现模拟退火算法。
2.  为背包优化问题创建一个遗传算法。
3.  调整 PSO 参数以拟合函数 $f(x) = x^2 + 10\sin x$。
4.  比较不同挥发率下 ACO 求解 TSP 的路径。
### 97. 强化学习（Q-learning，策略梯度）

强化学习（RL）是关于通过交互进行学习，智能体探索环境、采取行动并从奖励中学习。
与监督学习（提供正确标签）不同，强化学习通过试错来学习该做什么。

本节介绍两种核心方法：

- Q-learning（基于价值）
- 策略梯度（基于策略）

#### 1. 强化学习设定

强化学习问题被建模为马尔可夫决策过程（MDP）：

- 状态 $S$
- 动作 $A$
- 转移 $P(s' \mid s, a)$
- 奖励 $R(s, a)$
- 折扣因子 $\gamma$

智能体的目标是找到一个策略 $\pi(a \mid s)$，以最大化期望回报：

$$
G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
$$



#### 2. 价值函数

价值函数衡量一个状态（或状态-动作对）有多好。

- 状态价值：
$$
V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
$$

- 动作价值（Q函数）：
$$
Q^\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]
$$


#### 3. 贝尔曼方程

贝尔曼方程将一个状态的价值与其邻居状态联系起来：

$$
Q^*(s,a) = R(s,a) + \gamma \max_{a'} Q^*(s',a')
$$

这个递归定义驱动了价值迭代和 Q-learning。


#### 4. Q-Learning

Q-learning 以离策略方式学习最优动作价值函数（独立于行为策略）：

更新规则：
$$
Q(s,a) \leftarrow Q(s,a) + \alpha \big[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \big]
$$

微型代码

```c
Q[s][a] += alpha * (r + gamma * max(Q[s_next]) - Q[s][a]);
s = s_next;
```

在探索时重复（例如，$\varepsilon$-贪心）：

- 以概率 $\varepsilon$，选择一个随机动作
- 以概率 $1 - \varepsilon$，选择最佳动作

随着时间的推移，$Q$ 收敛到 $Q^*$。



#### 5. 探索 vs 利用

强化学习是一种平衡行为：

- 探索：尝试新动作以收集知识
- 利用：使用当前最佳知识以最大化奖励

策略：

- ε-贪心
- Softmax 动作选择
- 上置信界（UCB）

#### 6. 策略梯度方法

不学习 Q 值，而是直接学习策略。
用参数 $\theta$ 表示策略：

$$
\pi_\theta(a|s) = P(a | s; \theta)
$$

目标：最大化期望回报
$$
J(\theta) = \mathbb{E}*{\pi*\theta}[G_t]
$$

梯度上升更新：
$$
\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)
$$

REINFORCE 算法：
$$
\nabla_\theta J(\theta) = \mathbb{E}\big[ G_t \nabla_\theta \log \pi_\theta(a_t|s_t) \big]
$$

微型代码

```c
theta += alpha * G_t * grad_logpi(a_t, s_t);
```


#### 7. Actor-Critic 架构

结合了策略梯度（actor）和价值估计（critic）。

- Actor：更新策略
- Critic：估计价值（基线）

更新：
$$
\theta \leftarrow \theta + \alpha_\theta \delta_t \nabla_\theta \log \pi_\theta(a_t|s_t)
$$

$$
w \leftarrow w + \alpha_w \delta_t \nabla_w V_w(s_t)
$$

其中 TD 误差：
$$
\delta_t = r + \gamma V(s') - V(s)
$$


#### 8. 方法比较

| 方法          | 类型         | 学习内容  | 在/离策略 | 连续动作？ |     |
| --------------- | ------------ | ------- | ------------- | ----------- | --- |
| Q-learning      | 基于价值  | Q(s, a) | 离策略    | 否          |     |
| Policy Gradient | 基于策略 | π(a     | s)            | 在策略   | 是 |
| Actor-Critic    | 混合       | 两者    | 在策略     | 是         |     |


#### 9. 扩展

- 深度 Q 网络（DQN）：使用神经网络表示 Q(s, a)
- PPO / A3C：高级的 actor-critic 方法
- TD(λ)：蒙特卡洛和 TD 学习之间的权衡
- 双重 Q-learning：减少过高估计
- 熵正则化：鼓励探索

#### 10. 为何重要

强化学习为自主智能体、游戏 AI 和控制系统提供动力。
它是 AlphaGo、机器人控制和自适应决策系统的基础。

> "智能体不是从指令中学习，而是从经验中学习。"


#### 动手尝试

1.  为网格世界迷宫实现 Q-learning。
2.  添加 ε-贪心探索。
3.  可视化学习到的策略。
4.  尝试使用简单策略（例如动作上的 softmax）实现 REINFORCE。
5.  比较 Q-learning 与策略梯度的收敛性。
### 98. 近似算法与在线算法

在现实世界中，我们常常不能等待一个完美的解决方案，数据是动态到达的，或者问题本身太难而无法精确求解。这正是近似算法和在线算法大放异彩的地方。它们的目标是在不确定性的条件下，快速、自适应地获得足够好的结果。

#### 1. 概览

- **近似算法**：以可证明的界限求解 NP 难问题。
- **在线算法**：在不知道未来信息的情况下立即做出决策。
两者都为了效率或适应性而牺牲了最优性。

#### 2. 近似算法

近似算法寻找一个在最优解 $\rho$ 倍范围内的解。

如果 $C$ 是算法的成本，$C^*$ 是最优成本：

$$
\rho = \max\left(\frac{C}{C^*}, \frac{C^*}{C}\right)
$$

例如：$\rho = 2$ → 解最多比最优解差两倍。

#### 3. 示例：顶点覆盖

问题：给定图 $G(V,E)$，选择覆盖所有边的最小顶点集合。

算法（2-近似）：

1. 初始化覆盖集 = ∅
2. 当还有边剩余时：
   - 选择任意一条边 (u, v)
   - 将 u 和 v 都加入覆盖集
   - 移除所有与 u 或 v 关联的边

保证：覆盖集大小最多为最优解的 2 倍。

微型代码

```c
cover = {};
while (!edges.empty()) {
    (u, v) = edges.pop();
    cover.add(u);
    cover.add(v);
    remove_incident_edges(u, v);
}
```

#### 4. 示例：度量 TSP（三角不等式）

算法（Christofides）：

1. 找到最小生成树
2. 找到奇度顶点
3. 找到最小完美匹配
4. 合并并短路以得到回路

保证：≤ 1.5 × 最优解。

#### 5. 贪心近似：集合覆盖

目标：用最少的集合 $S_i$ 覆盖全集 $U$。

贪心算法：每次选择覆盖最多未覆盖元素的集合。
保证：$H_n \approx \ln n$ 倍近似。

#### 6. 在线算法

在线算法必须在未来输入未知的情况下立即做出决策。

目标：最小化竞争比：

$$
\text{CR} = \max_{\text{输入}} \frac{\text{在线算法成本}}{\text{最优离线算法成本}}
$$

竞争比越低 → 适应性越好。

#### 7. 经典示例：在线页面调度

缓存中有 k 个页面，有一系列页面请求。

- 如果页面在缓存中 → 命中
- 否则 → 未命中，必须驱逐一个页面

策略：
- LRU（最近最少使用）：驱逐最久未使用的
- FIFO：驱逐最先加载的
- Random：随机选择

竞争比：
- LRU: ≤ $k$
- Random: ≤ $2k-1$

微型代码

```c
cache = LRUCache(k);
for (page in requests) {
    if (!cache.contains(page))
        cache.evict_oldest();
    cache.add(page);
}
```

#### 8. 在线二分图匹配（Karp-Vazirani-Vazirani）

给定离线集合 U 和在线集合 V（逐个到达），贪心地匹配。
竞争比：$1 - \frac{1}{e}$

用于广告分配和资源分配。

#### 9. 近似与在线相结合

现代算法融合了二者：

- **流式算法**：单次遍历，小内存（Count-Min、蓄水池采样）
- **在线学习**：增量更新模型（SGD、感知器）
- **近似动态规划**：强化学习和启发式搜索

这些是近似的在线求解器，既快速又具有适应性。

#### 10. 为何重要

近似算法为我们提供了可证明的接近最优的答案。
在线算法为我们提供了实时适应性。
两者共同模拟了在时间和信息稀缺的限制条件下的智能行为。

> "有时，及时且足够好胜过完美但迟到。"

#### 动手实践

1.  在一个小图上实现 2-近似顶点覆盖算法。
2.  用 LRU 与 Random 策略模拟在线页面调度。
3.  构建一个贪心集合覆盖求解器。
4.  在测试序列上测量竞争比。
5.  结合思想：为大数据过滤实现流式+近似算法。
### 99. 公平性、因果推断与鲁棒优化

随着算法日益影响从招聘、贷款到医疗保健等领域的决策，我们必须确保它们是公平的、因果合理的，并且对不确定性具有鲁棒性。
本节融合了伦理学、统计学和优化领域的思想，旨在使算法不仅高效，而且负责任且可靠。

#### 1. 为何公平性至关重要

机器学习系统常常从数据中继承偏见。
若不加以干预，它们可能会加剧不平等或歧视。

具有公平意识的算法能够明确地衡量并纠正这些影响。

常见的偏见来源：

- 历史偏见（有偏数据）
- 测量偏见（特征不精确）
- 选择偏见（样本有偏）

目标：在敏感群体（性别、种族、地区等）间实现公平对待。

#### 2. 形式化的公平性标准

存在多种公平性概念，它们常常相互冲突：

| 标准 | 描述 | 示例 | | |
| :--- | :--- | :--- | :--- | :--- |
| 人口统计学均等 | ( P$\hat{Y}=1$ \| A=a$ = P$\hat{Y}=1$ \| A=b$ ) | 相等的正类率 | | |
| 机会均等 | 相等的真正类率 | 所有群体具有相同的召回率 | | |
| 均衡几率 | 相等的真正类率与假正类率 | 平衡的错误率 | | |
| 校准 | 预测概率含义相同 | 如果模型预测为70%，所有群体都应达到70% | | |

没有单一标准适用于所有情况，公平性取决于具体情境和权衡取舍。

#### 3. 算法公平性技术

1. 预处理
   在训练前重新平衡或重新加权数据。
   示例：重新加权、采样。

2. 处理中
   在损失函数中添加公平性约束。
   示例：对抗性去偏。

3. 后处理
   训练后调整预测结果。
   示例：阈值偏移。

微型代码（对抗性去偏框架）

```python
for x, a, y in data:
    y_pred = model(x)
    loss_main = loss_fn(y_pred, y)
    loss_adv = adv_fn(y_pred, a)
    loss_total = loss_main - λ * loss_adv
    update(loss_total)
```

此处，对抗器试图预测敏感属性，从而鼓励模型对敏感属性具有不变性。

#### 4. 因果推断基础

相关性 ≠ 因果性。
为了推理公平性和鲁棒性，我们需要因果理解——即如果我们改变某些因素，结果*会*发生什么。

因果推断通过有向无环图（DAGs）来建模关系：

- 节点：变量
- 边：因果影响

#### 5. 反事实推理

一个反事实问题询问：

> "如果我们采取不同的干预措施，结果会是什么？"

形式化表示：
$$
P(Y_{do(X=x)})
$$

应用于：

- 公平性（反事实公平性）
- 政策评估
- 鲁棒决策制定

#### 6. 反事实公平性

如果一个算法在假设敏感属性发生改变的情况下，其预测保持不变，则该算法具有反事实公平性。

$$
\hat{Y}*{A \leftarrow a}(U) = \hat{Y}*{A \leftarrow a'}(U)
$$

这需要因果模型，而不仅仅是数据。

#### 7. 鲁棒优化

在不确定的环境中，我们希望找到在最坏情况下仍然有效的解决方案。

公式化表示：
$$
\min_x \max_{\xi \in \Xi} f(x, \xi)
$$

其中 $\Xi$ 是不确定性集合。

示例：
设计一个在不同市场条件下都能表现良好的投资组合。

微型代码

```c
double robust_objective(double x[], Scenario Xi[], int N) {
    double worst = -INF;
    for (i=0; i<N; i++)
        worst = max(worst, f(x, Xi[i]));
    return worst;
}
```

此代码寻找一个能最小化最坏情况损失的解决方案。

#### 8. 分布鲁棒性

不是针对最坏情况的实例，而是针对最坏情况的分布进行保护：

$$
\min_\theta \sup_{Q \in \mathcal{B}(P)} \mathbb{E}_{x \sim Q}[L(\theta, x)]
$$

应用于对抗性训练和领域自适应。

示例：
添加噪声或扰动以提高鲁棒性：

```python
x_adv = x + ε * sign(grad(loss, x))
```

#### 9. 平衡公平性、因果性与鲁棒性

| 目标 | 方法 | 挑战 |
| :--- | :--- | :--- |
| 公平性 | 均等、对抗性、反事实 | 定义相互竞争 |
| 因果性 | DAGs、do-演算、SCMs | 识别真实结构 |
| 鲁棒性 | 最小-最大、DRO、对抗性训练 | 与准确性的权衡 |

现实世界的设计涉及平衡这些权衡取舍。
有时，提高公平性会降低准确性，或者提高鲁棒性会增加保守性。

#### 10. 为何这很重要

算法并非孤立存在，它们影响着人们。
嵌入公平性、因果性和鲁棒性，能确保系统是可信赖的、可解释的和公正的。

> "目标不仅是智能的算法，更是负责任的算法。"

#### 动手尝试

1.  在有偏数据上训练一个简单的分类器。
2.  应用重新加权或对抗性去偏技术。
3.  为你的数据特征绘制一个因果 DAG。
4.  计算一个样本的反事实公平性。
5.  使用对抗性扰动实现一个鲁棒损失函数。
### 100. AI 规划、搜索与学习系统

AI 系统不仅仅是模式识别器，它们也是决策者。它们在结构化环境中进行规划、搜索和学习，选择能够实现长期目标的行动。本节探讨现代 AI 如何结合规划、搜索和学习来解决复杂任务。

#### 1. 什么是 AI 规划？

AI 规划旨在找到一个行动序列，将初始状态转变为目标状态。

形式上，一个规划问题包含：

- 状态 ( S )
- 动作 ( A )
- 转移函数 ( T(s, a) \to s' )
- 目标条件 $G \subseteq S$
- 代价函数 ( c(a) )

目标：
找到一个计划 $\pi = [a_1, a_2, \ldots, a_n]$，以最小化总代价或最大化奖励。

#### 2. 基于搜索的规划

规划的核心在于搜索。搜索在启发式引导下探索可能的行动序列。

| 算法      | 类型       | 描述                     |
| --------- | ---------- | ------------------------ |
| DFS       | 无信息     | 深度探索，无最优保证     |
| BFS       | 无信息     | 找到最短路径             |
| Dijkstra  | 加权       | 若代价 ≥ 0 则最优        |
| A*        | 启发式     | 结合代价与启发式         |

A* 搜索公式：
$$
f(n) = g(n) + h(n)
$$
其中：

- ( g(n) ): 当前已花费的代价
- ( h(n) ): 到目标的启发式估计

如果 ( h ) 是可采纳的，则 A* 是最优的。

微型代码 (A* 框架)

```c
priority_queue<Node> open;
g[start] = 0;
open.push({start, h(start)});

while (!open.empty()) {
    n = open.pop_min();
    if (goal(n)) break;
    for (a in actions(n)) {
        s = step(n, a);
        cost = g[n] + c(n, a);
        if (cost < g[s]) {
            g[s] = cost;
            f[s] = g[s] + h(s);
            open.push({s, f[s]});
        }
    }
}
```

#### 3. 启发式与可采纳性

启发式 ( h(s) ) 估计到目标的距离。

- 可采纳的：从不高估
- 一致的：满足三角不等式

示例：

- 曼哈顿距离（网格）
- 欧几里得距离（几何）
- 模式数据库（谜题）

好的启发式 = 更快的收敛。

#### 4. 经典规划 (STRIPS)

在符号 AI 中，状态由事实（谓词）表示，动作具有前提条件和效果。

示例：

```
动作：移动(x, y)
前提条件：位于(x), 空闲(y)
效果：¬位于(x), 位于(y)
```

搜索发生在逻辑状态空间中。

规划器：

- 前向搜索（递进）
- 后向搜索（回归）
- 启发式规划器（FF, HSP）

#### 5. 分层规划

将复杂目标分解为子目标。

- HTN（分层任务网络）：定义分解为子任务的高层任务。

示例：
"做晚餐" → [煮饭, 炒菜, 摆桌子]

层次结构使规划模块化且可解释。

#### 6. 概率规划

当动作不确定时：

- MDPs：完全可观测，随机转移
- POMDPs：部分可观测

使用值迭代、策略迭代或蒙特卡洛规划。

#### 7. 学习规划

将学习与搜索结合：

- 学习到的启发式：神经网络近似 ( h(s) )
- AlphaZero 式规划：学习价值 + 策略，指导树搜索
- 模仿学习：模仿专家演示

这连接了经典 AI 和现代 ML。

微型代码（学习引导的 A*）

```python
f = g + alpha * learned_heuristic(s)
```

神经网络从已解决的示例中学习 ( h_\theta(s) )。

#### 8. 集成系统

现代 AI 栈结合了：

- 搜索（规划主干）
- 学习（策略、启发式、模型）
- 仿真（数据生成）

示例：

- AlphaZero：自我对弈 + MCTS + 神经网络
- MuZero：联合学习模型 + 价值 + 策略
- 大型语言智能体：使用推理 + 记忆 + 搜索

#### 9. 实际应用

- 机器人学：运动规划、路径查找
- 游戏：围棋、国际象棋、策略游戏
- 物流：路线优化
- 自主系统：无人机、车辆、AI 助手
- 合成：程序和查询生成

每一项都融合了符号推理和统计学习。

#### 10. 为何重要

规划、搜索和学习构成了智能的三要素：

- 搜索探索可能性
- 规划将行动序列导向目标
- 学习从经验中调整启发式

它们共同为能够思考、适应和行动的系统提供动力。

> "智能不仅仅是知道，它是在约束下明智地选择。"

#### 动手尝试

1.  在网格迷宫中实现 A* 搜索。
2.  添加曼哈顿距离启发式。
3.  扩展到概率转移（模拟噪声）。
4.  构建一个具有前提条件和效果的简单规划器。
5.  训练一个神经网络启发式来指导谜题搜索。
