---
title: 第一章
date: 2026-02-25
disableNunjucks: true
---

# 第 1 章 算法基础

## 第 1 节 什么是算法？
### 1 欧几里得最大公约数算法

欧几里得算法是数学中最古老、最优雅的算法之一。它通过反复应用一个简单的规则来计算两个整数的最大公约数（GCD）：用较大数除以较小数所得的余数替换较大数。当余数变为零时，该步骤中的较小数就是最大公约数。

#### 我们要解决什么问题？

我们想要求两个整数 $a$ 和 $b$ 的最大公约数：即能同时整除这两个数且没有余数的最大数字。

一种朴素的方法是检查从 $\min(a,b)$ 到 1 的所有数字。这需要 $O(\min(a,b))$ 步，对于大的输入来说太慢了。欧几里得的洞见给出了一种使用除法的、快得多的递归方法：

$$
\gcd(a, b) =
\begin{cases}
a, & \text{如果 } b = 0, \\
\gcd(b, a \bmod b), & \text{否则。}
\end{cases}
$$

#### 工作原理（通俗解释）

想象两根长度分别为 $a$ 和 $b$ 的木棍。你可以不断地用较短木棍的长度去截取较长木棍，直到其中一根能被另一根整除。最后一个非零余数的长度就是最大公约数。

步骤：

1.  取 $a, b$，其中 $a \ge b$。
2.  用 $b$ 替换 $a$，用 $a \bmod b$ 替换 $b$。
3.  重复直到 $b = 0$。
4.  返回 $a$。

这个过程总是会终止，因为 $b$ 每一步都严格递减。

#### 逐步示例

求 $\gcd(48, 18)$：

| 步骤 | $a$ | $b$ | $a \bmod b$ |
| ---- | --- | --- | ----------- |
| 1    | 48  | 18  | 12          |
| 2    | 18  | 12  | 6           |
| 3    | 12  | 6   | 0           |

当 $b = 0$ 时，$a = 6$。
所以 $\gcd(48, 18) = 6$。

#### 微型代码（Python）

```python
def gcd(a, b):
    while b != 0:
        a, b = b, a % b
    return a

print(gcd(48, 18))  # 输出: 6
```

#### 为何重要

-   算法思维的基础范例
-   模运算、数论和密码学的核心构建模块
-   高效：在 $O(\log \min(a,b))$ 步内运行
-   易于迭代或递归实现

#### 一个温和的证明（为何有效）

如果 $a = bq + r$，那么 $a$ 和 $b$ 的任何公约数也整除 $r$，因为 $r = a - bq$。因此，$(a, b)$ 和 $(b, r)$ 的公约数集合是相同的，它们的最大元素（即最大公约数）保持不变。

重复应用这个性质会导致 $b = 0$，此时 $\gcd(a, 0) = a$。

#### 动手试试

1.  逐步计算 $\gcd(270, 192)$。
2.  实现递归版本：

$$
\gcd(a, b) = \gcd(b,, a \bmod b)
$$

3.  扩展到使用 $\gcd(\gcd(a, b), c)$ 来求 $\gcd(a, b, c)$。

#### 测试用例

| 输入 $(a, b)$ | 期望输出 |
| -------------- | --------------- |
| (48, 18)       | 6               |
| (270, 192)     | 6               |
| (7, 3)         | 1               |
| (10, 0)        | 10              |

#### 复杂度

| 操作 | 时间                | 空间  |
| --------- | ------------------- | ------ |
| 最大公约数       | $O(\log \min(a,b))$ | $O(1)$ |

欧几里得最大公约数算法是算法优雅性的起点，它是一个永恒的除法循环，将数学转化为运动。
### 2 埃拉托斯特尼筛法

埃拉托斯特尼筛法是一种经典的古老算法，用于找出所有小于或等于给定上限的质数。它通过迭代地标记每个质数的倍数（从 2 开始）来工作。最终未被标记的数字就是质数。

#### 我们要解决什么问题？

我们想找出所有小于或等于 $n$ 的质数。
一种朴素的方法是检查每个数字 $k$，通过测试从 $2$ 到 $\sqrt{k}$ 的可除性，这对于大的 $n$ 来说太慢了。
筛法通过使用淘汰法而不是重复检查来改进这一点。

我们的目标是设计一个时间复杂度接近 $O(n \log \log n)$ 的算法。

#### 工作原理（通俗解释）

1.  创建一个列表 `is_prime[0..n]` 并将所有元素标记为 `true`。
2.  将 0 和 1 标记为非质数。
3.  从 $p = 2$ 开始，如果 $p$ 仍被标记为质数：
    *   标记 $p$ 的所有倍数（从 $p^2$ 到 $n$）为非质数。
4.  递增 $p$ 并重复，直到 $p^2 > n$。
5.  所有仍被标记为 `true` 的索引就是质数。

这个过程一步步"筛除"合数，就像用越来越细的筛子过滤沙子一样。

#### 逐步示例

找出所有小于等于 $30$ 的质数：

初始列表：
$[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]$

-   $p = 2$：划掉 2 的倍数
-   $p = 3$：划掉 3 的倍数
-   $p = 5$：划掉 5 的倍数

剩余数字：
$2, 3, 5, 7, 11, 13, 17, 19, 23, 29$

#### 微型代码（Python）

```python
def sieve(n):
    is_prime = [True] * (n + 1)
    is_prime[0] = is_prime[1] = False

    p = 2
    while p * p <= n:
        if is_prime[p]:
            for i in range(p * p, n + 1, p):
                is_prime[i] = False
        p += 1

    return [i for i in range(2, n + 1) if is_prime[i]]

print(sieve(30))  # [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]
```

#### 为什么它很重要

-   是最早且最高效的生成质数的方法之一
-   为数论算法和密码系统奠定了基础
-   概念简单但数学内涵深刻
-   展示了淘汰法而非暴力破解

#### 一个温和的证明（为什么它有效）

每个合数 $n$ 都有一个最小的质因数 $p \le \sqrt{n}$。
因此，当我们标记到 $\sqrt{n}$ 为止的质数的倍数时，每个合数都会被其最小的质因数划掉。
根据定义，未被标记的数字就是质数。

#### 自己动手试试

1.  对 $n = 50$ 运行筛法并列出质数。
2.  修改代码以计数质数而不是列出它们。
3.  对于大的 $n$，与朴素质数测试比较运行时间。
4.  扩展为分段筛法以处理 $n > 10^7$ 的情况。

#### 测试用例

| 输入 $n$ | 期望的质数列表                     |
| -------- | ---------------------------------- |
| 10       | [2, 3, 5, 7]                       |
| 20       | [2, 3, 5, 7, 11, 13, 17, 19]       |
| 30       | [2, 3, 5, 7, 11, 13, 17, 19, 23, 29] |

#### 复杂度

| 操作     | 时间复杂度         | 空间复杂度 |
| -------- | ------------------ | ---------- |
| 筛法     | $O(n \log \log n)$ | $O(n)$     |

埃拉托斯特尼筛法将寻找质数转化为一种优雅的淘汰模式，简单的循环揭示了数字隐藏的秩序。
### 3 线性步骤跟踪

线性步骤跟踪是一种简单而强大的可视化工具，用于理解算法如何逐行推进。它记录执行的每一步，展示变量如何随时间变化，帮助初学者看清计算的*流程*。

#### 我们正在解决什么问题？

在学习算法时，很容易在每条指令执行后迷失对正在发生的事情的跟踪。
线性步骤跟踪帮助我们*看到*执行过程，一次一步，一次一个更新。

它不仅仅是抽象的推理，而是让我们遵循程序运行期间发生的精确状态变化，这使得调试和推理变得容易得多。

#### 它是如何工作的（通俗解释）

1.  写下你的伪代码或代码。
2.  创建一个表格，列包括步骤编号、当前行和变量值。
3.  每次执行一行时，记录行号和更新后的变量。
4.  继续直到程序结束。

这种方法与算法无关，适用于循环、递归、条件判断和所有流程模式。

#### 逐步示例

让我们跟踪一个简单的循环：

```
sum = 0
for i in 1..4:
    sum = sum + i
```

| 步骤 | 行号 | i  | sum | 备注           |
| ---- | ---- | -- | --- | -------------- |
| 1    | 1    | -  | 0   | 初始化 sum     |
| 2    | 2    | 1  | 0   | 循环开始       |
| 3    | 3    | 1  | 1   | sum = 0 + 1    |
| 4    | 2    | 2  | 1   | 下一次迭代     |
| 5    | 3    | 2  | 3   | sum = 1 + 2    |
| 6    | 2    | 3  | 3   | 下一次迭代     |
| 7    | 3    | 3  | 6   | sum = 3 + 3    |
| 8    | 2    | 4  | 6   | 下一次迭代     |
| 9    | 3    | 4  | 10  | sum = 6 + 4    |
| 10   | -    | -  | 10  | 结束           |

最终结果：$sum = 10$。

#### 微型代码（Python）

```python
sum = 0
trace = []

for i in range(1, 5):
    trace.append((i, sum))
    sum += i

trace.append(("final", sum))
print(trace)
# [(1, 0), (2, 1), (3, 3), (4, 6), ('final', 10)]
```

#### 为什么它很重要

-   培养阅读算法的*逐步理解能力*
-   非常适合教授循环、条件和递归
-   揭示隐藏的假设和逻辑错误
-   是调试和分析的理想工具

#### 一个温和的证明（为什么它有效）

每个算法都可以表示为一连串的状态转换。
如果记录了每次转换，我们就得到了计算的完整跟踪。
因此，可以通过比较预期状态序列与实际状态序列来验证正确性。
这相当于一个归纳证明：每一步都符合规范。

#### 亲自尝试

1.  逐步跟踪一个递归的阶乘函数。
2.  添加一个“调用栈”列来可视化递归深度。
3.  跟踪一个数组排序循环并标记交换操作。
4.  比较优化前后的跟踪记录。

#### 测试用例

| 程序          | 预期最终状态 |
| ------------- | ------------ |
| 1..4 求和     | sum = 10     |
| 1..10 求和    | sum = 55     |
| 阶乘(5)       | result = 120 |

#### 复杂度

| 操作           | 时间   | 空间   |
| -------------- | ------ | ------ |
| 跟踪记录       | $O(n)$ | $O(n)$ |

线性步骤跟踪将不可见的逻辑转化为可见的路径，讲述每一行代码的旅程，一次一个状态。
### 4 算法流程图构建器

算法流程图构建器将抽象的伪代码转化为视觉地图，即控制流图，它展示了决策分支、循环重复以及计算结束的位置。它是代码与理解之间的桥梁。

#### 我们正在解决什么问题？

当算法变得复杂时，很容易迷失其结构。
我们可能知道每一行代码的作用，但不知道*控制流如何在程序中移动*。

流程图明确地展示了这种控制结构，让人一眼就能看清循环、分支、合并和退出点。

#### 它是如何工作的（通俗解释）

1.  识别动作和决策
    *   动作：赋值、计算
    *   决策：if、while、for、switch
2.  用符号表示它们
    *   矩形 → 动作
    *   菱形 → 决策
    *   箭头 → 控制流
3.  根据接下来发生的情况连接节点
4.  为迭代添加回环箭头，并标记退出点

这样就得到了一个控制图，一个你可以从头到尾跟随的形状。

#### 逐步示例

让我们为计算 $1$ 到 $n$ 的数字之和绘制流程图：

伪代码：

```
sum = 0
i = 1
while i ≤ n:
    sum = sum + i
    i = i + 1
print(sum)
```

流程大纲：

1.  开始
2.  初始化 `sum = 0`，`i = 1`
3.  决策：`i ≤ n?`
    *   是 → 更新 `sum`，递增 `i` → 循环返回
    *   否 → 打印 sum → 结束

文本图示：

```
  [开始]
     |
[sum=0, i=1]
     |
  (i ≤ n?) ----否----> [打印 sum] -> [结束]
     |
    是
     |
 [sum = sum + i]
     |
 [i = i + 1]
     |
   (返回 i ≤ n?)
```

#### 微型代码（Python）

```python
def sum_to_n(n):
    sum = 0
    i = 1
    while i <= n:
        sum += i
        i += 1
    return sum
```

可以使用像 `graphviz` 或 `pyflowchart` 这样的库，利用此代码自动生成流程图。

#### 为什么它很重要

-   一目了然地揭示结构
-   通过可视化可能的路径使调试更容易
-   有助于在编码前进行设计
-   通用表示（与语言无关）

#### 一个温和的证明（为什么它有效）

每个算法的执行路径都可以建模为一个有向图：

-   顶点 = 程序状态或动作
-   边 = 转换（下一步）

流程图就是这个控制图的可视化呈现。
它保持了正确性，因为每条边都对应控制流中的一个有效跳转。

#### 自己动手试试

1.  为二分搜索绘制一个流程图。
2.  标记所有可能的比较结果。
3.  为中点更新添加循环返回。
4.  与递归版本进行比较，注意结构差异。

#### 测试用例

| 算法         | 关键决策节点      | 预期路径                 |
| ------------ | ----------------- | ------------------------ |
| 求和循环     | $i \le n$         | 2 （继续，退出）         |
| 二分搜索     | $key == mid?$     | 3 （向左，向右，找到）   |

#### 复杂度

| 操作               | 时间           | 空间           |
| ------------------ | -------------- | -------------- |
| 图构建             | $O(n)$ 个节点  | $O(n)$ 条边    |

算法流程图是一个透镜，它将不可见的执行路径转化为一张你可以从“开始”走到“结束”的地图。
### 5 长除法

长除法是一种用于将一个整数除以另一个整数的逐步算法。它是系统性计算过程的最早示例之一，展示了如何通过一系列局部的、可重复的步骤来解决大型问题。

#### 我们要解决什么问题？

我们想要计算两个整数 $a$（被除数）和 $b$（除数）相除时的商和余数。

简单来说，重复减法需要 $O(a/b)$ 步，对于大数字来说，步数太多。
长除法通过按 10 的幂分组减法，高效地进行逐位计算，从而改进了这一点。

#### 工作原理（通俗解释）

1.  对齐 $a$（被除数）的各位数字。
2.  将 $a$ 的当前部分与 $b$ 进行比较。
3.  找到能放入当前部分的最大 $b$ 的倍数。
4.  减去该倍数，写下商的一位数字，并移下下一位数字。
5.  重复直到所有数字都处理完毕。
6.  写下的数字构成商；剩下的部分就是余数。

此方法自然地扩展到小数除法，只需继续移下零即可。

#### 逐步示例

计算 $153 \div 7$：

| 步骤 | 当前部分           | 商位数字 | 余数 | 操作                                    |
| ---- | ------------------ | -------- | ---- | --------------------------------------- |
| 1    | 15                 | 2        | 1    | $7 \times 2 = 14$, 减去 $15 - 14 = 1$ |
| 2    | 移下 3 → 13        | 1        | 6    | $7 \times 1 = 7$, 减去 $13 - 7 = 6$   |
| 3    | 没有更多数字       |,        | 6    | 完成                                      |

结果：
商 $= 21$, 余数 $= 6$
验证：$7 \times 21 + 6 = 153$

#### 微型代码（Python）

```python
def long_division(a, b):
    quotient = 0
    remainder = 0
    for digit in str(a):
        remainder = remainder * 10 + int(digit)
        q = remainder // b
        remainder = remainder % b
        quotient = quotient * 10 + q
    return quotient, remainder

print(long_division(153, 7))  # (21, 6)
```

#### 为什么它重要

-   引入了循环不变式和逐位推理
-   是任意精度算术中除法的基础
-   是 CPU 和大整数库中实现除法的核心
-   展示了如何将大任务分解为简单的局部操作

#### 一个温和的证明（为什么它有效）

在每一步：

-   当前余数 $r_i$ 满足 $0 \le r_i < b$。
-   算法保持以下不变式：
  $$
  a = b \times Q_i + r_i
  $$
  其中 $Q_i$ 是到目前为止的部分商。
-   每一步都减少了 $a$ 的未处理部分，
  确保算法终止并得到正确的 $Q$ 和 $r$。

#### 亲自尝试

1.  手动计算 $2345 \div 13$。
2.  使用 Python 的 `divmod(2345, 13)` 进行验证。
3.  扩展你的代码以生成小数展开。
4.  将逐位跟踪与手动过程进行比较。

#### 测试用例

| 被除数 $a$ | 除数 $b$ | 预期输出 $(Q, R)$ |
| ---------- | -------- | ----------------- |
| 153        | 7        | (21, 6)           |
| 100        | 8        | (12, 4)           |
| 99         | 9        | (11, 0)           |
| 23         | 5        | (4, 3)            |

#### 复杂度

| 操作         | 时间   | 空间  |
| ------------ | ------ | ----- |
| 长除法       | $O(d)$ | $O(1)$ |

其中 $d$ 是 $a$ 的位数。

长除法不仅仅是算术，它是第一次接触算法思维：状态、迭代以及正确性如何逐位展开。
### 6 模加法

模加法是在钟表上进行的算术运算，我们将数字相加，然后在达到固定上限时回绕。这是模算术最简单的例子，模算术是密码学、哈希和循环数据结构的基础。

#### 我们要解决什么问题？

我们希望将两个整数 $a$ 和 $b$ 相加，但保持结果在一个固定的模数 $m$ 之内。
这意味着我们计算和除以 $m$ 后的余数。

形式上，我们想要：

$$
(a + b) \bmod m
$$

这确保了无论 $a$ 或 $b$ 变得多大，结果始终在范围 $[0, m - 1]$ 内。

#### 工作原理（通俗解释）

1.  计算和 $s = a + b$。
2.  将 $s$ 除以 $m$ 求余数。
3.  该余数就是模和。

如果 $s \ge m$，我们通过减去 $m$ 来"回绕"，直到结果符合模范围。

这个想法类似于钟表上的小时：
在 $12$ 小时制钟表上，$10 + 5$ 小时 → $3$。

#### 逐步示例

设 $a = 10$, $b = 7$, $m = 12$。

1.  计算 $s = 10 + 7 = 17$。
2.  $17 \bmod 12 = 5$。
3.  所以 $(10 + 7) \bmod 12 = 5$。

验证：$17 - 12 = 5$，符合 $[0, 11]$ 范围。

#### 微型代码（Python）

```python
def mod_add(a, b, m):
    return (a + b) % m

print(mod_add(10, 7, 12))  # 5
```

#### 为什么它重要

-   模算术的基础
-   用于哈希、循环缓冲区和数论
-   对安全加密（RSA、ECC）至关重要
-   展示了有界系统中的回绕逻辑

#### 一个温和的证明（为什么它有效）

根据模数的定义：

$$
x \bmod m = r \quad \text{使得 } x = q \times m + r,\ 0 \le r < m
$$

因此，对于 $a + b = q \times m + r$，
我们有 $(a + b) \bmod m = r$。
所有等价的和相差 $m$ 的倍数，
所以模加法保持同余关系：

$$
(a + b) \bmod m \equiv (a \bmod m + b \bmod m) \bmod m
$$

#### 亲自尝试

1.  计算 $(15 + 8) \bmod 10$。
2.  验证 $(a + b) \bmod m = ((a \bmod m) + (b \bmod m)) \bmod m$。
3.  用负值测试：$(−3 + 5) \bmod 7$。
4.  应用于时间算术：在 $12$ 小时制钟表上，$11 + 5$ 是多少？

#### 测试用例

| $a$ | $b$ | $m$ | 结果 |
| --- | --- | --- | ---- |
| 10  | 7   | 12  | 5    |
| 5   | 5   | 10  | 0    |
| 8   | 15  | 10  | 3    |
| 11  | 5   | 12  | 4    |

#### 复杂度

| 操作         | 时间复杂度 | 空间复杂度 |
| ------------ | ---------- | ---------- |
| 模加法       | $O(1)$     | $O(1)$     |

模加法教授了模算术的节奏，每一次相加都回绕回和谐，始终保持在它有限的世界里。
### 7 进制转换

进制转换是一种将数字用不同数制系统表示的算法过程。它让我们能够在十进制、二进制、八进制、十六进制或任何进制之间进行转换，这些进制是计算机和数学共同的语言。

#### 我们要解决什么问题？

我们希望用基数 $b$ 来表示一个整数 $n$。
在十进制中，数字从 0 到 9。
在二进制中，只有 0 和 1。
在十六进制中，数字是 $0 \ldots 9$ 和 $A \ldots F$。

目标是找到一个数字序列 $d_k d_{k-1} \ldots d_0$，使得：

$$
n = \sum_{i=0}^{k} d_i \cdot b^i
$$

其中 $0 \le d_i < b$。

#### 它是如何工作的（通俗解释）

1.  从整数 $n$ 开始。
2.  重复地将 $n$ 除以 $b$。
3.  每次记录余数（这些余数就是数字）。
4.  当 $n = 0$ 时停止。
5.  $b$ 进制的表示就是按相反顺序读取这些余数。

这种方法之所以有效，是因为除法从最低有效位开始提取数字。

#### 逐步示例

将 $45$ 转换为二进制（$b = 2$）：

| 步骤 | $n$ | $n \div 2$ | 余数 |
| ---- | --- | ---------- | ---- |
| 1    | 45  | 22         | 1    |
| 2    | 22  | 11         | 0    |
| 3    | 11  | 5          | 1    |
| 4    | 5   | 2          | 1    |
| 5    | 2   | 1          | 0    |
| 6    | 1   | 0          | 1    |

向上读取余数：101101

所以 $45_{10} = 101101_2$。

验证：$1 \cdot 2^5 + 0 \cdot 2^4 + 1 \cdot 2^3 + 1 \cdot 2^2 + 0 \cdot 2^1 + 1 \cdot 2^0 = 32 + 0 + 8 + 4 + 0 + 1 = 45$ ✅

#### 微型代码（Python）

```python
def to_base(n, b):
    digits = []
    while n > 0:
        digits.append(n % b)
        n //= b
    return digits[::-1] or [0]

print(to_base(45, 2))  # [1, 0, 1, 1, 0, 1]
```

#### 为什么它很重要

-   在人类和机器表示之间转换数字
-   编码、压缩和密码学的核心
-   建立对位置数制系统的直观理解
-   用于解析、序列化和数字电路

#### 一个温和的证明（为什么它有效）

每次除法步骤产生一个数字 $r_i = n_i \bmod b$。
我们有：

$$
n_i = b \cdot n_{i+1} + r_i
$$

展开递推关系得到：

$$
n = \sum_{i=0}^{k} r_i b^i
$$

因此，按相反顺序收集余数就能精确地重建 $n$。

#### 亲自尝试

1.  将 $100_{10}$ 转换为八进制。
2.  将 $255_{10}$ 转换为十六进制。
3.  通过 $\sum d_i b^i$ 重新组合数字来验证。
4.  编写一个反向转换器：从 $b$ 进制到十进制。

#### 测试用例

| 十进制 $n$ | 基数 $b$ | 表示形式 |
| ---------- | -------- | -------- |
| 45         | 2        | 101101   |
| 100        | 8        | 144      |
| 255        | 16       | FF       |
| 31         | 5        | 111      |

#### 复杂度

| 操作         | 时间          | 空间         |
| ------------ | ------------- | ------------ |
| 进制转换     | $O(\log_b n)$ | $O(\log_b n)$ |

进制转换是算术的叙事，一层层剥离余数，直到只剩下数字，通过不同的视角揭示同一个数字。
### 8 阶乘计算

阶乘计算是一种将连续整数序列相乘的算法操作，这是一个简单但会爆炸性增长的规则。它是组合数学、概率论和数学分析的基础。

#### 我们要解决什么问题？

我们希望计算一个非负整数 $n$ 的阶乘，记作 $n!$，其定义为：

$$
n! = n \times (n - 1) \times (n - 2) \times \cdots \times 1
$$

其基本情况为：

$$
0! = 1
$$

阶乘计算的是排列 $n$ 个不同对象的方法数，是排列与组合的基础构件。

#### 工作原理（通俗解释）

主要有两种方法：

迭代法：

- 从 `result = 1` 开始
- 将 `result` 乘以从 1 到 $n$ 的每个 $i$
- 返回结果

递归法：

- $n! = n \times (n - 1)!$
- 当 $n = 0$ 时停止

两种方法产生相同的结果；递归法反映了数学定义，迭代法则避免了函数调用开销。

#### 逐步示例

计算 $5!$：

| 步骤 | $n$ | 乘积 |
| ---- | --- | ------- |
| 1    | 1   | 1       |
| 2    | 2   | 2       |
| 3    | 3   | 6       |
| 4    | 4   | 24      |
| 5    | 5   | 120     |

所以 $5! = 120$ ✅

#### 微型代码（Python）

迭代版本

```python
def factorial_iter(n):
    result = 1
    for i in range(1, n + 1):
        result *= i
    return result

print(factorial_iter(5))  # 120
```

递归版本

```python
def factorial_rec(n):
    if n == 0:
        return 1
    return n * factorial_rec(n - 1)

print(factorial_rec(5))  # 120
```

#### 为何重要

- 是组合数学、微积分和概率论中的核心运算
- 展示了递归、迭代和归纳法
- 增长迅速，可用于测试溢出和渐近行为
- 出现在二项式系数、泰勒级数和排列中

#### 一个温和的证明（为何有效）

根据定义，$n! = n \times (n - 1)!$。
假设 $(n - 1)!$ 被正确计算。那么乘以 $n$ 就得到 $n!$。

通过归纳法：

- 基本情况：$0! = 1$
- 归纳步骤：如果 $(n - 1)!$ 正确，则 $n!$ 也正确

因此，递归定义和迭代定义是等价且正确的。

#### 动手尝试

1.  分别用迭代法和递归法计算 $6!$。
2.  打印中间乘积以追踪增长过程。
3.  比较 $n = 1000$ 时两种方法的运行时间。
4.  探索非整数情况下的浮点阶乘（`math.gamma`）。

#### 测试用例

| 输入 $n$ | 期望输出 $n!$ |
| --------- | -------------------- |
| 0         | 1                    |
| 1         | 1                    |
| 3         | 6                    |
| 5         | 120                  |
| 6         | 720                  |

#### 复杂度

| 操作       | 时间复杂度 | 空间复杂度        |
| ---------- | ---------- | ----------------- |
| 迭代法     | $O(n)$     | $O(1)$            |
| 递归法     | $O(n)$     | $O(n)$（调用栈）  |

阶乘计算是简单与无限的相遇，是一个单一的规则，以优雅的必然性从 1 扩展到天文数字。
### 9 迭代过程跟踪器

迭代过程跟踪器是一种诊断算法，它跟踪循环的每一次迭代，记录变量状态、条件和更新。它有助于可视化程序内部状态的演变过程，将循环逻辑转化为清晰的时间线。

#### 我们要解决什么问题？

在编写迭代算法时，很容易忽视每一步发生了什么。
变量是否正确更新？循环条件是否按预期执行？
跟踪器逐步捕获这个过程，以便我们验证正确性、发现错误，并清晰地教授迭代。

#### 工作原理（通俗解释）

1.  识别循环（`for` 或 `while`）。
2.  在每次迭代之前或之后，记录：
    *   迭代次数
    *   关键变量的值
    *   条件评估结果
3.  将这些快照存储在跟踪表中。
4.  执行后，查看值如何随时间演变。

可以把它想象成一个"执行日记"，每次迭代都有一篇日志条目。

#### 逐步示例

让我们跟踪一个简单的累加过程：

```
sum = 0
for i in 1..5:
    sum = sum + i
```

| 步骤 | $i$ | $sum$ | 描述           |
| ---- | --- | ----- | -------------- |
| 1    | 1   | 1     | 加第一个数     |
| 2    | 2   | 3     | 加第二个数     |
| 3    | 3   | 6     | 加第三个数     |
| 4    | 4   | 10    | 加第四个数     |
| 5    | 5   | 15    | 加第五个数     |

最终结果：$sum = 15$

#### 微型代码（Python）

```python
def trace_sum(n):
    sum = 0
    trace = []
    for i in range(1, n + 1):
        sum += i
        trace.append((i, sum))
    return trace

print(trace_sum(5))
# [(1, 1), (2, 3), (3, 6), (4, 10), (5, 15)]
```

#### 为何重要

-   将隐藏的状态变化转化为可见的数据
-   非常适合调试循环和验证不变量
-   支持算法教学和逐步推理
-   在性能分析、日志记录和单元测试中很有用

#### 一个温和的证明（为何有效）

迭代算法是一系列确定性的状态转换：

$$
S_{i+1} = f(S_i)
$$

在每次迭代时记录 $S_i$，就得到了执行的完整轨迹。
跟踪表捕获了所有中间状态，确保了可重现性和清晰性，这是一种操作性的证明。

#### 动手试试

1.  跟踪乘法循环中的变量更新。
2.  添加条件检查（例如提前退出）。
3.  记录更新前和更新后的状态。
4.  比较迭代版本和递归版本的跟踪结果。

#### 测试用例

| 输入 $n$ | 预期跟踪结果                           |
| --------- | -------------------------------------- |
| 3         | [(1, 1), (2, 3), (3, 6)]               |
| 4         | [(1, 1), (2, 3), (3, 6), (4, 10)]      |
| 5         | [(1, 1), (2, 3), (3, 6), (4, 10), (5, 15)] |

#### 复杂度

| 操作     | 时间复杂度 | 空间复杂度 |
| -------- | ---------- | ---------- |
| 跟踪     | $O(n)$     | $O(n)$     |

迭代过程跟踪器使思考过程变得可见，将一个循环的内部节奏逐步展开，直到最终结果显现。
### 10 汉诺塔

汉诺塔是一个传奇的递归谜题，它完美地展示了如何通过简单的重复结构来解决复杂问题。这是*分而治之*思想最纯粹形式的永恒范例。

#### 我们要解决什么问题？

我们想将 $n$ 个圆盘从源柱移动到目标柱，使用一个辅助柱。
规则：

1.  一次只能移动一个圆盘。
2.  永远不要将较大的圆盘放在较小的圆盘之上。

挑战在于找到实现此目标的最少移动序列。

#### 工作原理（通俗解释）

关键见解：
要移动 $n$ 个圆盘，首先将 $n-1$ 个圆盘移到一边，移动最大的那个，然后再将较小的圆盘移回来。

步骤：

1.  将 $n-1$ 个圆盘从源柱 → 辅助柱
2.  将最大的圆盘从源柱 → 目标柱
3.  将 $n-1$ 个圆盘从辅助柱 → 目标柱

这种递归结构会重复，直到最小的圆盘被直接移动。

#### 逐步示例

对于 $n = 3$，柱子：A（源柱），B（辅助柱），C（目标柱）

| 步骤 | 移动     |
| ---- | -------- |
| 1    | A → C    |
| 2    | A → B    |
| 3    | C → B    |
| 4    | A → C    |
| 5    | B → A    |
| 6    | B → C    |
| 7    | A → C    |

总移动次数：$2^3 - 1 = 7$

#### 微型代码（Python）

```python
def hanoi(n, source, target, aux):
    if n == 1:
        print(f"{source} → {target}")
        return
    hanoi(n - 1, source, aux, target)
    print(f"{source} → {target}")
    hanoi(n - 1, aux, target, source)

hanoi(3, 'A', 'C', 'B')
```

#### 为什么它很重要

-   经典的递归模式：分解 → 解决 → 合并
-   展示了指数级增长（$2^n - 1$ 次移动）
-   训练递归推理和栈可视化能力
-   出现在算法分析、递归树和组合数学中

#### 一个温和的证明（为什么它有效）

令 $T(n)$ 表示移动 $n$ 个圆盘所需的移动次数。
我们必须移动 $n-1$ 个圆盘两次，并移动最大的圆盘一次：

$$
T(n) = 2T(n-1) + 1, \quad T(1) = 1
$$

求解递推关系：

$$
T(n) = 2^n - 1
$$

每个递归步骤都遵循规则并减小问题规模，通过结构归纳法确保了正确性。

#### 亲自尝试

1.  手动追踪 $n = 2$ 和 $n = 3$ 的情况。
2.  计算递归调用次数。
3.  修改代码，将移动记录到列表中。
4.  扩展功能，在每次移动后显示柱子的状态。

#### 测试用例

| $n$ | 预期移动次数 |
| --- | ------------ |
| 1   | 1            |
| 2   | 3            |
| 3   | 7            |
| 4   | 15           |

#### 复杂度

| 操作       | 时间复杂度 | 空间复杂度              |
| ---------- | ---------- | ----------------------- |
| 移动次数   | $O(2^n)$   | $O(n)$ （递归栈）       |

汉诺塔将递归变成了艺术，每一次移动都由对称性引导，每一步都揭示了简单如何一次一个圆盘地构建出复杂性。

## 第 2 节. 度量时间和空间
### 11 操作计数

操作计数是理解时间复杂度的第一步。它是一种将代码转化为数学的艺术，通过衡量算法执行了多少*基本步骤*，帮助我们在运行前预测性能。

#### 我们要解决什么问题？

我们想要估算一个算法需要多长时间，不是通过时钟时间，而是通过它执行了多少基本操作。
我们不依赖硬件速度，而是计算抽象的步骤、比较、赋值、加法，每个都被视为一个工作单元。

这将算法转化为可分析的公式。

#### 工作原理（通俗解释）

1.  确定单位步骤（如一次比较或一次加法）。
2.  将算法分解成行或循环。
3.  计算每个操作的重复次数。
4.  对所有计数求和，得到总步骤函数 $T(n)$。
5.  简化为主导项，用于渐近分析。

我们不是在测量*秒数*，我们是在测量*结构*。

#### 逐步示例

计算以下代码的操作次数：

```python
sum = 0
for i in range(1, n + 1):
    sum += i
```

分解：

| 行号 | 操作             | 计数     |
| ---- | ---------------- | -------- |
| 1    | 初始化           | 1        |
| 2    | 循环条件比较     | $n + 1$  |
| 3    | 加法 + 赋值      | $n$      |

总计：
$$
T(n) = 1 + (n + 1) + n = 2n + 2
$$

渐近地：
$$
T(n) = O(n)
$$

#### 微型代码（Python）

```python
def count_sum_ops(n):
    ops = 0
    ops += 1  # 初始化 sum
    for i in range(1, n + 1):
        ops += 1  # 循环检查
        ops += 1  # sum += i
    ops += 1  # 最终循环检查
    return ops
```

测试：`count_sum_ops(5)` → `13`

#### 为什么这很重要

-   建立对算法增长的直觉
-   揭示隐藏成本（嵌套循环、递归）
-   为 Big-O 和运行时间证明奠定基础
-   与语言无关：适用于任何伪代码

#### 一个温和的证明（为什么它有效）

每个程序都可以建模为一个由输入规模 $n$ 参数化的有限操作序列。
如果 $f(n)$ 精确地计数这些操作，那么对于大的 $n$，增长率 $\Theta(f(n))$ 与实际性能在常数因子范围内匹配。
因此，操作计数可以预测渐近运行时间行为。

#### 亲自尝试

1.  计算嵌套循环中的操作次数：

    ```python
    for i in range(n):
        for j in range(n):
            x += 1
    ```
2.  推导出 $T(n) = n^2 + 2n + 1$。
3.  简化为 $O(n^2)$。
4.  比较迭代计数与递归计数。

#### 测试用例

| 算法类型     | 步骤函数        | Big-O    |
| ------------ | --------------- | -------- |
| 线性循环     | $2n + 2$        | $O(n)$   |
| 嵌套循环     | $n^2 + 2n + 1$  | $O(n^2)$ |
| 常量工作     | $c$             | $O(1)$   |

#### 复杂度

| 操作         | 时间              | 空间  |
| ------------ | ----------------- | ----- |
| 计数步骤     | $O(1)$ (分析阶段) | $O(1)$ |

操作计数将代码转化为数学，这是一台显微镜，用于理解循环、分支和递归如何随输入规模变化。
### 12 循环分析

循环分析是揭示算法增长规律的关键，它告诉我们循环运行多少次，从而得知执行了多少次操作。每当你看到一个循环，你其实是在看一个伪装起来的公式。

#### 我们要解决什么问题？

我们想要确定循环执行的迭代次数，并将其表示为输入规模 $n$ 的函数。
这有助于我们在实际测量之前估算总运行时间。

无论循环是线性的、嵌套的、对数的还是混合的，理解其迭代次数都能揭示算法的真实复杂度。

#### 工作原理（通俗解释）

1.  识别循环变量（例如 `for i in range(...)` 中的 `i`）。
2.  找到它的更新规则，是加法（`i += 1`）还是乘法（`i *= 2`）。
3.  求解循环条件为真的次数。
4.  如果是嵌套循环，则乘以内层循环的工作量。
5.  将所有独立循环的贡献相加。

这会将循环转化为你可以推理的代数表达式。

#### 逐步示例

示例 1：线性循环

```python
for i in range(1, n + 1):
    work()
```

$i$ 从 $1$ 到 $n$，每次增加 $1$。
迭代次数：$n$
工作量：$O(n)$

示例 2：对数循环

```python
i = 1
while i <= n:
    work()
    i *= 2
```

$i$ 每一步翻倍：$1, 2, 4, 8, \dots, n$
迭代次数：$\log_2 n + 1$
工作量：$O(\log n)$

示例 3：嵌套循环

```python
for i in range(n):
    for j in range(n):
        work()
```

外层循环：$n$
内层循环：$n$
总工作量：$n \times n = n^2$

#### 微型代码（Python）

```python
def linear_loop(n):
    count = 0
    for i in range(n):
        count += 1
    return count  # n

def log_loop(n):
    count = 0
    i = 1
    while i <= n:
        count += 1
        i *= 2
    return count  # ≈ log2(n)
```

#### 为什么这很重要

-   揭示隐藏在循环内部的复杂度
-   推导 $O(n)$、$O(\log n)$ 和 $O(n^2)$ 的核心工具
-   使渐进行为可预测和可度量
-   适用于 for 循环、while 循环和嵌套结构

#### 一个温和的证明（为什么它有效）

每次循环迭代都对应其守卫条件为真。
如果循环变量 $i$ 单调变化（通过加法或乘法），
那么总迭代次数就是满足退出条件的最小 $k$。

对于加法更新：
$$
i_0 + k \cdot \Delta \ge n \implies k = \frac{n - i_0}{\Delta}
$$

对于乘法更新：
$$
i_0 \cdot r^k \ge n \implies k = \log_r \frac{n}{i_0}
$$

#### 动手试一试

1.  分析循环：

    ```python
    i = n
    while i > 0:
        i //= 2
    ```

    → $O(\log n)$
2.  分析双重循环：

    ```python
    for i in range(n):
        for j in range(i):
            work()
    ```

    → $\frac{n(n-1)}{2} = O(n^2)$
3.  结合加法循环和乘法循环进行分析。

#### 测试用例

| 代码模式                            | 迭代次数           | 复杂度      |
| ----------------------------------- | ------------------ | ----------- |
| `for i in range(n)`                 | $n$                | $O(n)$      |
| `while i < n: i *= 2`               | $\log_2 n$         | $O(\log n)$ |
| `for i in range(n): for j in range(n)` | $n^2$              | $O(n^2)$    |
| `for i in range(n): for j in range(i)` | $\frac{n(n-1)}{2}$ | $O(n^2)$    |

#### 复杂度

| 操作         | 时间                | 空间    |
| ------------ | ------------------- | ------- |
| 循环分析     | $O(1)$（每个循环）  | $O(1)$  |

循环分析将重复转化为算术，每次迭代变成一个项，每个循环都成为用增长语言讲述的一个故事。
### 13 递归展开

递归展开是我们*展开*递归定义以揭示其真实成本的方法。许多递归算法（如归并排序或快速排序）根据自身更小规模的副本定义运行时间。通过展开递归式，我们可以逐步揭示总工作量。

#### 我们要解决什么问题？

递归算法通常将其运行时间表示为：

$$
T(n) = a \cdot T!\left(\frac{n}{b}\right) + f(n)
$$

其中：

- $a$ = 递归调用的次数
- $b$ = 输入规模减小的因子
- $f(n)$ = 递归之外完成的工作（分割、合并等）

我们希望通过展开这个关系式直到基准情形来估算 $T(n)$。

#### 工作原理（通俗解释）

将递归展开想象成剥洋葱。
每一层递归都会贡献一些成本，我们将所有层相加直到基准情形。

步骤：

1.  写出递归式。
2.  展开一层：用其公式替换 $T(\cdot)$。
3.  重复直到参数变为基准情形。
4.  将每一层完成的工作求和。
5.  简化求和式以获得渐近形式。

#### 逐步示例

以归并排序为例：

$$
T(n) = 2T!\left(\frac{n}{2}\right) + n
$$

展开：

- 第 0 层：$T(n) = 2T(n/2) + n$
- 第 1 层：$T(n/2) = 2T(n/4) + n/2$ → 代入
  $T(n) = 4T(n/4) + 2n$
- 第 2 层：$T(n) = 8T(n/8) + 3n$
- …
- 第 $\log_2 n$ 层：$T(1) = c$

对各层工作求和：

$$
T(n) = n \log_2 n + n = O(n \log n)
$$

#### 微型代码（Python）

```python
def recurrence_expand(a, b, f, n, base=1):
    level = 0
    total = 0
    size = n
    while size >= base:
        cost = (a  level) * f(size)
        total += cost
        size //= b
        level += 1
    return total
```

对于归并排序，使用 `f = lambda x: x`。

#### 为何重要

- 分析递归算法的核心工具
- 在应用主定理之前建立直觉
- 将抽象的递归式转化为可感知的模式
- 有助于可视化每个递归层的总工作量

#### 一个温和的证明（为何有效）

在第 $i$ 层：

- 有 $a^i$ 个子问题。
- 每个子问题的规模为 $\frac{n}{b^i}$。
- 每层工作量：$a^i \cdot f!\left(\frac{n}{b^i}\right)$

总成本：

$$
T(n) = \sum_{i=0}^{\log_b n} a^i f!\left(\frac{n}{b^i}\right)
$$

根据 $f(n)$ 与 $n^{\log_b a}$ 的比较关系，顶层、底层或中间层将主导总成本。

#### 动手试试

1.  展开 $T(n) = 3T(n/2) + n^2$。
2.  展开 $T(n) = T(n/2) + 1$。
3.  可视化每层的总工作量。
4.  用主定理检查你的结果。

#### 测试用例

| 递归式               | 展开结果       | 复杂度          |
| -------------------- | -------------- | --------------- |
| $T(n) = 2T(n/2) + n$ | $n \log n$     | $O(n \log n)$   |
| $T(n) = T(n/2) + 1$  | $\log n$       | $O(\log n)$     |
| $T(n) = 4T(n/2) + n$ | $n^2$          | $O(n^2)$        |

#### 复杂度

| 操作       | 时间复杂度         | 空间复杂度             |
| ---------- | ------------------ | ---------------------- |
| 展开       | $O(\log n)$ 层     | $O(\log n)$ 树深度     |

递归展开将递归转化为节奏，每一层添加自己的诗节，求和揭示了算法增长的旋律。
### 14 摊还分析

摊还分析着眼于超越单个操作的最坏情况，以捕捉长序列中*每次操作的平均成本*。它告诉我们“昂贵”的操作何时会相互抵消，揭示出比初看起来更快的算法。

#### 我们要解决什么问题？

有些操作偶尔需要很长时间（比如调整数组大小），但大多数操作是廉价的。朴素的最坏情况分析会夸大总成本。摊还分析则能找出一个序列中真实的平均成本。

我们不是对*不同输入*求平均，而是对*单次运行中的操作*求平均。

#### 工作原理（通俗解释）

假设一个操作通常是 $O(1)$，但有时是 $O(n)$。如果这种昂贵的情况发生得足够少，那么*每次操作的平均成本*仍然很小。

主要有三种方法：

1.  聚合方法：总成本 ÷ 操作次数
2.  记账方法：对廉价操作额外收费，为昂贵操作储蓄信用
3.  势能方法：定义势能（存储的工作量）并跟踪其变化

#### 逐步示例

动态数组大小调整

当数组已满时，将其大小加倍并复制元素。

| 操作         | 成本 | 注释                 |
| ------------ | ---- | -------------------- |
| 插入 #1–#1   | 1    | 直接插入             |
| 插入 #2      | 2    | 调整大小为 2，复制 1 |
| 插入 #3      | 3    | 调整大小为 4，复制 2 |
| 插入 #5      | 5    | 调整大小为 8，复制 4 |
| ...          | ...  | ...                  |

$n$ 次插入后的总成本 ≈ $2n$
平均成本 = $2n / n = O(1)$
因此，每次插入的摊还成本是 $O(1)$，而不是 $O(n)$。

#### 微型代码（Python）

```python
def dynamic_array(n):
    arr = []
    capacity = 1
    cost = 0
    for i in range(n):
        if len(arr) == capacity:
            capacity *= 2
            cost += len(arr)  # 复制成本
        arr.append(i)
        cost += 1  # 插入成本
    return cost, cost / n  # 总成本，摊还平均值
```

尝试 `dynamic_array(10)` → 大致总成本 ≈ 20，平均成本 ≈ 2。

#### 为何重要

-   展示算法在序列上的平均效率
-   是分析栈、队列、哈希表和动态数组的关键
-   解释了为什么“偶尔昂贵”的操作整体上仍然是高效的
-   区分了感知（最坏情况）与现实（聚合行为）

#### 一个温和的证明（为何有效）

令 $C_i$ = 第 $i$ 次操作的成本，
$n$ = 总操作次数。

聚合方法：

$$
\text{摊还成本} = \frac{\sum_{i=1}^n C_i}{n}
$$

如果 $\sum C_i = O(n)$，则每次操作的平均成本 = $O(1)$。

势能方法：

定义势能 $\Phi_i$ 代表已存储的工作量。
摊还成本 = $C_i + \Phi_i - \Phi_{i-1}$
对所有操作求和，势能项会相互抵消，
使得总成本受限于初始势能与最终势能之和。

#### 亲自尝试

1.  分析偶尔需要完全弹出的栈的摊还成本。
2.  使用记账方法为插入操作分配“信用”。
3.  展示带大小调整的哈希表中 $O(1)$ 摊还插入。
4.  比较摊还时间与最坏情况时间。

#### 测试用例

| 操作类型                   | 最坏情况  | 摊还成本       |
| -------------------------- | --------- | -------------- |
| 数组插入（加倍策略）       | $O(n)$    | $O(1)$         |
| 栈压入                     | $O(1)$    | $O(1)$         |
| 队列出队（双栈实现）       | $O(n)$    | $O(1)$         |
| 并查集（路径压缩）         | $O(\log n)$ | $O(\alpha(n))$ |

#### 复杂度

| 分析类型   | 公式                           | 目标         |
| ---------- | ------------------------------ | ------------ |
| 聚合方法   | $\frac{\text{总成本}}{n}$      | 简洁性       |
| 记账方法   | 分配信用                       | 直观性       |
| 势能方法   | $\Delta \Phi$                  | 形式严谨性   |

摊还分析揭示了混乱之下的平静——
几场风暴不能定义天气，一个 $O(n)$ 的时刻也不会破坏 $O(1)$ 的和谐。
### 15 空间计数

空间计数是操作计数在空间上的孪生概念，我们不再测量时间，而是测量算法消耗了多少*内存*。每个变量、数组、栈帧或临时缓冲区都会增加其足迹。理解它有助于我们编写适应内存并能优雅扩展的程序。

#### 我们要解决什么问题？

我们想要估算算法的空间复杂度——即随着输入规模 $n$ 的增长，它需要多少内存。

这包括：

- 静态空间（固定变量）
- 动态空间（数组、递归、数据结构）
- 辅助空间（超出输入之外的额外工作内存）

我们的目标：将总内存表示为函数 $S(n)$。

#### 工作原理（通俗解释）

1.  计算基本变量（常量、计数器、指针）。
    → 常数空间 $O(1)$
2.  加上数据结构的大小（数组、列表、矩阵）。
    → 通常与 $n$、$n^2$ 等成正比
3.  如果适用，加上递归栈深度。
4.  对于渐近空间分析，忽略常数，关注增长趋势。

最终，
$$
S(n) = S_{\text{静态}} + S_{\text{动态}} + S_{\text{递归}}
$$

#### 逐步示例

示例 1：线性数组

```python
arr = [0] * n
```

- $n$ 个整数 → $O(n)$ 空间

示例 2：二维矩阵

```python
matrix = [[0] * n for _ in range(n)]
```

- $n \times n$ 个元素 → $O(n^2)$ 空间

示例 3：递归阶乘

```python
def fact(n):
    if n == 0:
        return 1
    return n * fact(n - 1)
```

- 深度 = $n$ → 栈空间 = $O(n)$
- 无额外数据结构 → $S(n) = O(n)$

#### 微型代码（Python）

```python
def space_counter(n):
    const = 1             # O(1)
    arr = [0] * n         # O(n)
    matrix = [[0]*n for _ in range(n)]  # O(n^2)
    return const + len(arr) + len(matrix)
```

这个简单的例子说明了空间贡献的累加性。

#### 为何重要

- 在大型系统中，内存是首要约束条件
- 对于嵌入式、流式和实时算法至关重要
- 揭示了时间与空间之间的权衡
- 指导原地（in-place）与非原地（out-of-place）解决方案的设计

#### 一个温和的证明（为何有效）

每个算法操作一个有限的数据元素集合。
如果 $s_i$ 是为结构 $i$ 分配的空间，
则总空间为：

$$
S(n) = \sum_i s_i(n)
$$

渐近空间由最大的项主导，
因此 $S(n) = \Theta(\max_i s_i(n))$。

这确保了我们的分析能随数据增长而扩展。

#### 动手试试

1.  计算归并排序的空间（临时数组）。
2.  与快速排序（原地）进行比较。
3.  显式地加上递归成本。
4.  分析动态规划的时间-空间权衡。

#### 测试用例

| 算法         | 空间          | 原因                     |
| ------------ | ------------- | ------------------------ |
| 线性搜索     | $O(1)$        | 常数额外内存             |
| 归并排序     | $O(n)$        | 用于合并的额外数组       |
| 快速排序     | $O(\log n)$   | 栈深度                   |
| 动态规划表（2D） | $O(n^2)$      | 完整的状态网格           |

#### 复杂度

| 组件           | 示例           | 成本       |
| -------------- | -------------- | ---------- |
| 变量           | $a, b, c$      | $O(1)$     |
| 数组           | `arr[n]`       | $O(n)$     |
| 矩阵           | `matrix[n][n]` | $O(n^2)$   |
| 递归栈         | 深度 $n$       | $O(n)$     |

空间计数将内存转化为可测量的量，每个变量都是一个足迹，每个结构都是一个表面，每个栈帧都是算法架构中的一层。
### 16 内存占用估算器

内存占用估算器用于计算算法或数据结构实际消耗的内存，不仅仅是渐近意义上的，而是以*真实字节数*来衡量。它弥合了理论空间复杂度与实际实现之间的差距。

#### 我们正在解决什么问题？

当在接近内存限制的情况下工作时，仅仅知道一个算法的空间复杂度是 $O(n)$ 是不够的。
我们需要实际的估算：每个元素占用多少字节，总分配量是多少，以及存在哪些开销。

内存占用估算器将理论计数转换为现实世界扩展的定量估算。

#### 工作原理（通俗解释）

1.  识别使用的数据类型：`int`、`float`、`pointer`、`struct` 等。
2.  估算每个元素的大小（取决于编程语言，例如 `int = 4 字节`）。
3.  乘以数量以找到总内存使用量。
4.  包含以下开销：
    *   对象头或元数据
    *   填充或对齐
    *   指针或引用

最终占用空间：
$$
\text{内存} = \sum_i (\text{数量}_i \times \text{大小}_i) + \text{开销}
$$

#### 逐步示例

假设我们在 Python 中有一个包含 $n = 1{,}000{,}000$ 个整数的列表。

| 组件           | 大小（字节） | 数量      | 总计        |
| -------------- | ------------ | --------- | ----------- |
| 列表对象       | 64           | 1         | 64          |
| 指针           | 8            | 1,000,000 | 8,000,000   |
| 整数对象       | 28           | 1,000,000 | 28,000,000  |

总计 ≈ 36 MB（加上解释器开销）。

如果使用固定的 `array('i')`（C 风格整数）：
$4 \text{ 字节} \times 10^6 = 4$ MB，内存效率要高得多。

#### 微型代码（Python）

```python
import sys

n = 1_000_000
arr_list = list(range(n))
arr_array = bytearray(n * 4)

print(sys.getsizeof(arr_list))   # 列表对象
print(sys.getsizeof(arr_array))  # 原始字节数组
```

使用 `sys.getsizeof()` 比较内存成本。

#### 为何重要

*   揭示真实的内存需求
*   对于大型数据集、嵌入式系统和数据库至关重要
*   解释具有对象开销的语言中的性能权衡
*   支持系统设计和容量规划

#### 一个温和的证明（为何有效）

每个变量或元素根据其类型消耗固定数量的字节。
如果分配了 $n_i$ 个类型为 $t_i$ 的元素，则总内存为：

$$
M(n) = \sum_i n_i \cdot s(t_i)
$$

由于 $s(t_i)$ 是常数，增长率遵循数量：
$M(n) = O(\max_i n_i)$，与渐近分析相匹配，同时给出了具体的数量级。

#### 亲自尝试

1.  估算一个 $1000 \times 1000$ 浮点数矩阵（每个 8 字节）的内存。
2.  比较 Python 列表的列表与 NumPy 数组。
3.  添加指针和对象头的开销。
4.  对具有多个字段的自定义 `struct` 或类重复此操作。

#### 测试用例

| 结构                     | 公式            | 近似内存            |
| ------------------------ | --------------- | ------------------- |
| 包含 $n$ 个整数的列表    | $n \times 28$ B | 28 MB（100 万项）   |
| 包含 $n$ 个整数的数组    | $n \times 4$ B  | 4 MB                |
| $n \times n$ 浮点数矩阵  | $8n^2$ B        | 8 MB（当 $n=1000$） |
| 包含 $n$ 个条目的哈希表  | $O(n)$          | 取决于负载因子      |

#### 复杂度

| 指标     | 增长率 | 单位     |
| -------- | ------ | -------- |
| 空间     | $O(n)$ | 字节     |
| 开销     | $O(1)$ | 元数据   |

内存占用估算器将抽象的“$O(n)$ 空间”转化为有形的字节数，让你在程序耗尽内存之前*看到*你离极限有多近。
### 17 时间复杂度表

时间复杂度表总结了不同算法如何随输入规模增大而增长，它是从公式到感知的映射，展示了哪些复杂度是快速的、哪些是危险的，以及它们在规模上的比较。

#### 我们正在解决什么问题？

我们想要一个快速参考，将数学增长率与实际性能联系起来。
知道一个算法是 $O(n \log n)$ 是好的；理解这对于 $n = 10^6$ 意味着什么则更好。

该表有助于评估可行性：
这个算法能处理一百万个输入吗？十亿个呢？

#### 它是如何工作的（通俗解释）

1.  列出常见的复杂度类别：常数、对数、线性等。
2.  写出它们的公式和解释。
3.  估算不同 $n$ 值下的操作次数。
4.  突出性能变得不可行的临界点。

这就创建了一个算法增长的*直觉网格*。

#### 逐步示例

设 $n = 10^6$（一百万）。
估算每个复杂度类别的操作次数（近似规模）：

| 复杂度        | 公式                       | 操作次数 (n=10⁶)          | 直觉           |
| ------------- | -------------------------- | ------------------------- | -------------- |
| $O(1)$        | 常数                       | 1                         | 瞬间完成       |
| $O(\log n)$   | $\log_2 10^6 \approx 20$   | 20                        | 极快           |
| $O(n)$        | $10^6$                     | 1,000,000                 | 可管理         |
| $O(n \log n)$ | $10^6 \cdot 20$            | 20M                       | 仍然可以       |
| $O(n^2)$      | $(10^6)^2$                 | $10^{12}$                 | 太慢           |
| $O(2^n)$      | $2^{20} \approx 10^6$      | $n>30$ 时不可行           |                |
| $O(n!)$       | 阶乘                       | $10^6!$                   | 极其巨大       |

该表让复杂度变得*真实可感*。

#### 微型代码（Python）

```python
import math

def ops_estimate(n):
    return {
        "O(1)": 1,
        "O(log n)": math.log2(n),
        "O(n)": n,
        "O(n log n)": n * math.log2(n),
        "O(n^2)": n2
    }

print(ops_estimate(106))
```

#### 为什么它很重要

-   为渐近分析建立*数值直觉*
-   帮助为大的 $n$ 选择合适的算法
-   解释为什么 $O(n^2)$ 对于 $n=1000$ 可能可行，但对于 $n=10^6$ 则不行
-   将抽象的数学与现实世界的可行性联系起来

#### 一个温和的证明（为什么它有效）

每个复杂度类别描述了一个界定操作次数的函数 $f(n)$。
比较常见 $n$ 值下的 $f(n)$ 说明了相对增长率。
因为渐近符号忽略了常数因子，
随着 $n$ 增长，增长率的差异将占主导地位。

因此，数值示例是渐近行为的忠实近似。

#### 亲自尝试

1.  为 $n = 10^3, 10^4, 10^6$ 填写表格。
2.  绘制每个 $f(n)$ 的增长曲线。
3.  比较如果每次操作 = 1 微秒时的运行时间。
4.  根据你的硬件识别可行与不可行的复杂度。

#### 测试用例

| $n$    | $O(1)$ | $O(\log n)$ | $O(n)$        | $O(n^2)$  |
| ------ | ------ | ----------- | ------------- | --------- |
| $10^3$ | 1      | 10          | 1,000         | 1,000,000 |
| $10^6$ | 1      | 20          | 1,000,000     | $10^{12}$ |
| $10^9$ | 1      | 30          | 1,000,000,000 | $10^{18}$ |

#### 复杂度

| 操作           | 类型   | 见解                 |
| -------------- | ------ | -------------------- |
| 表格生成       | $O(1)$ | 静态参考             |
| 评估           | $O(1)$ | 分析性               |

时间复杂度表将抽象的大 O 符号转化为一张生动的图表，其中 $O(\log n)$ 感觉微小，$O(n^2)$ 感觉沉重，而 $O(2^n)$ 感觉不可能。
### 18 时空权衡探索器

时空权衡探索器帮助我们理解算法设计中最基本的平衡之一：使用更多内存来换取速度，或者为了节省内存而牺牲时间。这是在存储和计算之间寻找平衡的艺术。

#### 我们要解决什么问题？

我们经常面临一个选择：

- 预先计算并存储结果以便即时访问（更多空间，更少时间）
- 按需计算以节省内存（更少空间，更多时间）

目标是分析双方，并根据问题的约束选择最佳方案。

#### 工作原理（通俗解释）

1.  识别可以存储的重复计算。
2.  估算存储预计算数据的内存成本。
3.  估算每次查询或重用所节省的时间。
4.  使用总成本模型比较权衡：
   $$
   \text{总成本} = \text{时间成本} + \lambda \cdot \text{空间成本}
   $$
   其中 $\lambda$ 反映了系统的优先级。
5.  决定缓存、制表还是重新计算更可取。

你正在用两个旋钮调整性能，一个用于内存，一个用于时间。

#### 分步示例

示例 1：斐波那契数列

- 递归（无内存）：$O(2^n)$ 时间，$O(1)$ 空间
- 记忆化：$O(n)$ 时间，$O(n)$ 空间
- 迭代（制表）：$O(n)$ 时间，$O(1)$ 空间（仅存储最后两个）

同一个问题的不同权衡。

示例 2：查找表

假设你需要许多 $x$ 值的 $\sin(x)$：

- 每次都计算 → 每次查询 $O(n)$
- 存储所有结果 → $O(n)$ 内存，$O(1)$ 查找
- 混合：存储采样点，插值 → 平衡

#### 微型代码（Python）

```python
def fib_naive(n):
    if n <= 1: return n
    return fib_naive(n-1) + fib_naive(n-2)

def fib_memo(n, memo={}):
    if n in memo: return memo[n]
    if n <= 1: return n
    memo[n] = fib_memo(n-1, memo) + fib_memo(n-2, memo)
    return memo[n]
```

比较每个版本的时间与内存。

#### 为什么这很重要

- 帮助在内存限制或实时约束下设计算法
- 在数据库、图形、编译器和 AI 缓存中至关重要
- 将理论（渐近分析）与工程（资源）联系起来
- 促进在权衡曲线而非绝对意义上思考

#### 一个温和的证明（为什么它有效）

令 $T(n)$ = 时间，$S(n)$ = 空间。
如果我们预先计算 $k$ 个结果，
$$
T'(n) = T(n) - \Delta T, \quad S'(n) = S(n) + \Delta S
$$

由于 $\Delta T$ 和 $\Delta S$ 通常是单调的，
最小化一个会增加另一个。
因此，最优配置出现在
$$
\frac{dT}{dS} = -\lambda
$$
的地方，这反映了系统对时间与内存的估值。

#### 亲自尝试

1.  比较朴素、记忆化和迭代的斐波那契数列实现。
2.  为阶乘模 $M$ 构建一个查找表。
3.  探索动态规划制表（空间重）与滚动数组（空间轻）。
4.  评估递归树遍历中的缓存。

#### 测试用例

| 问题         | 空间      | 时间        | 策略             |
| ------------ | --------- | ----------- | ---------------- |
| 斐波那契数列 | $O(1)$    | $O(2^n)$    | 朴素递归         |
| 斐波那契数列 | $O(n)$    | $O(n)$      | 记忆化           |
| 斐波那契数列 | $O(1)$    | $O(n)$      | 迭代             |
| 查找表       | $O(n)$    | $O(1)$      | 预计算           |
| 重新计算     | $O(1)$    | $O(n)$      | 按需             |

#### 复杂度

| 操作             | 维度      | 备注           |
| ---------------- | --------- | -------------- |
| 时空分析         | $O(1)$    | 概念性         |
| 优化             | $O(1)$    | 权衡曲线       |

时空权衡探索器将资源限制转化为创造性的杠杆，帮助你选择何时记住、何时重新计算以及何时和谐地平衡两者。
### 19 算法性能分析

对算法进行性能分析意味着测量它*实际的行为*，包括运行时间、内存使用量、循环迭代频率以及时间具体消耗在何处。它将理论上的复杂度转化为实际的性能数据。

#### 我们要解决什么问题？

大 O 表示法告诉我们算法的扩展性，但无法说明它在*实践中的表现*。
常数因子、系统负载、编译器优化和缓存效应都很重要。

性能分析回答以下问题：

-   时间花在哪里？
-   哪个函数占主导地位？
-   我们受限于 CPU、内存还是 I/O？

它是运行时行为的显微镜。

#### 工作原理（通俗解释）

1.  插装你的代码，插入计时器、计数器或使用内置的性能分析器。
2.  使用有代表性的输入运行。
3.  记录运行时间、调用次数和内存分配情况。
4.  分析热点，即导致 90% 成本的 10% 代码。
5.  只在关键的地方进行优化。

性能分析不靠猜测，而是靠测量。

#### 逐步示例

#### 示例 1：为函数计时

```python
import time

start = time.perf_counter()
result = algorithm(n)
end = time.perf_counter()

print("Elapsed:", end - start)
```

测量给定输入规模下的总运行时间。

#### 示例 2：行级性能分析

```python
import cProfile, pstats

cProfile.run('algorithm(1000)', 'stats')
p = pstats.Stats('stats')
p.sort_stats('cumtime').print_stats(10)
```

显示最耗时的 10 个函数。

#### 微型代码（Python）

```python
def slow_sum(n):
    s = 0
    for i in range(n):
        for j in range(i):
            s += j
    return s

import cProfile
cProfile.run('slow_sum(500)')
```

输出列出函数、调用次数、总时间和累计时间。

#### 为何重要

-   连接理论（大 O）与实践（运行时间）
-   识别需要优化的瓶颈
-   验证算法在不同输入下的预期扩展性
-   防止过早优化，先测量，后修复

#### 一个温和的证明（为何有效）

每次算法执行都是一系列操作的轨迹。
性能分析实时采样或计数这些操作。

如果 $t_i$ 是组件 $i$ 花费的时间，
那么总运行时间 $T = \sum_i t_i$。
对 $t_i$ 进行排序可以经验性地揭示主导项，
从而证实或反驳理论假设。

#### 动手尝试

1.  分析一个递归函数（如 Fibonacci）。
2.  比较迭代与递归的运行时间。
3.  绘制 $n$ 与运行时间的关系图，可视化经验复杂度。
4.  使用 `memory_profiler` 捕获空间使用情况。

#### 测试用例

| 算法             | 预期复杂度    | 观测结果（示例）                       | 备注                     |
| ---------------- | ------------- | -------------------------------------- | ------------------------ |
| 线性搜索         | $O(n)$        | 运行时间 ∝ $n$                         | 线性扩展                 |
| 归并排序         | $O(n \log n)$ | 运行时间增长略快于 $n$                 | 归并操作开销             |
| 朴素 Fibonacci   | $O(2^n)$      | 在 $n>30$ 时急剧增长                   | 证实了指数级成本         |

#### 复杂度

| 操作             | 时间                    | 空间    |
| ---------------- | ----------------------- | ------- |
| 性能分析运行     | $O(n)$ （每次试验）     | $O(1)$  |
| 报告生成         | $O(f)$ （每个函数）     | $O(f)$  |

性能分析是数学与秒表相遇的地方，它将渐近的猜测转化为具体的数字，揭示出算法真正的心跳。
### 20 基准测试框架

基准测试框架提供了一种结构化方法来在相同条件下比较算法。它通过测量不同输入规模、多次试验和不同硬件下的性能，揭示哪种实现在实践中真正表现最佳。

#### 我们正在解决什么问题？

你有几种解决同一问题的算法——哪一种*实际上更快*？哪一种扩展性更好？哪一种使用更少的内存？

基准测试通过公平、可重复的实验来回答这些问题，而不是依靠直觉或孤立的计时测试。

#### 它是如何工作的（通俗解释）

1.  定义测试用例（输入规模、数据模式）。
2.  在相同条件下运行所有候选算法。
3.  重复试验以减少噪声。
4.  记录指标：
    *   运行时间
    *   内存使用情况
    *   吞吐量或延迟
5.  汇总结果并可视化趋势。

可以把它看作一场"锦标赛"，每个算法都遵循相同的规则。

#### 逐步示例

假设我们想对排序方法进行基准测试：

1.  输入：规模为 $10^3$、$10^4$、$10^5$ 的随机数组
2.  算法：`bubble_sort`、`merge_sort`、`timsort`
3.  指标：5 次运行的平均运行时间
4.  结果：表格或图表

| 规模   | 冒泡排序 | 归并排序 | Timsort |
| ------ | -------- | -------- | ------- |
| $10^3$ | 0.05s    | 0.001s   | 0.0008s |
| $10^4$ | 5.4s     | 0.02s    | 0.012s  |
| $10^5$ | –        | 0.25s    | 0.15s   |

Timsort 在所有规模上都胜出，数据证实了理论。

#### 微型代码（Python）

```python
import timeit
import random

def bench(func, n, trials=5):
    data = [random.randint(0, n) for _ in range(n)]
    return min(timeit.repeat(lambda: func(data.copy()), number=1, repeat=trials))

def bubble_sort(arr):
    for i in range(len(arr)):
        for j in range(len(arr)-1):
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]

def merge_sort(arr):
    if len(arr) <= 1: return arr
    mid = len(arr)//2
    return merge(merge_sort(arr[:mid]), merge_sort(arr[mid:]))

def merge(left, right):
    result = []
    while left and right:
        result.append(left.pop(0) if left[0] < right[0] else right.pop(0))
    return result + left + right

print("Bubble:", bench(bubble_sort, 1000))
print("Merge:", bench(merge_sort, 1000))
```

#### 为什么它很重要

-   将抽象复杂度转化为实证性能
-   支持基于证据的优化
-   检测大 O 表示法隐藏的常数因子影响
-   确保算法间的公平比较

#### 一个温和的证明（为什么它有效）

令 $t_{i,j}$ 为算法 $i$ 在第 $j$ 次试验中的时间。
基准测试报告 $\min$、$\max$ 或 $\text{mean}(t_{i,*})$。

通过控制条件（硬件、输入分布），我们将 $t_{i,j}$ 视为同一分布的样本，从而允许对 $E[t_i]$（期望运行时间）进行有效比较。
因此，结果反映了真实的相对性能。

#### 亲自尝试

1.  对有序数组上的线性搜索与二分搜索进行基准测试。
2.  测试动态数组插入与链表插入。
3.  在输入规模 $10^3$、$10^4$、$10^5$ 下运行。
4.  绘制结果：$n$（x 轴）与时间（y 轴）。

#### 测试用例

| 比较                   | 预期结果                         |
| ---------------------- | -------------------------------- |
| 冒泡排序 vs 归并排序   | 在 $n$ 较小时归并排序更快        |
| 线性搜索 vs 二分搜索   | 当 $n > 100$ 时二分搜索更快      |
| 列表查找 vs 字典查找   | 字典的 $O(1)$ 优于列表的 $O(n)$ |

#### 复杂度

| 步骤             | 时间    | 空间   |
| ---------------- | ------- | ------ |
| 运行每次试验     | $O(n)$  | $O(1)$ |
| 汇总结果         | $O(k)$  | $O(k)$ |
| 总基准测试       | $O(nk)$ | $O(1)$ |

($k$ = 试验次数)

基准测试框架将比较转化为科学、公平的测试、真实的数据，以及基于实验而非直觉的性能真相。

## 第 3 节. 大 O、大 Θ、大 Ω
### 21 增长率比较器

增长率比较器帮助我们*直观地*理解函数之间如何相对增长，这是渐近推理的核心。它让我们能够回答诸如：$n^2$ 是否比 $n \log n$ 增长得更快？$2^n$ 与 $n!$ 相比增长有多快？

#### 我们要解决什么问题？

我们需要一种清晰的方法来比较两个函数在 $n$ 变大时增长的速度。
在分析算法时，像 $n$、$n \log n$ 和 $n^2$ 这样的运行时函数在小规模时看起来很相似，但它们的增长率很快就会分道扬镳。

比较器为我们提供了一种数学和可视化的方式来对它们进行排序。

#### 工作原理（通俗解释）

1.  写出两个函数 $f(n)$ 和 $g(n)$。
2.  计算当 $n \to \infty$ 时比值 $\dfrac{f(n)}{g(n)}$。
3.  解释结果：
   * 如果 $\dfrac{f(n)}{g(n)} \to 0$ → $f(n) = o(g(n))$（增长更慢）
   * 如果 $\dfrac{f(n)}{g(n)} \to c > 0$ → $f(n) = \Theta(g(n))$（相同增长）
   * 如果 $\dfrac{f(n)}{g(n)} \to \infty$ → $f(n) = \omega(g(n))$（增长更快）

这个比值检验告诉我们对于大的 $n$，哪个函数占主导地位。

#### 分步示例

示例 1：比较 $n \log n$ 与 $n^2$

$$
\frac{n \log n}{n^2} = \frac{\log n}{n}
$$

当 $n \to \infty$ 时，$\frac{\log n}{n} \to 0$
→ $n \log n = o(n^2)$

示例 2：比较 $2^n$ 与 $n!$

$$
\frac{2^n}{n!} \to 0
$$

因为 $n!$ 比 $2^n$ 增长得更快。
→ $2^n = o(n!)$

#### 微型代码（Python）

```python
import math

def compare_growth(f, g, ns):
    for n in ns:
        ratio = f(n)/g(n)
        print(f"n={n:6}, ratio={ratio:.6e}")

compare_growth(lambda n: n*math.log2(n),
               lambda n: n2,
               [10, 100, 1000, 10000])
```

输出显示比值在缩小 → 证实了增长更慢。

#### 为什么它很重要

- 建立对渐近主导性的直觉
- 对于 Big-O、Big-Theta、Big-Omega 证明至关重要
- 阐明为什么某些算法扩展性更好
- 将数学转化为视觉和数值比较

#### 一个温和的证明（为什么它有效）

根据渐近记法的定义：

如果 $\displaystyle \lim_{n \to \infty} \frac{f(n)}{g(n)} = 0$，
那么对于任意 $\varepsilon > 0$，对于足够大的 $n$，有 $f(n) < \varepsilon g(n)$。

因此，$f(n)$ 比 $g(n)$ 增长得慢。

这个正式的极限检验是 Big-O 推理的基础。

#### 自己动手试试

1.  比较 $n^3$ 与 $2^n$
2.  比较 $\sqrt{n}$ 与 $\log n$
3.  比较 $n!$ 与 $n^n$
4.  绘制两个函数，看看一个在哪里超过另一个

#### 测试用例

| $f(n)$   | $g(n)$     | 结果 | 关系          |
| -------- | ---------- | ---- | ------------- |
| $\log n$ | $\sqrt{n}$ | $0$  | $o(\sqrt{n})$ |
| $n$      | $n \log n$ | $0$  | $o(n \log n)$ |
| $n^2$    | $2^n$      | $0$  | $o(2^n)$      |
| $2^n$    | $n!$       | $0$  | $o(n!)$       |

#### 复杂度

| 操作       | 时间            | 空间  |
| ---------- | --------------- | ----- |
| 比较       | $O(1)$ 每对函数 | $O(1)$ |

增长率比较器将渐近理论转化为一场对话，用数字和极限展示，当 $n$ 向无穷大攀升时，谁的增长速度真正更快。
### 22 主导项提取器

主导项提取器通过识别随着 $n$ 增大哪个项最为重要，来简化复杂度表达式。这就是我们将杂乱的运行时公式转化为简洁的大 O 记法的方式——只保留真正驱动增长的部分。

#### 我们要解决什么问题？

算法常常产生复合的成本公式，例如：
$$
T(n) = 3n^2 + 10n + 25
$$
并非所有项都以相同速度增长。主导项决定了长期行为，因此我们希望将其分离出来并丢弃其余部分。

这一步连接了详细的操作计数和渐近记法。

#### 工作原理（通俗解释）

1.  写出运行时函数 $T(n)$（来自步骤计数）。
2.  按增长类型（$n^3$, $n^2$, $n$, $\log n$, 常数）列出所有项。
3.  找出当 $n \to \infty$ 时增长最快的项。
4.  去掉系数和低阶项。
5.  结果就是大 O 类别。

可以将其想象为拉远观察一条曲线，较小的波动在无穷远处会消失。

#### 逐步示例

示例 1：
$$
T(n) = 5n^3 + 2n^2 + 7n + 12
$$

对于大的 $n$，$n^3$ 占主导。

因此：
$$
T(n) = O(n^3)
$$

示例 2：
$$
T(n) = n^2 + n\log n + 10n
$$

逐项比较：
$$
n^2 > n \log n > n
$$

所以主导项是 $n^2$。
$\Rightarrow T(n) = O(n^2)$

#### 微型代码（Python）

```python
def dominant_term(terms):
    growth_order = {'1': 0, 'logn': 1, 'n': 2, 'nlogn': 3, 'n^2': 4, 'n^3': 5, '2^n': 6}
    return max(terms, key=lambda t: growth_order[t])

print(dominant_term(['n^2', 'nlogn', 'n']))  # n^2
```

你可以使用 SymPy 进行符号简化来扩展此功能。

#### 为何重要

-   将详细公式简化为简洁的渐近记法
-   关注于缩放行为，而非常数
-   使性能比较变得直接
-   从原始步骤计数推导大 O 记法的核心步骤

#### 一个温和的证明（为何有效）

令
$$
T(n) = a_k n^k + a_{k-1} n^{k-1} + \dots + a_0
$$

当 $n \to \infty$ 时，
$$
\frac{a_{k-1} n^{k-1}}{a_k n^k} = \frac{a_{k-1}}{a_k n} \to 0
$$

所有低阶项相对于最大指数项都趋于消失。
因此 $T(n) = \Theta(n^k)$。

这可以推广到多项式之外，适用于任何具有严格增长顺序的函数族。

#### 亲自尝试

1.  简化 $T(n) = 4n \log n + 10n + 100$。
2.  简化 $T(n) = 2n^3 + 50n^2 + 1000$。
3.  简化 $T(n) = 5n + 10\log n + 100$。
4.  使用比值测试验证：$\frac{\text{低阶项}}{\text{主导项}} \to 0$。

#### 测试用例

| 表达式              | 主导项         | 大 O 记法      |
| ------------------- | -------------- | -------------- |
| $3n^2 + 4n + 10$    | $n^2$          | $O(n^2)$       |
| $5n + 8\log n + 7$  | $n$            | $O(n)$         |
| $n \log n + 100n$   | $n \log n$     | $O(n \log n)$  |
| $4n^3 + n^2 + 2n$   | $n^3$          | $O(n^3)$       |

#### 复杂度

| 操作         | 时间复杂度 | 空间复杂度 |
| ------------ | ---------- | ---------- |
| 提取         | $O(k)$     | $O(1)$     |

（$k$ = 项数）

主导项提取器就像一盏聚光灯，它照亮了决定算法步伐的那一项，让你看清算法的真实渐近特性。
### 23 基于极限的复杂度测试

基于极限的复杂度测试是一种通过使用极限来精确比较两个函数增长速度的方法。它是一种数学工具，能将直觉（"这个感觉更快"）转化为证明（"这个*确实*更快"）。

#### 我们要解决什么问题？

在分析算法时，我们经常问：
$f(n)$ 的增长速度是比 $g(n)$ 慢、相等还是快？
与其猜测，我们使用极限来确定确切的关系，并使用大 O、$\Theta$ 或 $\Omega$ 对它们进行分类。

这种方法能对增长率进行正式且可靠的比较。

#### 工作原理（通俗解释）

1.  从两个正函数 $f(n)$ 和 $g(n)$ 开始。
2.  计算比值：
    $$
    L = \lim_{n \to \infty} \frac{f(n)}{g(n)}
    $$
3.  解释极限结果：

    *   如果 $L = 0$，则 $f(n) = o(g(n))$ → $f$ 增长更慢。
    *   如果 $0 < L < \infty$，则 $f(n) = \Theta(g(n))$ → 增长速度相同。
    *   如果 $L = \infty$，则 $f(n) = \omega(g(n))$ → $f$ 增长更快。

这个比值告诉我们一个函数相对于另一个函数的"缩放"情况。

#### 逐步示例

示例 1：

比较 $f(n) = n \log n$ 和 $g(n) = n^2$。

$$
\frac{f(n)}{g(n)} = \frac{n \log n}{n^2} = \frac{\log n}{n}
$$

当 $n \to \infty$ 时，$\frac{\log n}{n} \to 0$。
所以 $n \log n = o(n^2)$ → 增长更慢。

示例 2：

比较 $f(n) = 3n^2 + 4n$ 和 $g(n) = n^2$。

$$
\frac{f(n)}{g(n)} = \frac{3n^2 + 4n}{n^2} = 3 + \frac{4}{n}
$$

当 $n \to \infty$ 时，$\frac{4}{n} \to 0$。
所以 $\lim = 3$，常数且为正。
因此，$f(n) = \Theta(g(n))$。

#### 微型代码（Python）

```python
import sympy as sp

n = sp.symbols('n', positive=True)
f = n * sp.log(n)
g = n2
L = sp.limit(f/g, n, sp.oo)
print("Limit:", L)
```

输出 `0`，确认了 $n \log n = o(n^2)$。

#### 为什么它很重要

-   为渐近关系提供形式化证明
-   消除了比较增长率时的猜测
-   是大 O 证明和递归分析的核心步骤
-   有助于验证近似是否有效

#### 一个温和的证明（为什么它有效）

渐近比较的定义使用了极限：

如果 $\displaystyle \lim_{n \to \infty} \frac{f(n)}{g(n)} = 0$，
那么对于任何 $\varepsilon > 0$，
存在 $N$ 使得对于所有 $n > N$，
$f(n) \le \varepsilon g(n)$。

这满足了 $f(n) = o(g(n))$ 的形式条件。
类似地，常数或无穷极限定义了 $\Theta$ 和 $\omega$。

#### 自己试试

1.  比较 $n^3$ 和 $2^n$。
2.  比较 $\sqrt{n}$ 和 $\log n$。
3.  比较 $n!$ 和 $n^n$。
4.  检查 $n^2 + n$ 与 $n^2$ 的比值。

#### 测试用例

| $f(n)$    | $g(n)$     | 极限      | 关系           |
| --------- | ---------- | --------- | -------------- |
| $n$       | $n \log n$ | 0         | $o(g(n))$      |
| $n^2 + n$ | $n^2$      | 1         | $\Theta(g(n))$ |
| $2^n$     | $n^3$      | $\infty$  | $\omega(g(n))$ |
| $\log n$  | $\sqrt{n}$ | 0         | $o(g(n))$      |

#### 复杂度

| 操作           | 时间            | 空间   |
| -------------- | --------------- | ------ |
| 极限求值       | $O(1)$ 符号计算 | $O(1)$ |

基于极限的复杂度测试是你的数学放大镜，一种清晰、严谨的方法，用于比较算法增长，并将渐近直觉转化为确定性。
### 24 求和化简器

求和化简器利用代数和已知的求和规则，将循环和递归成本表达式转换为闭式公式。它在原始迭代计数和大 O 记号之间架起了桥梁。

#### 我们要解决什么问题？

在分析循环时，我们常常得到以和式表示的总工作量：

$$
T(n) = \sum_{i=1}^{n} i \quad \text{或} \quad T(n) = \sum_{i=1}^{n} \log i
$$

但大 O 记号要求我们将这些和式简化为熟悉的 $n$ 的函数。
求和化简将迭代模式转化为渐近形式。

#### 工作原理（通俗解释）

1.  写下循环或递推关系中的和式。
2.  应用标准公式或近似公式：

   * $\sum_{i=1}^{n} 1 = n$
   * $\sum_{i=1}^{n} i = \frac{n(n+1)}{2}$
   * $\sum_{i=1}^{n} i^2 = \frac{n(n+1)(2n+1)}{6}$
   * $\sum_{i=1}^{n} \log i = O(n \log n)$
3.  去掉常数项和低阶项。
4.  返回简化后的函数 $f(n)$ → 然后应用大 O 记号。

这就像是对迭代计数进行代数压缩。

#### 逐步示例

示例 1：
$$
T(n) = \sum_{i=1}^{n} i
$$
使用公式：
$$
T(n) = \frac{n(n+1)}{2}
$$
简化：
$$
T(n) = O(n^2)
$$

示例 2：
$$
T(n) = \sum_{i=1}^{n} \log i
$$
用积分近似：
$$
\int_1^n \log x , dx = n \log n - n + 1
$$
所以 $T(n) = O(n \log n)$

示例 3：
$$
T(n) = \sum_{i=1}^{n} \frac{1}{i}
$$
≈ $\log n$ (调和级数)

#### 微型代码 (Python)

```python
import sympy as sp

i, n = sp.symbols('i n', positive=True)
expr = sp.summation(i, (i, 1, n))
print(sp.simplify(expr))  # n*(n+1)/2
```

或者使用 `sp.summation(sp.log(i), (i,1,n))` 来处理对数求和。

#### 为何重要

- 将嵌套循环转换为可分析的公式
- 时间复杂度推导的核心工具
- 帮助可视化累积工作量如何形成
- 连接离散步骤与连续近似

#### 一个温和的证明（为何有效）

如果 $f(i)$ 是正的且递增，
那么根据积分判别法：

$$
\int_1^n f(x),dx \le \sum_{i=1}^n f(i) \le f(n) + \int_1^n f(x),dx
$$

因此，对于渐近分析的目的，
$\sum f(i)$ 和 $\int f(x)$ 以相同的速率增长。

这种等价性证明了像 $\sum \log i = O(n \log n)$ 这样的近似是合理的。

#### 动手试试

1.  简化 $\sum_{i=1}^n i^3$。
2.  简化 $\sum_{i=1}^n \sqrt{i}$。
3.  简化 $\sum_{i=1}^n \frac{1}{i^2}$。
4.  使用积分近似 $\sum_{i=1}^{n/2} i$。

#### 测试用例

| 求和式              | 公式                          | 大 O 记号       |
| ------------------ | ---------------------------- | ------------- |
| $\sum 1$           | $n$                          | $O(n)$        |
| $\sum i$           | $\frac{n(n+1)}{2}$           | $O(n^2)$      |
| $\sum i^2$         | $\frac{n(n+1)(2n+1)}{6}$     | $O(n^3)$      |
| $\sum \log i$      | $n \log n$                   | $O(n \log n)$ |
| $\sum \frac{1}{i}$ | $\log n$                     | $O(\log n)$   |

#### 复杂度

| 操作         | 时间                      | 空间   |
| ------------ | ------------------------- | ------ |
| 化简         | $O(1)$ (公式查找)         | $O(1)$ |

求和化简器将循环运算转化为优雅的公式，这是计数步骤和*看清*增长形态之间的区别。
### 25 递归树方法

递归树方法是一种用于求解分治递归式的可视化技术。它将递归公式展开成一棵子问题树，对每一层所做的工作进行求和，从而揭示总成本。

#### 我们正在解决什么问题？

许多递归算法（如归并排序或快速排序）将其运行时间定义为
$$
T(n) = a , T!\left(\frac{n}{b}\right) + f(n)
$$
其中：

- $a$ = 子问题数量，
- $b$ = 规模缩减因子，
- $f(n)$ = 每次调用的非递归工作量。

递归树让我们可以通过对各层求和来查看总成本，而不是立即应用闭式定理。

#### 工作原理（通俗解释）

1.  绘制递归树
    *   根节点：规模为 $n$ 的问题，成本 $f(n)$。
    *   每个节点：规模为 $\frac{n}{b}$ 的子问题，成本 $f(\frac{n}{b})$。
2.  展开各层直到基本情况（$n=1$）。
3.  对每一层的工作量求和：
    *   第 $i$ 层有 $a^i$ 个节点，每个节点规模为 $\frac{n}{b^i}$。
    *   第 $i$ 层的总工作量：
        $$
        W_i = a^i \cdot f!\left(\frac{n}{b^i}\right)
        $$
4.  将所有层相加：
    $$
    T(n) = \sum_{i=0}^{\log_b n} W_i
    $$
5.  确定主导层（顶层、中间层或底层）。
6.  简化为大 O 形式。

#### 逐步示例

以归并排序为例：

$$
T(n) = 2T!\left(\frac{n}{2}\right) + n
$$

第 0 层：$1 \times n = n$
第 1 层：$2 \times \frac{n}{2} = n$
第 2 层：$4 \times \frac{n}{4} = n$
⋯
深度：$\log_2 n$ 层

总工作量：
$$
T(n) = n \log_2 n + n = O(n \log n)
$$

每一层成本为 $n$，总计 = $n \times \log n$。

#### 微型代码（Python）

```python
import math

def recurrence_tree(a, b, f, n):
    total = 0
    level = 0
    while n >= 1:
        work = (a**level) * f(n/(b**level))
        total += work
        level += 1
        n /= b
    return total
```

对于 $f(n) = n$，使用 `f = lambda x: x`。

#### 为什么它很重要

-   使递归结构可见且直观
-   解释了主定理结果成立的原因
-   突显了主导层（顶层主导 vs 底层主导）
-   是分解递归成本的绝佳教学和推理工具

#### 一个温和的证明（为什么它有效）

每个递归调用贡献 $f(n)$ 的工作量加上子调用。
因为每一层的子问题规模相等，所以总成本是可加的：

$$
T(n) = \sum_{i=0}^{\log_b n} a^i f!\left(\frac{n}{b^i}\right)
$$

主导层决定了渐近阶：

-   顶层主导：$f(n)$ 占主导 → $O(f(n))$
-   平衡：所有层相等 → $O(f(n) \log n)$
-   底层主导：叶子节点占主导 → $O(n^{\log_b a})$

这个推理直接引出了主定理。

#### 自己动手试试

1.  为 $T(n) = 3T(n/2) + n$ 构建树。
2.  对每一层的工作量求和。
3.  与主定理结果进行比较。
4.  尝试 $T(n) = T(n/2) + 1$（对数树）。

#### 测试用例

| 递归式        | 每层工作量                  | 层数             | 总计          | 大 O           |
| ------------- | --------------------------- | ---------------- | ------------- | -------------- |
| $2T(n/2)+n$   | $n$                         | $\log n$         | $n \log n$    | $O(n \log n)$  |
| $T(n/2)+1$    | $1$                         | $\log n$         | $\log n$      | $O(\log n)$    |
| $4T(n/2)+n$   | $a^i = 4^i$, work = $n \cdot 2^i$ | 底层主导         | $O(n^2)$      |                |

#### 复杂度

| 步骤              | 时间               | 空间       |
| ----------------- | ------------------ | ---------- |
| 树构建            | $O(\log n)$ 层     | $O(\log n)$ |

递归树方法将抽象公式转化为生动的图表，展示了每一层的努力，揭示了真正驱动算法成本的层级。
### 26 主定理求值器

主定理求值器提供了一种基于公式的快速方法，用于求解形式如下的分治递归式：
$$
T(n) = a , T!\left(\frac{n}{b}\right) + f(n)
$$
它无需完全展开或求和，就能告诉你 $T(n)$ 的渐近行为，这是从递归树中诞生的一条捷径。

#### 我们要解决什么问题？

我们想快速找到分治算法的大 O 复杂度。
手动展开递归（通过递归树）是可行的，但很繁琐。
主定理通过比较递归部分的工作量 ($a , T(n/b)$) 和非递归部分的工作量 ($f(n)$) 来对解进行分类。

#### 工作原理（通俗解释）

给定
$$
T(n) = a , T!\left(\frac{n}{b}\right) + f(n)
$$

- $a$ = 子问题的数量
- $b$ = 缩减因子
- $f(n)$ = 递归外完成的工作量

计算临界指数：
$$
n^{\log_b a}
$$

将 $f(n)$ 与 $n^{\log_b a}$ 进行比较：

1. **情况 1（顶部主导）**：
   如果 $f(n) = O(n^{\log_b a - \varepsilon})$，
   $$T(n) = \Theta(n^{\log_b a})$$
   递归部分占主导。

2. **情况 2（平衡）**：
   如果 $f(n) = \Theta(n^{\log_b a} \log^k n)$，
   $$T(n) = \Theta(n^{\log_b a} \log^{k+1} n)$$
   两部分贡献相等。

3. **情况 3（底部主导）**：
   如果 $f(n) = \Omega(n^{\log_b a + \varepsilon})$
   且正则性条件成立：
   对于某个 $c<1$，有 $$a f(n/b) \le c f(n)$$，
   那么 $$T(n) = \Theta(f(n))$$
   非递归部分占主导。

#### 逐步示例

**示例 1**：
$$
T(n) = 2T(n/2) + n
$$

- $a = 2$, $b = 2$, $f(n) = n$
- $n^{\log_2 2} = n$
  所以 $f(n) = \Theta(n^{\log_2 2})$ → 情况 2

$$
T(n) = \Theta(n \log n)
$$

**示例 2**：
$$
T(n) = 4T(n/2) + n
$$

- $a = 4$, $b = 2$ → $n^{\log_2 4} = n^2$
- $f(n) = n = O(n^{2 - \varepsilon})$ → 情况 1

$$
T(n) = \Theta(n^2)
$$

**示例 3**：
$$
T(n) = T(n/2) + n
$$

- $a=1$, $b=2$, $n^{\log_2 1}=1$
- $f(n)=n = \Omega(n^{0+\varepsilon})$ → 情况 3

$$
T(n) = \Theta(n)
$$

#### 微型代码（Python）

```python
import math

def master_theorem(a, b, f_exp):
    critical = math.log(a, b)
    if f_exp < critical:
        return f"O(n^{critical:.2f})"
    elif f_exp == critical:
        return f"O(n^{critical:.2f} log n)"
    else:
        return f"O(n^{f_exp})"
```

对于 $T(n) = 2T(n/2) + n$，调用 `master_theorem(2,2,1)` → `O(n log n)`

#### 为什么它很重要

- 几秒钟内解决递归式
- 分析分治算法的基础
- 验证从递归树得到的直觉
- 广泛应用于排序、搜索、矩阵乘法、FFT

#### 一个温和的证明（为什么它有效）

每个递归层的代价为：
$$a^i , f!\left(\frac{n}{b^i}\right)$$

总代价为：
$$T(n) = \sum_{i=0}^{\log_b n} a^i f!\left(\frac{n}{b^i}\right)$$

$f(n)$ 相对于 $n^{\log_b a}$ 的增长速度决定了哪一层（顶部、中间或底部）占主导，从而产生了三种标准情况。

#### 自己试试

1. $T(n) = 3T(n/2) + n$
2. $T(n) = 2T(n/2) + n^2$
3. $T(n) = 8T(n/2) + n^3$
4. 识别 $a, b, f(n)$ 并应用定理。

#### 测试用例

| 递归式             | 情况 | 结果           |
| ------------------ | ---- | -------------- |
| $2T(n/2)+n$        | 2    | $O(n \log n)$  |
| $4T(n/2)+n$        | 1    | $O(n^2)$       |
| $T(n/2)+n$         | 3    | $O(n)$         |
| $3T(n/3)+n\log n$  | 2    | $O(n\log^2 n)$ |

#### 复杂度

| 步骤       | 时间   | 空间  |
| ---------- | ------ | ----- |
| 求值       | $O(1)$ | $O(1)$ |

主定理求值器是你的公式化指南针，它能立即指向隐藏在递归方程中的渐近真相，无需绘制递归树。
### 27 大Θ证明构建器

大Θ证明构建器帮助你正式证明一个函数与另一个函数以相同的速率增长。这是精确展示 $f(n)$ 和 $g(n)$ 渐近等价的方法，表明在常数因子之外，它们既不会增长得更快，也不会增长得更慢。

#### 我们要解决什么问题？

我们常说一个算法是 $T(n) = \Theta(n \log n)$，但我们如何证明它呢？
大Θ证明使用不等式将 $T(n)$ 夹在一个更简单函数 $g(n)$ 的两个缩放版本之间，从而确认紧的渐近界。

这能将直觉转化为严谨的证据。

#### 工作原理（通俗解释）

我们说
$$
f(n) = \Theta(g(n))
$$
如果存在常数 $c_1, c_2 > 0$ 和 $n_0$，使得对于所有 $n \ge n_0$：
$$
c_1 g(n) \le f(n) \le c_2 g(n)
$$

因此，$f(n)$ 被夹在 $g(n)$ 的两个常数倍之间。

步骤：

1.  确定 $f(n)$ 和候选的 $g(n)$。
2.  找到常数 $c_1$、$c_2$ 和阈值 $n_0$。
3.  验证对于所有 $n \ge n_0$ 不等式成立。
4.  得出结论 $f(n) = \Theta(g(n))$。

#### 逐步示例

示例 1：
$$
f(n) = 3n^2 + 10n + 5
$$
候选：$g(n) = n^2$

对于大的 $n$，$10n + 5$ 相对于 $3n^2$ 很小。

我们可以证明：
$$
3n^2 \le 3n^2 + 10n + 5 \le 4n^2, \quad \text{对于 } n \ge 10
$$

因此，$f(n) = \Theta(n^2)$，其中 $c_1 = 3$，$c_2 = 4$，$n_0 = 10$。

示例 2：
$$
f(n) = n \log n + 100n
$$
候选：$g(n) = n \log n$

对于 $n \ge 2$，$\log n \ge 1$，所以 $100n \le 100n \log n$。
因此，
$$
n \log n \le f(n) \le 101n \log n
$$
→ $f(n) = \Theta(n \log n)$

#### 微型代码（Python）

```python
def big_theta_proof(f, g, n0, c1, c2):
    for n in range(n0, n0 + 5):
        if not (c1*g(n) <= f(n) <= c2*g(n)):
            return False
    return True

f = lambda n: 3*n2 + 10*n + 5
g = lambda n: n2
print(big_theta_proof(f, g, 10, 3, 4))  # True
```

#### 为什么它很重要

-   将非正式的说法（"它大概是 $n^2$ 级别的"）转化为正式的证明
-   在渐近推理中建立严谨性
-   对于算法分析、递归式证明和课程学习至关重要
-   加深对常数和阈值的理解

#### 一个温和的证明（为什么它有效）

根据定义，
$$
f(n) = \Theta(g(n)) \iff \exists c_1, c_2, n_0 : c_1 g(n) \le f(n) \le c_2 g(n)
$$
这反映了大O和大Ω如何结合：

-   $f(n) = O(g(n))$ 给出上界，
-   $f(n) = \Omega(g(n))$ 给出下界。
    它们一起构成了一个紧的界，因此是 $\Theta$。

#### 自己尝试

1.  证明 $5n^3 + n^2 + 100 = \Theta(n^3)$。
2.  证明 $4n + 10 = \Theta(n)$。
3.  证明 $n \log n + 100n = \Theta(n \log n)$。
4.  尝试一个失败的证明：$n^2 + 3n = \Theta(n)$（不成立）。

#### 测试用例

| $f(n)$            | $g(n)$     | $c_1, c_2, n_0$ | 结果               |
| ----------------- | ---------- | --------------- | ------------------ |
| $3n^2 + 10n + 5$  | $n^2$      | $3,4,10$        | $\Theta(n^2)$      |
| $n \log n + 100n$ | $n \log n$ | $1,101,2$       | $\Theta(n \log n)$ |
| $10n + 50$        | $n$        | $10,11,5$       | $\Theta(n)$        |

#### 复杂度

| 步骤         | 时间                | 空间  |
| ------------ | ------------------- | ----- |
| 验证         | $O(1)$（符号计算）  | $O(1)$ |

大Θ证明构建器是你的渐近法庭，你提供证据、常数和不等式，而证明将给出裁决：$\Theta(g(n))$，不容置疑。
### 28 大 Omega 情形查找器

大 Omega 情形查找器帮助你确定算法增长的下界，即*保证的最小*成本，即使在最佳情况下也是如此。它是大 O 的镜像，展示了算法至少必须做到什么程度。

#### 我们要解决什么问题？

大 O 给了我们一个上界（"它不会比这个慢"），但有时我们需要知道下限，即一个它永远无法超越的复杂度。

大 Omega 帮助我们陈述：

- 可能的最快渐进行为，或者
- 问题本身固有的最小成本。

这在分析最佳情况性能或复杂度极限（例如比较排序的 $\Omega(n \log n)$ 下界）时至关重要。

#### 工作原理（通俗解释）

我们说
$$
f(n) = \Omega(g(n))
$$
如果存在 $c > 0, n_0$ 使得
$$
f(n) \ge c \cdot g(n) \quad \text{对于所有 } n \ge n_0
$$

步骤：

1.  确定候选下界函数 $g(n)$。
2.  证明 $f(n)$ 最终保持在 $g(n)$ 的某个常数倍之上。
3.  找到常数 $c$ 和 $n_0$。
4.  得出结论 $f(n) = \Omega(g(n))$。

#### 逐步示例

示例 1：
$$
f(n) = 3n^2 + 5n + 10
$$
候选：$g(n) = n^2$

对于 $n \ge 1$，
$$
f(n) \ge 3n^2 \ge 3 \cdot n^2
$$

所以 $f(n) = \Omega(n^2)$，其中 $c = 3$, $n_0 = 1$。

示例 2：
$$
f(n) = n \log n + 100n
$$
候选：$g(n) = n$

由于对于 $n \ge 2$，$\log n \ge 1$，
$$
f(n) = n \log n + 100n \ge n + 100n = 101n
$$
→ $f(n) = \Omega(n)$，其中 $c = 101$, $n_0 = 2$

#### 微型代码（Python）

```python
def big_omega_proof(f, g, n0, c):
    for n in range(n0, n0 + 5):
        if f(n) < c * g(n):
            return False
    return True

f = lambda n: 3*n**2 + 5*n + 10
g = lambda n: n**2
print(big_omega_proof(f, g, 1, 3))  # True
```

#### 为什么它很重要

- 定义最佳情况性能
- 提供理论下限（无法超越）
- 补充大 O（上界）和大 Theta（紧确界）
- 证明问题难度或最优性的关键

#### 一个温和的证明（为什么它有效）

如果
$$
\lim_{n \to \infty} \frac{f(n)}{g(n)} = L > 0,
$$
那么对于任何 $c \le L$，对于足够大的 $n$，都有 $f(n) \ge c \cdot g(n)$。
因此 $f(n) = \Omega(g(n))$。
这与 $\Omega$ 的形式定义一致，并直接源于渐近比值推理。

#### 自己试试

1.  证明 $4n^3 + n^2 = \Omega(n^3)$
2.  证明 $n \log n + n = \Omega(n)$
3.  证明 $2^n + n^5 = \Omega(2^n)$
4.  与它们的大 O 形式进行比较以形成对比。

#### 测试用例

| $f(n)$            | $g(n)$ | 常数             | 结果          |
| ----------------- | ------ | ---------------- | ------------- |
| $3n^2 + 10n$      | $n^2$  | $c=3$, $n_0=1$   | $\Omega(n^2)$ |
| $n \log n + 100n$ | $n$    | $c=101$, $n_0=2$ | $\Omega(n)$   |
| $n^3 + n^2$       | $n^3$  | $c=1$, $n_0=1$   | $\Omega(n^3)$ |

#### 复杂度

| 步骤         | 时间   | 空间  |
| ------------ | ------ | ----- |
| 验证         | $O(1)$ | $O(1)$ |

大 Omega 情形查找器展示了*曲线之下的地板*，确保无论算法试图运行得多快，它都建立在一个坚实的下界之上。
### 29 经验复杂度估计器

经验复杂度估计器连接了理论与实验，它测量不同输入规模的实际运行时间，并将其拟合到已知的增长模型，如 $O(n)$、$O(n \log n)$ 或 $O(n^2)$。当数学分析不明确或代码复杂时，这就是我们*发现*复杂度的方法。

#### 我们要解决什么问题？

有时 $T(n)$ 的确切公式过于混乱，或者实现细节不透明。
我们仍然可以通过观察运行时间如何随 $n$ 增长来经验性地估计复杂度。

这种方法特别适用于：

- 黑盒代码（实现未知）
- 对渐近性断言进行实验验证
- 比较实际扩展性与理论预测

#### 工作原理（通俗解释）

1. 选择有代表性的输入规模 $n_1, n_2, \dots, n_k$。
2. 测量每个规模对应的运行时间 $T(n_i)$。
3. 进行归一化或比较比率：
   * $T(2n)/T(n) \approx 2$ → $O(n)$
   * $T(2n)/T(n) \approx 4$ → $O(n^2)$
   * $T(2n)/T(n) \approx \log 2$ → $O(\log n)$
4. 使用回归或比率检验将数据拟合到候选模型。
5. 可视化趋势（例如，对数-对数图）以识别斜率 = 指数。

#### 逐步示例

假设我们测试输入规模：$n = 1000, 2000, 4000, 8000$

| $n$  | $T(n)$ (毫秒) | 比率 $T(2n)/T(n)$ |
| ---- | ----------- | ------------------ |
| 1000 | 5           | –                  |
| 2000 | 10          | 2.0                |
| 4000 | 20          | 2.0                |
| 8000 | 40          | 2.0                |

比率 $\approx 2$ → 线性增长 → $T(n) = O(n)$

现在假设：

| $n$  | $T(n)$ | 比率 |
| ---- | ------ | ----- |
| 1000 | 5      | –     |
| 2000 | 20     | 4     |
| 4000 | 80     | 4     |
| 8000 | 320    | 4     |

比率 $\approx 4$ → 二次增长 → $O(n^2)$

#### 微型代码（Python）

```python
import time, math

def empirical_estimate(f, ns):
    times = []
    for n in ns:
        start = time.perf_counter()
        f(n)
        end = time.perf_counter()
        times.append(end - start)
    for i in range(1, len(ns)):
        ratio = times[i] / times[i-1]
        print(f"n={ns[i]:6}, ratio={ratio:.2f}")
```

用不同的算法测试以观察扩展性。

#### 为什么这很重要

- 将运行时间数据转换为大 O 形式
- 检测瓶颈或意外的扩展行为
- 在理论分析困难时很有用
- 有助于验证优化或重构效果

#### 一个温和的证明（为什么它有效）

如果 $T(n) \approx c \cdot f(n)$，
那么比率检验
$$
\frac{T(kn)}{T(n)} \approx \frac{f(kn)}{f(n)}
$$
如果 $f(n) = n^p$，则揭示了指数 $p$：
$$
\frac{f(kn)}{f(n)} = k^p \implies p = \log_k \frac{T(kn)}{T(n)}
$$

在多个 $n$ 值上重复此操作，结果会收敛到真实的增长指数。

#### 亲自尝试

1. 测量排序算法在 $n$ 递增时的运行时间。
2. 使用比率检验估计 $p$。
3. 绘制 $\log n$ 与 $\log T(n)$ 的关系图，斜率 ≈ 指数。
4. 将 $p$ 与理论值进行比较。

#### 测试用例

| 算法          | 观测比率 | 估计复杂度       |
| ------------- | -------------- | -------------------- |
| 冒泡排序      | 4              | $O(n^2)$             |
| 归并排序      | 2.2            | $O(n \log n)$        |
| 线性搜索      | 2              | $O(n)$               |
| 二分搜索      | 1.1            | $O(\log n)$          |

#### 复杂度

| 步骤        | 时间              | 空间  |
| ----------- | ----------------- | ------ |
| 测量        | $O(k \cdot T(n))$ | $O(k)$ |
| 估计        | $O(k)$            | $O(1)$ |

($k$ = 样本点数量)

经验复杂度估计器将秒表转化为科学，将性能数据转化为曲线，将曲线转化为方程，将方程转化为大 O 直觉。
### 30 复杂度类别标识符

复杂度类别标识符帮助你将问题和算法归类到广泛的复杂度类别中，例如常数时间、对数时间、线性时间、二次时间、指数时间或多项式时间。这是一种理解*你的算法在计算增长的广阔版图中处于何处*的方法。

#### 我们要解决什么问题？

在分析算法时，我们通常想知道随着输入增长，其时间成本会变得多大。
我们不使用精确公式，而是根据算法的渐近增长将其分类到不同的族中。

这告诉我们什么是*可行的*（多项式）和什么是*爆炸性的*（指数），从而指导设计选择和理论极限。

#### 它是如何工作的（通俗解释）

我们将 $T(n)$ 的增长率映射到已知的复杂度类别：

| 类别          | 示例                       | 描述                     |
| ------------- | ------------------------ | --------------------------- |
| $O(1)$        | 哈希查找                   | 常数时间，不随规模变化       |
| $O(\log n)$   | 二分查找                   | 次线性，每一步减半           |
| $O(n)$        | 线性扫描                   | 工作量随输入大小增长         |
| $O(n \log n)$ | 归并排序                   | 带有对数因子的近线性         |
| $O(n^2)$      | 嵌套循环                   | 二次增长                   |
| $O(n^3)$      | 矩阵乘法                   | 三次增长                   |
| $O(2^n)$      | 回溯法                     | 指数爆炸                   |
| $O(n!)$       | 暴力排列                   | 阶乘爆炸                   |

识别步骤：

1. 分析循环和递归结构。
2. 统计主导操作。
3. 将模式与上表匹配。
4. 使用递推关系或比值测试验证。
5. 分配类别：常数 → 对数 → 多项式 → 指数。

#### 分步示例

示例 1：
单层循环：

```python
for i in range(n):
    work()
```

→ $O(n)$ → 线性

示例 2：
嵌套循环：

```python
for i in range(n):
    for j in range(n):
        work()
```

→ $O(n^2)$ → 二次

示例 3：
分治法：
$$
T(n) = 2T(n/2) + n
$$
→ $O(n \log n)$ → 对数线性

示例 4：
暴力枚举子集：
$$
2^n \text{ 种可能性}
$$
→ $O(2^n)$ → 指数

#### 微型代码（Python）

```python
def classify_complexity(code_structure):
    if "nested n" in code_structure:
        return "O(n^2)"
    if "divide and conquer" in code_structure:
        return "O(n log n)"
    if "constant" in code_structure:
        return "O(1)"
    return "O(n)"
```

你可以扩展此功能以模式匹配伪代码的结构。

#### 为什么它重要

- 提供关于可扩展性的即时直觉
- 指导设计权衡（速度 vs. 简单性）
- 将实际代码与理论极限联系起来
- 帮助比较解决同一问题的算法

#### 一个温和的证明（为什么它有效）

如果一个算法对大小为 $n$ 的输入执行 $f(n)$ 次基本操作，
并且 $f(n)$ 在渐近意义上类似于已知类别 $g(n)$：
$$
f(n) = \Theta(g(n))
$$
那么它就属于同一类别。
类别在 $\Theta$ 记法下形成等价组，将无限多的函数简化为有限的分类法。

#### 自己试试

对以下各项进行分类：

1. $T(n) = 5n + 10$
2. $T(n) = n \log n + 100$
3. $T(n) = n^3 + 4n^2$
4. $T(n) = 2^n$

识别它们的大 O 类别并解释可行性。

#### 测试用例

| $T(n)$       | 类别          | 描述       |
| ------------ | ------------- | ----------- |
| $7n + 3$     | $O(n)$        | 线性        |
| $3n^2 + 10n$ | $O(n^2)$      | 二次        |
| $n \log n$   | $O(n \log n)$ | 对数线性    |
| $2^n$        | $O(2^n)$      | 指数        |
| $100$        | $O(1)$        | 常数        |

#### 复杂度

| 步骤           | 时间   | 空间  |
| -------------- | ------ | ------ |
| 分类           | $O(1)$ | $O(1)$ |

复杂度类别标识符是你的算法宇宙地图，帮助你定位你的代码所处的位置，从平静的常数时间到阶乘增长的咆哮无限。

## 第 4 节 算法范式
### 31 贪心硬币示例

贪心硬币示例介绍了贪心算法范式，通过始终选择当前最优选项来解决问题，希望这能导向全局最优解。在硬币找零问题中，我们反复选取不超过剩余金额的最大面额硬币。

#### 我们正在解决什么问题？

我们希望使用尽可能少的硬币来凑出目标金额。
贪心算法总是选择局部最优的硬币，即面额 ≤ 剩余总额的最大硬币，并重复此过程直到达到目标金额。

这种方法适用于规范的硬币系统（如美国货币），但对于某些任意的面额组合会失败。

#### 工作原理（通俗解释）

1.  将可用的硬币面额按降序排序。
2.  对于每种硬币：
    *   在不超出总额的前提下，尽可能多地选取该硬币。
    *   从剩余金额中减去这些硬币的总值。
3.  继续使用更小面额的硬币，直到剩余金额为 0。

贪心算法的假设是：局部最优 → 全局最优。

#### 逐步示例

设硬币面额 = {25, 10, 5, 1}，目标金额 = 63

| 步骤 | 硬币 | 数量 | 剩余金额 |
| ---- | ---- | ----- | --------- |
| 1    | 25   | 2     | 13        |
| 2    | 10   | 1     | 3         |
| 3    | 5    | 0     | 3         |
| 4    | 1    | 3     | 0         |

总额 = 2×25 + 1×10 + 3×1 = 63
使用硬币数 = 6

贪心解 = 最优解（美国货币系统是规范的）。

反例：

硬币面额 = {4, 3, 1}，目标金额 = 6

- 贪心解：4 + 1 + 1 = 3 枚硬币
- 最优解：3 + 3 = 2 枚硬币

因此，对于非规范系统，贪心算法可能失败。

#### 微型代码（Python）

```python
def greedy_change(coins, amount):
    coins.sort(reverse=True) # 将硬币面额按降序排序
    result = []
    for coin in coins:
        while amount >= coin:
            amount -= coin
            result.append(coin)
    return result

print(greedy_change([25,10,5,1], 63))  # [25, 25, 10, 1, 1, 1]
```

#### 为何重要

- 展示了局部决策过程
- 快速且简单：对于面额数量是 $O(n)$ 复杂度
- 为生成树、调度、压缩等领域的贪心设计奠定了基础
- 突显了贪心算法何时有效、何时失效

#### 一个温和的证明（为何有效）

对于规范系统，贪心算法满足最优子结构和贪心选择性质：

- 贪心选择性质：局部最优选择是某个全局最优解的一部分。
- 最优子结构：剩余子问题具有最优的贪心解。

通过归纳法，贪心算法产生最少的硬币数量。

#### 亲自尝试

1.  尝试用 {25, 10, 5, 1} 为金额 68 进行贪心找零。
2.  尝试用 {9, 6, 1} 为金额 11 找零，并与暴力解法比较。
3.  识别贪心算法何时失效，测试 {4, 3, 1}。
4.  扩展算法，使其同时返回硬币列表和总数量。

#### 测试用例

| 硬币面额       | 金额 | 结果           | 是否最优？       |
| ----------- | ------ | ---------------- | -------------- |
| {25,10,5,1} | 63     | [25,25,10,1,1,1] | ✅              |
| {9,6,1}     | 11     | [9,1,1]          | ✅              |
| {4,3,1}     | 6      | [4,1,1]          | ❌ (3+3 更好) |

#### 复杂度

| 步骤      | 时间复杂度          | 空间复杂度  |
| --------- | ------------- | ------ |
| 排序   | $O(k \log k)$ | $O(1)$ |
| 选择 | $O(k)$        | $O(k)$ |

($k$ = 面额种类数量)

贪心硬币示例是贪心哲学的第一面镜子，它简单、直观且快速，为我们审视那些"现在最优即全局最优"的问题提供了一个视角。
### 32 贪心算法模板模拟器

贪心算法模板模拟器展示了所有贪心算法如何遵循相同的模式：重复选择最佳的局部选项，更新状态，并朝着目标前进。它是一个可复用的心智和编码框架，用于设计贪心算法解决方案。

#### 我们要解决什么问题？

许多优化问题可以通过做出局部选择而不重新审视先前的决策来解决。
与搜索所有路径（如回溯法）或构建表格（如动态规划）不同，贪心算法遵循一条由最佳下一个选择构成的确定性路径。

我们希望有一个通用的模板来模拟这种结构，适用于调度、找零和最小生成树等问题。

#### 工作原理（通俗解释）

1. 初始化问题状态（剩余值、容量等）。
2. 当目标未达成时：
   * 评估所有局部选择。
   * 选择最佳的即时选项（根据某个标准）。
   * 相应地更新状态。
3. 当没有更多有效操作时结束。

贪心算法依赖于一个选择规则（哪个局部选择最佳）和一个可行性检查（该选择是否有效？）。

#### 逐步示例

问题：按截止时间进行作业调度（最大化利润）

| 作业 | 截止时间 | 利润 |
| --- | -------- | ------ |
| A   | 2        | 60     |
| B   | 1        | 100    |
| C   | 3        | 20     |
| D   | 2        | 40     |

步骤：

1. 按利润降序排序作业：B(100), A(60), D(40), C(20)
2. 如果存在截止时间前的空闲时间槽，则接受每个作业
3. 填充时间槽：
   * 第 1 天：B
   * 第 2 天：A
   * 第 3 天：C
     → 总利润 = 180

贪心规则："如果截止时间允许，首先选择利润最高的作业。"

#### 微型代码（Python）

```python
def greedy_template(items, is_valid, select_best, update_state):
    state = initialize(items)
    while not goal_reached(state):
        best = select_best(items, state)
        if is_valid(best, state):
            update_state(best, state)
        else:
            break
    return state
```

具体的贪心解决方案只需填入：

- `select_best`：定义局部标准
- `is_valid`：定义可行性条件
- `update_state`：修改问题状态

#### 为什么重要

- 揭示了所有贪心算法背后的共同骨架
- 简化学习，"不同的身体，相同的骨骼"
- 通过基于模板的设计鼓励代码复用
- 帮助调试逻辑：如果失败，测试贪心选择性质

#### 一个温和的证明（为什么有效）

如果一个问题具有：

- 贪心选择性质：局部最优是全局最优的一部分
- 最优子结构：子问题的解决方案是最优的

那么任何遵循此模板的算法都会产生全局最优解。
可以通过对输入规模的归纳法进行形式化证明。

#### 亲自尝试

1. 为以下问题实现模板：
   * 找零问题
   * 分数背包问题
   * 区间调度问题
2. 与暴力搜索或动态规划进行比较以确认最优性。
3. 识别贪心算法何时失败（例如，非规范的硬币面额集合）。

#### 测试用例

| 问题                 | 局部规则               | 有效？ | 备注             |
| ----------------------- | ------------------------ | ------ | ---------------- |
| 分数背包问题     | 最大价值/重量比         | ✅      | 连续             |
| 区间调度问题     | 最早结束时间          | ✅      | 非重叠           |
| 找零问题 (25,10,5,1) | 最大硬币面额 ≤ 剩余金额 | ✅      | 仅适用于规范集合 |
| 作业调度          | 最高利润优先     | ✅      | 按利润排序       |

#### 复杂度

| 步骤      | 时间                 | 空间  |
| --------- | -------------------- | ------ |
| 选择 | $O(n \log n)$ (排序) | $O(n)$ |
| 迭代 | $O(n)$               | $O(1)$ |

贪心算法模板模拟器是贪心算法设计的万能钥匙，一旦你掌握了它的形态，每个贪心算法看起来都像一张熟悉的面孔。
### 33 分治算法骨架

分治算法骨架捕捉了通过将大问题拆分为更小、独立的子问题，递归求解每个子问题，然后合并它们的结果来解决问题的算法的通用结构。它是归并排序、快速排序、二分查找等算法背后的框架。

#### 我们要解决什么问题？

有些问题太大或太复杂，无法一次性处理。
分治算法通过将问题拆分为相同类型的更小子问题、递归求解，并将结果组合成整体来解决它们。

我们想要一个可重用的模板来揭示这种递归节奏。

#### 工作原理（通俗解释）

每个分治算法都遵循这个三步曲：

1.  **分解**：将问题分解为更小的子问题。
2.  **解决**：解决每个子问题（通常是递归地）。
3.  **合并**：合并或组装部分解。

这种递归持续进行，直到达到基本情况（小到可以直接解决）。

通用递推式：
$$
T(n) = aT!\left(\frac{n}{b}\right) + f(n)
$$

-   $a$：子问题的数量
-   $b$：规模减小的因子
-   $f(n)$：分解/合并的成本

#### 逐步示例

示例：归并排序

1.  分解：将数组分成两半
2.  解决：递归地对每一半进行排序
3.  合并：将两个已排序的半部分合并为一个

对于 $n = 8$：

-   第 0 层：大小为 8
-   第 1 层：大小为 4 + 4
-   第 2 层：大小为 2 + 2 + 2 + 2
-   第 3 层：大小为 1（基本情况）

每层成本为 $O(n)$ → 总成本 $O(n \log n)$。

#### 微型代码（Python）

```python
def divide_and_conquer(problem, base_case, divide, combine):
    if base_case(problem):
        return solve_directly(problem)
    subproblems = divide(problem)
    solutions = [divide_and_conquer(p, base_case, divide, combine) for p in subproblems]
    return combine(solutions)
```

为不同的问题插入自定义的 `divide`、`combine` 和基本情况逻辑。

#### 为什么它很重要

-   建模了许多核心算法的递归结构
-   通过递推式揭示了渐近模式
-   支持并行化（子问题独立求解）
-   在简单性（小子问题）和强大功能（问题规约）之间取得平衡

#### 一个温和的证明（为什么它有效）

如果每个递归层均匀地划分工作并在有限时间内重新组合，
那么总成本就是所有层成本的总和：
$$
T(n) = \sum_{i=0}^{\log_b n} a^i \cdot f!\left(\frac{n}{b^i}\right)
$$
主定理或递归树展开表明，根据 $f(n)$ 的不同，复杂度收敛到 $O(n^{\log_b a})$ 或 $O(n \log n)$。

正确性通过归纳法得出：每个子问题被最优解决 ⇒ 组合结果最优。

#### 动手试试

1.  为以下算法编写分治模板：
    *   二分查找
    *   归并排序
    *   Karatsuba（卡拉楚巴）乘法
2.  为每个算法确定 $a$、$b$、$f(n)$。
3.  使用主定理求解它们的递推式。

#### 测试用例

| 算法              | $a$ | $b$ | $f(n)$         | 复杂度            |
| ----------------- | --- | --- | -------------- | ----------------- |
| 二分查找          | 1   | 2   | 1              | $O(\log n)$       |
| 归并排序          | 2   | 2   | $n$            | $O(n \log n)$     |
| 快速排序          | 2   | 2   | $n$ (期望)     | $O(n \log n)$     |
| Karatsuba（卡拉楚巴） | 3   | 2   | $n$            | $O(n^{\log_2 3})$ |

#### 复杂度

| 步骤            | 时间                    | 空间               |
| --------------- | ----------------------- | ------------------ |
| 递归调用        | $O(n)$ 到 $O(n \log n)$ | $O(\log n)$ (栈)   |
| 合并            | $O(f(n))$               | 取决于合并方式     |

分治算法骨架是递归的心跳，是分解、解决、合并的节奏，贯穿算法设计的核心。
### 34 回溯法迷宫求解器

回溯法迷宫求解器展示了回溯范式，通过搜索空间探索所有可能的路径，在有效时前进，在遇到死胡同时撤销移动。这是递归搜索和约束满足问题的经典模型。

#### 我们正在解决什么问题？

我们想要在一个充满约束的迷宫或搜索空间中，找到一条从起点到目标的路径。
暴力方法会盲目尝试每条路径；回溯法通过一旦路径变得无效就进行剪枝，对此进行了改进。

这种方法为迷宫、数独、N皇后以及组合搜索问题的求解器提供了动力。

#### 工作原理（通俗解释）

1.  从初始位置开始。
2.  尝试一个移动方向（北、南、东、西）。
3.  如果移动有效，标记该位置并从那里递归继续。
4.  如果陷入困境，则回溯：撤销上一步移动并尝试新的方向。
5.  当到达目标或探索完所有路径时停止。

该算法本质上是深度优先的，它会完全探索一个分支后再返回。

#### 逐步示例

迷宫（网格示例）

```
S . . #
# . # .
. . . G
```

-   起点在 S (0,0)，目标在 G (2,3)
-   向右、向下移动，或绕过障碍物 (#)
-   标记已访问的单元格
-   当被困住时，退回一步并尝试另一条路径

找到的路径：
S → (0,1) → (1,1) → (2,1) → (2,2) → G

#### 微型代码（Python）

```python
def solve_maze(maze, x, y, goal):
    if (x, y) == goal:
        return True
    if not valid_move(maze, x, y):
        return False
    maze[x][y] = 'V'  # 标记为已访问
    for dx, dy in [(0,1), (1,0), (0,-1), (-1,0)]:
        if solve_maze(maze, x+dx, y+dy, goal):
            return True
    maze[x][y] = '.'  # 回溯
    return False
```

递归过程探索所有路径，并在过程中进行标记和取消标记。

#### 为何重要

-   展示了带撤销操作的搜索
-   是深度优先搜索、约束满足、谜题求解的基础
-   说明了状态探索和递归剪枝
-   为 N皇后、数独、图着色等问题提供了框架

#### 一个温和的证明（为何有效）

通过递归探索所有有效移动：

-   每条可行路径最终都会被检查。
-   不可行的分支会因有效性检查而提前终止。
-   回溯保证了所有组合都被探索一次。

因此，确保了完备性，如果存在路径，它将被找到。

#### 亲自尝试

1.  画一个包含一个解的 4×4 迷宫。
2.  手动运行回溯法，标记路径并撤销错误的转向。
3.  修改规则（例如，允许对角线移动）。
4.  与广度优先搜索（BFS，它找到最短路径）比较运行时间。

#### 测试用例

| 迷宫类型         | 是否找到解 | 备注                       |
| ---------------- | ---------- | -------------------------- |
| 开放网格         | 是         | 直接到达目标的路径         |
| 有障碍的迷宫     | 是         | 回退并重新规划路线         |
| 无路径的迷宫     | 否         | 穷尽所有选项               |

#### 复杂度

| 步骤   | 时间复杂度       | 空间复杂度           |
| ------ | ---------------- | -------------------- |
| 搜索   | 最坏情况 O(4ⁿ)   | O(n) 递归栈          |

(n = 单元格数量)

剪枝和约束降低了实际成本。

回溯法迷宫求解器是一次试错的旅程，是一个在迷宫中探索路径、优雅撤退、并找到隐藏解决方案的引导行者。
### 35 Karatsuba 乘法

Karatsuba 乘法算法是一种分治技术，它比传统的竖式乘法方法能更快地计算两个大数的乘积。它在每个递归步骤中将乘法次数从 4 次减少到 3 次，从而将复杂度从 O(n²) 提升到大约 O(n¹·⁵⁸⁵)。

#### 我们要解决什么问题？

当计算大数（或多项式）乘法时，标准方法需要执行所有数位对的乘法，对于 n 位数来说，工作量是 O(n²)。
Karatsuba 观察到其中部分工作是冗余的。通过巧妙地重用部分结果，我们可以减少乘法次数，从而获得速度提升。

这是许多快速算术算法和符号计算库的基础。

#### 工作原理（通俗解释）

给定两个 n 位数：

$$
x = 10^{m} \cdot a + b \\
y = 10^{m} \cdot c + d
$$

其中 ( a, b, c, d ) 大致是 x 和 y 的 n/2 位部分。

1.  计算三个乘积：
    *   ( p_1 = a \cdot c )
    *   ( p_2 = b \cdot d )
    *   ( p_3 = (a + b)(c + d) )
2.  使用以下公式组合结果：
    $$
    x \cdot y = 10^{2m} \cdot p_1 + 10^{m} \cdot (p_3 - p_1 - p_2) + p_2
    $$

这将递归乘法次数从 4 次减少到 3 次。

#### 分步示例

计算 12 × 34。

拆分：
- a = 1, b = 2
- c = 3, d = 4

计算：
- ( p_1 = 1 \times 3 = 3 )
- ( p_2 = 2 \times 4 = 8 )
- ( p_3 = (1 + 2)(3 + 4) = 3 \times 7 = 21 )

组合：
$$
(10^{2}) \cdot 3 + 10 \cdot (21 - 3 - 8) + 8 = 300 + 100 + 8 = 408
$$

所以 12 × 34 = 408（正确）。

#### 微型代码（Python）

```python
def karatsuba(x, y):
    # 基础情况：如果数字足够小，直接相乘
    if x < 10 or y < 10:
        return x * y
    # 确定数字的最大位数
    n = max(len(str(x)), len(str(y)))
    m = n // 2
    # 将 x 和 y 拆分为高位和低位部分
    a, b = divmod(x, 10**m)
    c, d = divmod(y, 10**m)
    # 递归计算三个乘积
    p1 = karatsuba(a, c)
    p2 = karatsuba(b, d)
    p3 = karatsuba(a + b, c + d)
    # 使用 Karatsuba 公式组合结果
    return p1 * 10**(2*m) + (p3 - p1 - p2) * 10**m + p2
```

#### 为什么它很重要

- 第一个次二次乘法算法
- 高级方法（Toom–Cook、基于 FFT 的方法）的基础
- 适用于整数、多项式、大数运算
- 展示了分治法的威力

#### 一个温和的证明（为什么它有效）

乘积展开式为：

$$
(a \cdot 10^m + b)(c \cdot 10^m + d) = a c \cdot 10^{2m} + (a d + b c)10^m + b d
$$

观察：
$$
(a + b)(c + d) = ac + ad + bc + bd
$$

因此：
$$
ad + bc = (a + b)(c + d) - ac - bd
$$

Karatsuba 利用这个恒等式来计算 ( ad + bc )，而无需进行单独的乘法。

递归关系：
$$
T(n) = 3T(n/2) + O(n)
$$
解：$T(n) = O(n^{\log_2 3}) \approx O(n^{1.585})$

#### 亲自尝试

1.  使用 Karatsuba 步骤计算 1234 × 5678。
2.  与竖式乘法的计算次数进行比较。
3.  将递归调用可视化为树。
4.  推导递归关系并验证复杂度。

#### 测试用例

| x    | y    | 结果      | 方法   |
| ---- | ---- | --------- | ------ |
| 12   | 34   | 408       | 有效   |
| 123  | 456  | 56088     | 有效   |
| 9999 | 9999 | 99980001  | 有效   |

#### 复杂度

| 步骤           | 时间复杂度 | 空间复杂度 |
| -------------- | ---------- | ---------- |
| 乘法           | O(n¹·⁵⁸⁵)  | O(n)       |
| 基础情况       | O(1)       | O(1)       |

Karatsuba 乘法揭示了代数重组的魔力，它利用一个巧妙的恒等式，将暴力算术转变为一场优雅、更快的分治之舞。
### 36 DP 状态图示例

DP 状态图示例介绍了将动态规划（DP）问题表示为通过状态转移连接的状态图的思想。这是一种可视化和结构化的方式，用于推理重叠子问题、依赖关系和递推关系。

#### 我们解决什么问题？

动态规划问题通常涉及一组相互依赖的子问题。
如果没有清晰的心智模型，很容易忘记哪些状态依赖于哪些其他状态。

状态图帮助我们：

- 将状态可视化为节点
- 将有向边显示为状态转移
- 理解迭代或递归的依赖顺序

这有助于建立对状态定义、转移逻辑和计算顺序的直觉。

#### 它是如何工作的（通俗解释）

1.  定义状态，即哪些参数代表一个子问题（例如，索引、容量、总和）。
2.  将每个状态绘制为一个节点。
3.  添加边以显示状态之间的转移。
4.  沿边分配递推关系：
   $$
   dp[\text{状态}] = \text{组合}(dp[\text{先前状态}])
   $$
5.  通过拓扑顺序（自底向上）或记忆化递归（自顶向下）来求解。

#### 逐步示例

示例：斐波那契数列

$$
F(n) = F(n-1) + F(n-2)
$$

状态图：

```
F(5)
↙   ↘
F(4) F(3)
↙↘   ↙↘
F(3)F(2)F(2)F(1)
```

每个节点 = 状态 `F(k)`
边 = 对 `F(k-1)` 和 `F(k-2)` 的依赖

观察：
许多状态重复出现，共享的子问题表明可以使用记忆化或自底向上的 DP。

另一个示例：0/1 背包问题

状态：`dp[i][w]` = 使用前 i 个物品、容量为 w 时的最大价值。
转移：

- 包含物品 i：`dp[i-1][w-weight[i]] + value[i]`
- 排除物品 i：`dp[i-1][w]`

图示：一个状态网格，每个单元格从前一行和左移的位置连接而来。

#### 微型代码（Python）

```python
def fib_dp(n):
    dp = [0, 1]
    for i in range(2, n + 1):
        dp.append(dp[i-1] + dp[i-2])
    return dp[n]
```

每个条目 `dp[i]` 代表一个状态，根据先前的依赖关系填充。

#### 为何重要

- 使 DP 可视化和具体化
- 阐明依赖方向（无环结构）
- 确保正确的计算顺序
- 作为自底向上或记忆化实现的蓝图

#### 一个温和的证明（为何有效）

如果一个问题的结构可以表示为一个状态的有向无环图（DAG），并且：

- 每个状态的值仅依赖于更早的状态
- 基础状态已知

那么通过按拓扑顺序评估节点，我们保证了正确性，每个子问题在其依赖项之后被解决。

这与基于递推深度的数学归纳法相匹配。

#### 亲自尝试

1.  绘制斐波那契的状态图。
2.  绘制 0/1 背包问题的网格（行 = 物品，列 = 容量）。
3.  可视化硬币找零问题的状态转移（凑出总金额的方法数）。
4.  追踪自底向上的计算顺序。

#### 测试用例

| 问题         | 状态       | 转移                            | 图形形状     |
| ------------ | ---------- | ------------------------------- | ------------ |
| 斐波那契     | dp[i]      | dp[i-1]+dp[i-2]                 | 链式         |
| 背包问题     | dp[i][w]   | max(包含, 排除)                 | 网格         |
| 硬币找零     | dp[i][s]   | dp[i-1][s]+dp[i][s-coin]        | 格点         |

#### 复杂度

| 步骤                 | 时间复杂度       | 空间复杂度 |
| -------------------- | ---------------- | ---------- |
| 图构建               | O(n²)（可视化）  | O(n²)      |
| DP 计算              | O(n·m) 典型情况  | O(n·m)     |

DP 状态图将抽象的递推关系转化为推理的地图，每条箭头代表一个依赖，每个节点代表一个已解决的步骤，引导你从基础情况走向最终解。
### 37 DP 表格可视化

DP 表格可视化是一种让动态规划变得具体可感的方法，它将状态和转移转化为一个清晰的表格，你可以逐行或逐列地填充。每个单元格代表一个子问题，填充它的过程展示了算法的结构。

#### 我们在解决什么问题？

当动态规划以递归关系的形式书写时，可能会感觉抽象。
表格将这种抽象转化为具体的东西：

- 行和列对应于子问题的参数
- 单元格值显示计算出的解
- 填充顺序揭示了依赖关系

这种方法对于列表法（自底向上的动态规划）尤其强大。

#### 它是如何工作的（通俗解释）

1.  定义你的 DP 状态（例如，`dp[i][j]` = 到第 i 个物品、容量为 j 时的最佳价值）。
2.  初始化基本情况（第一行/第一列）。
3.  按照依赖顺序遍历表格。
4.  在每个单元格应用递推关系：
   $$
   dp[i][j] = \text{combine}(dp[i-1][j], dp[i-1][j-w_i] + v_i)
   $$
5.  最终的单元格给出答案（通常是右下角）。

#### 逐步示例

示例：0/1 背包问题

物品：

| 物品 | 重量 | 价值 |
| ---- | ------ | ----- |
| 1    | 1      | 15    |
| 2    | 3      | 20    |
| 3    | 4      | 30    |

容量 = 4

状态：`dp[i][w]` = 使用前 i 个物品、容量为 w 时的最大价值。

递推关系：
$$
dp[i][w] = \max(dp[i-1][w], dp[i-1][w - w_i] + v_i)
$$

DP 表格：

| $i / w$ | 0 | 1  | 2  | 3  | 4  |
| ------- | - | -- | -- | -- | -- |
| 0       | 0 | 0  | 0  | 0  | 0  |
| 1       | 0 | 15 | 15 | 15 | 15 |
| 2       | 0 | 15 | 15 | 20 | 35 |
| 3       | 0 | 15 | 15 | 20 | 35 |


最终答案：35（物品 1 和 2）

#### 微型代码（Python）

```python
def knapsack(weights, values, W):
    n = len(weights)
    dp = [[0]*(W+1) for _ in range(n+1)]
    for i in range(1, n+1):
        for w in range(W+1):
            if weights[i-1] <= w:
                dp[i][w] = max(dp[i-1][w],
                               dp[i-1][w-weights[i-1]] + values[i-1])
            else:
                dp[i][w] = dp[i-1][w]
    return dp
```

每个 `dp[i][w]` 对应一个表格单元格，按照 i 和 w 递增的顺序填充。

#### 为什么它很重要

- 将递归关系转化为几何图形
- 使依赖关系可见且可追踪
- 明确了填充顺序（逐行、对角线等）
- 作为调试工具和教学辅助手段

#### 一个温和的证明（为什么它有效）

表格的填充顺序确保了每个子问题都在其依赖项之后被求解。
通过归纳法：

- 基础行/列被正确初始化
- 每个单元格都由有效的先前状态构建
- 最终单元格累积了最优解

这等价于在 DP 依赖图上进行拓扑排序。

#### 自己动手试试

1.  为硬币找零问题（方式数量）绘制 DP 表格。
2.  逐行填充。
3.  用箭头追踪依赖关系。
4.  标记出对最终答案有贡献的路径。

#### 测试用例

| 问题           | 状态      | 填充顺序   | 输出         |
| ------------- | -------- | ---------- | ---------- |
| 背包问题       | dp[i][w] | 逐行       | 最大价值    |
| LCS（最长公共子序列） | dp[i][j] | 对角线     | LCS 长度   |
| 编辑距离       | dp[i][j] | 逐行/逐列  | 最小操作数 |

#### 复杂度

| 步骤                 | 时间复杂度 | 空间复杂度 |
| -------------------- | ------ | ------ |
| 填充表格             | O(n·m) | O(n·m) |
| 回溯（可选）         | O(n+m) | O(1)   |

DP 表格可视化是递归的网格视图，是一片子问题的风景，每个子问题只求解一次，最终导向编码了完整解的最后一个单元格。
### 38 递归子问题树演示

递归子问题树演示展示了动态规划问题如何扩展成子问题树。它可视化递归结构、重复调用，以及记忆化或制表法可以在何处节省冗余工作。

#### 我们在解决什么问题？

编写递归解决方案时，相同的子问题经常被多次求解。
如果不将其可视化，我们可能无法意识到存在多少重叠。

通过将递归绘制成子问题树，我们可以：

- 识别重复节点（重复工作）
- 理解递归深度
- 决定使用记忆化（自顶向下）还是制表法（自底向上）

#### 工作原理（通俗解释）

1.  从根节点开始：完整问题（例如 `F(n)`）。
2.  递归地扩展为更小的子问题（子节点）。
3.  继续直到基本情况（叶子节点）。
4.  观察重复节点（同一子问题多次出现）。
5.  用查表替换重复计算。

最终结构是一棵树，在记忆化后变为一个有向无环图。

#### 逐步示例

示例：斐波那契数列（朴素递归）

$$
F(n) = F(n-1) + F(n-2)
$$

对于 $n = 5$：

```
        F(5)
       /    \
    F(4)    F(3)
   /   \    /   \
 F(3) F(2) F(2) F(1)
 / \
F(2) F(1)
```

重复节点：`F(3)`、`F(2)`
记忆化将存储这些结果并复用它们。

使用记忆化后（树被折叠）：

```
      F(5)
     /   \
   F(4)  F(3)
```

每个节点计算一次，重复调用被缓存查找取代。

#### 微型代码（Python）

```python
def fib(n, memo=None):
    if memo is None:
        memo = {}
    if n in memo:
        return memo[n]
    if n <= 1:
        return n
    memo[n] = fib(n-1, memo) + fib(n-2, memo)
    return memo[n]
```

memo 字典将递归树变成了一个有向无环图。

#### 为什么这很重要

- 揭示递归算法中隐藏的冗余
- 激发使用记忆化（缓存结果）
- 展示递归与迭代之间的联系
- 理解时间复杂度的可视化工具

#### 一个温和的证明（为什么它有效）

设 $T(n)$ 为递归树的大小。

斐波那契的朴素递归：
$$
T(n) = T(n-1) + T(n-2) + 1
$$
≈ $O(2^n)$ 次调用

使用记忆化后，每个子问题计算一次：
$$
T(n) = O(n)
$$

归纳法证明：

- 基本情况 $n=1$：显然成立
- 归纳步骤：如果所有更小的值都已记忆化，复用确保每个状态是常数时间查找

#### 亲自尝试

1.  绘制 Fibonacci(6) 的递归树。
2.  计算重复节点。
3.  添加一个记忆表并重绘为有向无环图。
4.  将相同技术应用于阶乘或网格路径问题。

#### 测试用例

| 函数      | 朴素调用次数 | 记忆化调用次数 | 时间复杂度       |
| --------- | ------------ | -------------- | ---------------- |
| fib(5)    | 15           | 6              | O(2ⁿ) → O(n)     |
| fib(10)   | 177          | 11             | O(2ⁿ) → O(n)     |
| fib(20)   | 21891        | 21             | O(2ⁿ) → O(n)     |

#### 复杂度

| 步骤             | 时间复杂度 | 空间复杂度 |
| ---------------- | ---------- | ---------- |
| 朴素递归         | O(2ⁿ)      | O(n)       |
| 使用记忆化       | O(n)       | O(n)       |

递归子问题树演示将隐藏的递归变成了一幅图画，每个分支代表一次计算，每个重复节点代表一个节省时间的机会，每个缓存条目都向高效迈进了一步。
### 39 贪心选择可视化

贪心选择可视化帮助您直观地看到贪心算法如何一步步做出决策，在每个节点选择局部最优选项并确定下来。通过可视化地追踪选择过程，您可以验证贪心策略是否真正导向全局最优解。

#### 我们要解决什么问题？

贪心算法总是选择当前最好的选项。
但并非所有问题都支持这种方法，有些问题需要回溯或动态规划。
要知道贪心何时有效，我们需要观察选择的链条及其影响。

贪心选择图揭示了：

-   每个局部决策是什么样子的
-   每个选择如何影响剩余的子问题
-   局部最优解是否会累积成全局最优解

#### 工作原理（通俗解释）

1.  从完整问题开始（例如，一组区间、硬币或物品）。
2.  根据贪心准则（例如，最大价值、最早结束时间）进行排序或优先级划分。
3.  选择当前可用的最佳选项。
4.  排除不兼容的元素（冲突、重叠）。
5.  重复直到没有有效的选择为止。
6.  将每一步可视化为一条增长的路径或序列。

生成的图片显示了选择边界，以及选择如何缩小可能性空间。

#### 逐步示例

**示例 1：区间调度**

目标：选择最多数量的不重叠区间。

| 区间 | 开始 | 结束 |
| :--- | :--- | :--- |
| A    | 1    | 4    |
| B    | 3    | 5    |
| C    | 0    | 6    |
| D    | 5    | 7    |
| E    | 8    | 9    |

贪心规则：选择最早结束时间。

步骤：

1.  按结束时间排序 → A(1–4), B(3–5), C(0–6), D(5–7), E(8–9)
2.  选择 A → 移除重叠区间 (B, C)
3.  接下来选择 D (5–7)
4.  接下来选择 E (8–9)

可视化：

```
时间线: 0---1---3---4---5---7---8---9
          [A]     [D]      [E]
```

总计 = 3 个区间 → 最优解。

**示例 2：分数背包问题**

| 物品 | 价值 | 重量 | 比率 |
| :--- | :--- | :--- | :--- |
| 1    | 60   | 10   | 6    |
| 2    | 100  | 20   | 5    |
| 3    | 120  | 30   | 4    |

贪心规则：最大价值/重量比率
可视化：按比率降序选择物品 → 1, 2, 3 的一部分。

#### 微型代码（Python）

```python
def greedy_choice(items, key):
    items = sorted(items, key=key, reverse=True)
    chosen = []
    for it in items:
        if valid(it, chosen):
            chosen.append(it)
    return chosen
```

通过在每次迭代中记录或绘图，您可以可视化解决方案是如何增长的。

#### 为什么重要

-   直观展示局部与全局的权衡
-   确认贪心选择性质（局部最优 = 全局最优）
-   帮助诊断贪心失败的情况（路径偏离最优解的地方）
-   加深对问题结构的理解

#### 一个温和的证明（为什么有效）

贪心算法在以下条件下有效：

1.  **贪心选择性质**：局部最优可以导向全局最优。
2.  **最优子结构**：整体最优解包含其子问题的最优解。

可视化有助于验证这些条件。如果每个选择的步骤都留下一个更小的问题，而该问题仍然可以用相同的规则最优求解，那么算法就是正确的。

#### 亲自尝试

1.  画出区间并应用最早结束贪心规则。
2.  可视化硬币找零问题的硬币选择过程。
3.  尝试一个贪心算法失败的例子（例如，硬币集合 {4,3,1}）。
4.  绘制选择顺序以观察与最优解的偏离。

#### 测试用例

| 问题                     | 贪心规则                   | 有效？ | 备注                     |
| :----------------------- | :------------------------- | :----- | :----------------------- |
| 区间调度                 | 最早结束时间               | 是     | 最优                     |
| 分数背包问题             | 最大比率                   | 是     | 连续分数                 |
| 硬币找零 (25,10,5,1)     | 最大硬币 ≤ 剩余金额        | 是     | 规范硬币系统             |
| 硬币找零 (4,3,1)         | 最大硬币 ≤ 剩余金额        | 否     | 非规范硬币系统           |

#### 复杂度

| 步骤           | 时间复杂度 | 空间复杂度 |
| :------------- | :--------- | :--------- |
| 元素排序       | O(n log n) | O(n)       |
| 选择循环       | O(n)       | O(1)       |

贪心选择可视化将抽象的决策逻辑转化为一幅图景，一条时间线或路径，清晰地展示了局部选择如何展开并通向（或偏离）全局目标。
### 40 摊还合并演示

摊还合并演示说明了昂贵的操作在长序列上平均后如何显得廉价。通过分析所有步骤的总成本，我们揭示了为什么某些偶尔进行繁重工作的算法整体上仍然运行高效。

#### 我们正在解决什么问题？

某些数据结构或算法偶尔会执行代价高昂的操作（例如合并数组、调整表大小或重建堆）。如果我们只关注每个步骤的最坏情况时间，它们看起来效率低下，但摊还分析表明，经过多次操作后，每次操作的*平均*成本仍然很低。

这种方法解释了为什么动态数组、并查集和增量合并仍然保持高效。

#### 它是如何工作的（通俗解释）

1.  执行一系列操作 ( O_1, O_2, \ldots, O_n )。
2.  有些操作是廉价的（常数时间），有些是昂贵的（线性或对数时间）。
3.  计算所有 ( n ) 次操作的总成本。
4.  将总成本除以 ( n ) → 每次操作的摊还成本。

摊还分析告诉我们：
$$
\text{摊还成本} = \frac{\text{序列总成本}}{n}
$$

即使少数操作是昂贵的，它们的成本也被"分摊"到了许多廉价操作上。

#### 逐步示例

示例：动态数组加倍

假设每次数组满时，我们将其容量加倍。

| 操作          | 容量 | 实际成本 | 总元素数 | 累计成本 |
| ------------- | ---- | -------- | -------- | -------- |
| 插入 1–1      | 1    | 1        | 1        | 1        |
| 插入 2–2      | 2    | 2        | 2        | 3        |
| 插入 3–4      | 4    | 3        | 3        | 6        |
| 插入 4–4      | 4    | 1        | 4        | 7        |
| 插入 5–8      | 8    | 5        | 5        | 12       |
| 插入 6–8      | 8    | 1        | 6        | 13       |
| 插入 7–8      | 8    | 1        | 7        | 14       |
| 插入 8–8      | 8    | 1        | 8        | 15       |
| 插入 9–16     | 16   | 9        | 9        | 24       |

总成本（9 次插入）= 24
摊还成本 = 24 / 9 ≈ 2.67 ≈ O(1)

因此，尽管有些插入操作成本为 O(n)，但每次插入的平均成本 = O(1)。

示例：并查集中的摊还合并

合并集合时，总是将较小的树附加到较大的树上。
每个元素的深度最多增加 O(log n) 次 → 总成本 O(n log n)。

#### 微型代码（Python）

```python
def dynamic_array_append(arr, x, capacity):
    if len(arr) == capacity:
        capacity *= 2  # 容量加倍
        arr.extend([None]*(capacity - len(arr)))  # 复制成本 = len(arr)
    arr[len(arr)//2] = x
    return arr, capacity
```

这段代码模拟了容量加倍，其中复制成本 = 当前数组大小。

#### 为什么这很重要

-   解释了调整结构大小背后的隐藏效率
-   说明了为什么偶尔的性能峰值不会破坏整体性能
-   是分析栈、队列、哈希表的基础
-   为理解摊还 O(1) 操作建立直觉

#### 一个温和的证明（为什么它有效）

考虑动态数组大小调整：

-   每次容量加倍时，每个元素最多被移动一次。
-   经过 n 次插入，总复制次数 ≤ 2n。

因此，
$$
\text{总成本} = O(n) \implies \text{摊还成本} = O(1)
$$

这里使用了摊还分析的聚合方法：

$$
\text{每次操作的摊还成本} =
\frac{\text{总工作量}}{\text{操作次数}}
$$

#### 自己动手试试

1.  模拟向一个容量加倍的数组中插入 10 个元素。
2.  跟踪总复制成本。
3.  绘制实际成本与摊还成本的对比图。
4.  使用三倍增长因子重复实验，比较平均成本。

#### 测试用例

| 操作类型       | 成本模型                 | 摊还成本   | 备注                       |
| -------------- | ------------------------ | ---------- | -------------------------- |
| 数组加倍       | 复制 + 插入              | O(1)       | 分摊成本                   |
| 并查集合并     | 将较小的附加到较大的     | O(α(n))    | α = 反阿克曼函数           |
| 栈压入         | 偶尔调整大小             | O(1)       | 平均常数时间               |
| 队列入队       | 循环缓冲区               | O(1)       | 循环复用                   |

#### 复杂度

| 步骤           | 最坏情况 | 摊还情况 | 空间   |
| -------------- | -------- | -------- | ------ |
| 单次插入       | O(n)     | O(1)     | O(n)   |
| n 次插入       | O(n)     | O(n)     | O(n)   |

摊还合并演示揭示了算法混沌之下的平静，即使某些步骤代价高昂，长期的节奏依然保持平稳、可预测和高效。

## 第 5 节. 递归关系
### 41 线性递推求解器

线性递推求解器用于寻找由先前值定义的序列的闭式解或迭代解。它将诸如 $T(n) = aT(n-1) + b$ 的递归定义转化为显式公式，帮助我们理解算法的增长。

#### 我们要解决什么问题？

许多算法，尤其是递归算法，通过递推关系来定义运行时间，例如：

$$
T(n) = a , T(n-1) + b
$$

为了推理复杂度或计算精确值，我们希望求解这个递推关系，将其从自引用的定义转换为关于 $n$ 的直接表达式。

这个求解器提供了一种系统的方法来完成此事。

#### 工作原理（通俗解释）

线性递推的一般形式为：

$$
T(n) = a_1T(n-1) + a_2T(n-2) + \cdots + a_kT(n-k) + f(n)
$$

1.  识别系数 ($a_1, a_2, \ldots$)。
2.  为齐次部分写出特征方程。
3.  求解根 ($r_1, r_2, \ldots$)。
4.  利用这些根构造齐次解。
5.  如果 $f(n)$ 非零，则加上一个特解。
6.  应用初始条件来确定常数。

#### 逐步示例

示例 1：
$$
T(n) = 2T(n-1) + 3, \quad T(0) = 1
$$

1.  齐次部分：$T(n) - 2T(n-1) = 0$
    → 特征根：$r = 2$
    → 齐次解：$T_h(n) = C \cdot 2^n$

2.  特解：常数 $p$
    代入：$p = 2p + 3 \implies p = -3$

3.  通解：
    $$
    T(n) = C \cdot 2^n - 3
    $$

4.  应用 $T(0)=1$：
    $1 = C - 3 \implies C = 4$

✅ 最终结果：
$$
T(n) = 4 \cdot 2^n - 3
$$

示例 2 (Fibonacci)：

$$
F(n) = F(n-1) + F(n-2), \quad F(0)=0, F(1)=1
$$

特征方程：
$$
r^2 - r - 1 = 0
$$

根：
$$
r_1 = \frac{1+\sqrt{5}}{2}, \quad r_2 = \frac{1-\sqrt{5}}{2}
$$

通解：
$$
F(n) = A r_1^n + B r_2^n
$$

求解常数得到 Binet 公式：
$$
F(n) = \frac{1}{\sqrt{5}}\left[\left(\frac{1+\sqrt{5}}{2}\right)^n - \left(\frac{1-\sqrt{5}}{2}\right)^n\right]
$$

#### 微型代码 (Python)

```python
def linear_recurrence(a, b, n, t0):
    T = [t0]
    for i in range(1, n + 1):
        T.append(a * T[i - 1] + b)
    return T
```

这段代码模拟了一个简单的一阶递推关系，如 $T(n) = aT(n-1) + b$。

#### 为何重要

-   将递归定义转换为显式公式
-   帮助分析递归算法的时间复杂度
-   连接了数学与算法设计
-   用于动态规划状态转移、计数问题和算法分析

#### 一个温和的证明（为何有效）

展开 $T(n) = aT(n-1) + b$：

$$
T(n) = a^nT(0) + b(a^{n-1} + a^{n-2} + \cdots + 1)
$$

和为等比数列：
$$
T(n) = a^nT(0) + b \frac{a^n - 1}{a - 1}
$$

因此闭式解为：
$$
T(n) = a^nT(0) + \frac{b(a^n - 1)}{a - 1}
$$

这与常系数特征方程的方法是一致的。

#### 动手尝试

1.  求解 $T(n) = 3T(n-1) + 2, , T(0)=1$
2.  求解 $T(n) = 2T(n-1) - T(n-2)$
3.  将数值结果与迭代模拟进行比较
4.  绘制递归树以确认增长趋势

#### 测试用例

| 递推关系           | 初始条件 | 解                  | 增长阶   |
| ------------------ | -------- | ------------------- | -------- |
| $T(n)=2T(n-1)+3$   | $T(0)=1$ | $4 \cdot 2^n - 3$   | $O(2^n)$ |
| $T(n)=T(n-1)+1$    | $T(0)=0$ | $n$                 | $O(n)$   |
| $T(n)=3T(n-1)$     | $T(0)=1$ | $3^n$               | $O(3^n)$ |

#### 复杂度

| 方法               | 时间   | 空间   |
| ------------------ | ------ | ------ |
| 递归（展开）       | O(n)   | O(n)   |
| 闭式解             | O(1)   | O(1)   |

线性递推求解器将重复的依赖关系转化为显式的增长，揭示了每个递归步骤背后隐藏的模式。
### 42 主定理

主定理提供了一种直接分析分治递归式的方法，使您无需展开或猜测就能找到渐近界。它是理解递归算法（如归并排序、二分查找和 Strassen 矩阵乘法）的基石工具。

#### 我们要解决什么问题？

许多递归算法可以表示为：

$$
T(n) = a , T!\left(\frac{n}{b}\right) + f(n)
$$

其中：

- $a$：子问题的数量
- $b$：缩减因子（每个子问题的大小）
- $f(n)$：递归之外额外的工作（合并、划分等）

我们希望通过比较递归成本（$n^{\log_b a}$）与非递归成本（$f(n)$）来找到 $T(n)$ 的渐近表达式。

#### 工作原理（通俗解释）

1.  将递归式写成标准形式：
   $$
   T(n) = a , T(n/b) + f(n)
   $$
2.  计算关键指数 $\log_b a$。
3.  比较 $f(n)$ 与 $n^{\log_b a}$：

   *   如果 $f(n)$ 更小，则递归部分占主导。
   *   如果它们相等，则两部分贡献相同。
   *   如果 $f(n)$ 更大，则外部工作占主导。

该定理根据哪一项增长更快，给出了三种标准情况。
### 三种情况

情况 1（递归工作占主导）：

如果对于某个 $\varepsilon > 0$，有
$$
f(n) = O(n^{\log_b a - \varepsilon})
$$
那么
$$
T(n) = \Theta(n^{\log_b a})
$$

情况 2（工作平衡）：

如果
$$
f(n) = \Theta(n^{\log_b a})
$$
那么
$$
T(n) = \Theta(n^{\log_b a} \log n)
$$

情况 3（非递归工作占主导）：

如果
$$
f(n) = \Omega(n^{\log_b a + \varepsilon})
$$
并且对于某个常数 $c < 1$，有
$$
a , f(n/b) \le c , f(n)
$$
那么
$$
T(n) = \Theta(f(n))
$$

#### 逐步示例

示例 1：归并排序

$$
T(n) = 2T(n/2) + O(n)
$$

- $a = 2$, $b = 2$, 所以 $\log_b a = 1$
- $f(n) = O(n)$
- $f(n) = \Theta(n^{\log_b a})$ → 情况 2

结果：
$$
T(n) = \Theta(n \log n)
$$

示例 2：二分查找

$$
T(n) = T(n/2) + O(1)
$$

- $a = 1$, $b = 2$, 所以 $\log_b a = 0$
- $f(n) = O(1) = \Theta(n^0)$ → 情况 2

结果：
$$
T(n) = \Theta(\log n)
$$

示例 3：Strassen 矩阵乘法

$$
T(n) = 7T(n/2) + O(n^2)
$$

- $a = 7$, $b = 2$, 所以 $\log_2 7 \approx 2.81$
- $f(n) = O(n^2) = O(n^{2.81 - \varepsilon})$ → 情况 1

结果：
$$
T(n) = \Theta(n^{\log_2 7})
$$

#### 微型代码 (Python)

```python
import math

def master_theorem(a, b, f_exp):
    log_term = math.log(a, b)
    if f_exp < log_term:
        return f"Theta(n^{round(log_term, 2)})"
    elif abs(f_exp - log_term) < 1e-9:
        return f"Theta(n^{round(log_term, 2)} * log n)"
    else:
        return f"Theta(n^{f_exp})"
```

这个辅助函数通过比较指数来近似结果。

#### 为何重要

- 将递归定义转换为渐近形式
- 避免重复代入或树展开
- 适用于大多数分治算法
- 阐明何时合并工作占主导或不占主导

#### 一个温和的证明（为何有效）

展开递推式：

$$
T(n) = aT(n/b) + f(n)
$$

经过 $k$ 层后：

$$
T(n) = a^k T(n/b^k) + \sum_{i=0}^{k-1} a^i f(n/b^i)
$$

递归深度：$k = \log_b n$

现在比较每层的总成本与 $n^{\log_b a}$：

- 如果 $f(n)$ 增长更慢，顶层占主导 → 情况 1
- 如果相等，所有层都有贡献 → 情况 2
- 如果更快，底层占主导 → 情况 3

渐近结果取决于求和中的哪个分量占主导。

#### 亲自尝试

1. 求解 $T(n) = 3T(n/2) + n$
2. 求解 $T(n) = 4T(n/2) + n^2$
3. 绘制递归树并检查哪一项占主导

#### 测试用例

| 递推式                | 情况   | 解                         |
| --------------------- | ------ | -------------------------- |
| $T(n)=2T(n/2)+n$      | 情况 2 | $\Theta(n \log n)$         |
| $T(n)=T(n/2)+1$       | 情况 2 | $\Theta(\log n)$           |
| $T(n)=7T(n/2)+n^2$    | 情况 1 | $\Theta(n^{\log_2 7})$     |
| $T(n)=2T(n/2)+n^2$    | 情况 3 | $\Theta(n^2)$              |

#### 复杂度总结

| 分量         | 表达式                     | 解释                           |
| ------------ | -------------------------- | ------------------------------ |
| 递归工作     | $n^{\log_b a}$             | 跨递归调用的工作               |
| 合并工作     | $f(n)$                     | 每层的工作                     |
| 总成本       | $\max(n^{\log_b a}, f(n))$ | 主导项决定增长                 |

主定理（Master Theorem）为分析递归算法提供了一个蓝图，一旦递推式符合标准形式，其复杂度只需通过简单的比较即可得出。
### 43 代入法

代入法是一种通过猜测解并用归纳法证明来系统性证明递归式渐近界的方法。它是验证时间复杂度最灵活的技术之一。

#### 我们要解决什么问题？

许多算法是递归定义的，例如：

$$
T(n) = 2T(n/2) + n
$$

我们通常希望证明 $T(n) = O(n \log n)$ 或 $T(n) = \Theta(n^2)$。
但在应用定理之前，我们必须确认我们的猜测是合适的。

代入法提供了一个证明框架：

1.  猜测渐近界。
2.  用归纳法证明它。
3.  必要时调整常数。

#### 工作原理（通俗解释）

1.  为 $T(n)$ 做出一个猜测，通常基于已知的模式。
    例如，对于 $T(n) = 2T(n/2) + n$，猜测 $T(n) = O(n \log n)$。
2.  写出归纳假设：
    假设对于所有 $k < n$，有 $T(k) \le c , k \log k$。
3.  代入递归式：
    用假设替换递归项。
4.  简化并验证：
    证明不等式对 $n$ 成立，必要时调整常数。
5.  得出结论：猜测是有效的。

#### 逐步示例

示例 1：

$$
T(n) = 2T(n/2) + n
$$

目标：证明 $T(n) = O(n \log n)$

1.  假设：
    对于所有 $k < n$，$T(k) \le c , k \log k$

2.  代入：
    $T(n) \le 2[c(n/2)\log(n/2)] + n$

3.  简化：
    $= c n \log(n/2) + n$
    $= c n (\log n - 1) + n$
    $= c n \log n - c n + n$

4.  调整常数：
    如果 $c \ge 1$，那么 $-cn + n \le 0$，所以
    $T(n) \le c n \log n$

✅ 因此，$T(n) = O(n \log n)$。

示例 2：

$$
T(n) = 3T(n/2) + n
$$

猜测：$T(n) = O(n^{\log_2 3})$

1.  假设：$T(k) \le c , k^{\log_2 3}$
2.  代入：
    $T(n) \le 3c(n/2)^{\log_2 3} + n = 3c \cdot n^{\log_2 3} / 3 + n = c n^{\log_2 3} + n$
3.  主导项：$n^{\log_2 3}$
    ✅ $T(n) = O(n^{\log_2 3})$

#### 微型代码（Python）

```python
def substitution_check(a, b, f_exp, guess_exp):
    from math import log
    lhs = a * (1 / b)  guess_exp
    rhs = 1
    if lhs < 1:
        return f"Guess n^{guess_exp} holds (Case 1)"
    elif abs(lhs - 1) < 1e-9:
        return f"Guess n^{guess_exp} log n (Case 2)"
    else:
        return f"Guess n^{guess_exp} fails (try larger exponent)"
```

帮助验证猜测的指数是否适合递归式。

#### 为什么它重要

*   建立基于证明的复杂度理解
*   确认来自直觉或主定理的渐近界
*   即使主定理失效（不规则形式）也适用
*   强化递归与增长率之间的联系

#### 一个温和的证明（为什么它有效）

令 $T(n) = aT(n/b) + f(n)$
猜测 $T(n) = O(n^{\log_b a})$。

归纳步骤：
$$
T(n) = aT(n/b) + f(n) \le a(c(n/b)^{\log_b a}) + f(n)
$$
$$
= c n^{\log_b a} + f(n)
$$

如果 $f(n)$ 增长更慢，通过选择足够大的 $c$，$T(n)$ 仍然是 $O(n^{\log_b a})$。

#### 自己试试

1.  证明 $T(n) = 2T(n/2) + n^2 = O(n^2)$
2.  证明 $T(n) = T(n-1) + 1 = O(n)$
3.  调整常数使归纳成立

#### 测试用例

| 递归式               | 猜测                  | 结果   |
| -------------------- | --------------------- | ------ |
| $T(n)=2T(n/2)+n$     | $O(n\log n)$          | 正确   |
| $T(n)=T(n-1)+1$      | $O(n)$                | 正确   |
| $T(n)=3T(n/2)+n$     | $O(n^{\log_2 3})$     | 正确   |

#### 复杂度总结

| 方法       | 工作量     | 何时使用                     |
| ---------- | ---------- | ---------------------------- |
| 主定理     | 快速       | 标准的分治算法               |
| 代入法     | 中等       | 自定义或不规则的递归式       |
| 迭代法     | 详细       | 逐步展开                     |

代入法融合了直觉与严谨性：你做出一个好的猜测，代数运算完成剩下的工作。
### 44 迭代法

迭代法（也称为递归展开法）通过反复代入递归项来求解递归式，直到模式变得清晰。这是一种推导闭式解或渐近解的结构化方法。

#### 我们要解决什么问题？

递归算法通常根据更小实例来定义其运行时间：

$$
T(n) = a , T(n/b) + f(n)
$$

与猜测或应用定理不同，迭代法逐层展开递归式，清晰地展示成本如何在递归层级间累积。

当 $f(n)$ 遵循可识别的模式（如线性、二次或对数函数）时，此方法尤其有用。

#### 工作原理（通俗解释）

1.  写下递归式：

    $$
    T(n) = a , T(n/b) + f(n)
    $$

2.  逐层展开：

    $$
    T(n) = a[a , T(n/b^2) + f(n/b)] + f(n)
    $$

    $$
    = a^2 T(n/b^2) + a f(n/b) + f(n)
    $$

3.  继续展开 $k$ 层，直到子问题规模变为 1：

    $$
    T(n) = a^k T(n/b^k) + \sum_{i=0}^{k-1} a^i f(n/b^i)
    $$

4.  当 $n/b^k = 1$ 时，我们有 $k = \log_b n$。
5.  代入 $k = \log_b n$ 以找到闭式解或渐近界。

#### 逐步示例

示例 1：归并排序

$$
T(n) = 2T(n/2) + n
$$

步骤 1：展开

[
\begin{aligned}
T(n) &= 2T(n/2) + n \
&= 2[2T(n/4) + n/2] + n \
&= 4T(n/4) + 2n \
&= 8T(n/8) + 3n \
&\cdots \
&= 2^k T(n/2^k) + k n
\end{aligned}
]

步骤 2：基本情况

当 $n/2^k = 1 \implies k = \log_2 n$

因此：
$$
T(n) = n \cdot T(1) + n \log_2 n = O(n \log n)
$$

✅ $T(n) = \Theta(n \log n)$

示例 2：二分查找

$$
T(n) = T(n/2) + 1
$$

展开：

[
\begin{aligned}
T(n) &= T(n/2) + 1 \
&= T(n/4) + 2 \
&= T(n/8) + 3 \
&\cdots \
&= T(1) + \log_2 n
\end{aligned}
]

✅ $T(n) = O(\log n)$

示例 3：线性递归

$$
T(n) = T(n-1) + 1
$$

展开：

$$
T(n) = T(n-1) + 1 = T(n-2) + 2 = \cdots = T(1) + (n-1)
$$

✅ $T(n) = O(n)$

#### 微型代码（Python）

```python
def iterate_recurrence(a, b, f, n):
    total = 0
    level = 0
    while n > 1:
        total += (a  level) * f(n / (b  level))
        n /= b
        level += 1
    return total
```

这段代码演示了逐层的求和过程。

#### 为何重要

-   使递归过程可视化且透明
-   适用于不规则的 $f(n)$（当主定理不适用时）
-   推导精确的和，而不仅仅是渐近界
-   建立对递归树和对数深度的直觉

#### 一个温和的证明（为何有效）

递归的每一层 $i$ 贡献：

$$
a^i \cdot f(n/b^i)
$$

总层数：
$$
\log_b n
$$

因此总成本：
$$
T(n) = \sum_{i=0}^{\log_b n - 1} a^i f(n/b^i)
$$

根据 $f(n)$ 的增长率，可以使用标准的求和技术来近似或界定这个和。

#### 动手试试

1.  求解 $T(n) = 3T(n/2) + n^2$
2.  求解 $T(n) = 2T(n/2) + n \log n$
3.  求解 $T(n) = T(n/2) + n/2$
4.  与主定理的结果进行比较

#### 测试用例

| 递归式               | 解          | 增长率          |
| -------------------- | ----------- | --------------- |
| $T(n)=2T(n/2)+n$     | $n \log n$  | $O(n \log n)$   |
| $T(n)=T(n/2)+1$      | $\log n$    | $O(\log n)$     |
| $T(n)=T(n-1)+1$      | $n$         | $O(n)$          |
| $T(n)=3T(n/2)+n^2$   | $n^2$       | $O(n^2)$        |

#### 复杂度总结

| 步骤       | 时间复杂度       | 空间复杂度               |
| ---------- | ---------------- | ------------------------ |
| 展开       | $O(\log n)$ 层   | 栈深度 $O(\log n)$       |
| 求和       | 取决于 $f(n)$    | 通常是几何级数或算术级数 |

迭代法将递归解包为层层工作，将递归式转化为具体的和，再将和转化为清晰的复杂度界。
### 45 生成函数法

生成函数法通过将序列编码为幂级数，将递推关系转化为代数方程。转换后，通过代数运算可以得到闭式表达式或渐近近似。

#### 我们要解决什么问题？

递推关系递归地定义了一个序列 $T(n)$：

$$
T(n) = a_1 T(n-1) + a_2 T(n-2) + \cdots + f(n)
$$

我们希望找到一个闭式公式，而不是一步一步地计算。
通过将 $T(n)$ 表示为幂级数中的系数，我们可以使用代数工具来清晰地求解递推关系，特别是常系数线性递推关系。

#### 工作原理（通俗解释）

1.  定义生成函数
   令
   $$
   G(x) = \sum_{n=0}^{\infty} T(n) x^n
   $$

2.  将递推关系乘以 $x^n$ 并对所有 $n$ 求和。

3.  利用求和的性质（平移下标、提取常数因子）用 $G(x)$ 重写表达式。

4.  求解关于 $G(x)$ 的代数方程。

5.  将 $G(x)$ 展开回幂级数形式（使用部分分式或已知展开式）。

6.  提取 $x^n$ 的系数作为 $T(n)$。

#### 分步示例

示例 1：斐波那契数列

$$
T(n) = T(n-1) + T(n-2), \quad T(0) = 0, ; T(1) = 1
$$

步骤 1：定义生成函数

$$
G(x) = \sum_{n=0}^{\infty} T(n) x^n
$$

步骤 2：将递推关系乘以 $x^n$ 并对 $n \ge 2$ 求和：

$$
\sum_{n=2}^{\infty} T(n) x^n = \sum_{n=2}^{\infty} T(n-1)x^n + \sum_{n=2}^{\infty} T(n-2)x^n
$$

步骤 3：利用平移重写：

$$
G(x) - T(0) - T(1)x = x(G(x) - T(0)) + x^2 G(x)
$$

代入初始值 $T(0)=0, T(1)=1$：

$$
G(x) - x = xG(x) + x^2 G(x)
$$

步骤 4：求解 $G(x)$：

$$
G(x)(1 - x - x^2) = x
$$

所以：

$$
G(x) = \frac{x}{1 - x - x^2}
$$

步骤 5：使用部分分式展开以获得系数：

$$
T(n) = \frac{1}{\sqrt{5}} \left[\left(\frac{1+\sqrt{5}}{2}\right)^n - \left(\frac{1-\sqrt{5}}{2}\right)^n\right]
$$

✅ 直接推导出 Binet 公式。

示例 2：$T(n) = 2T(n-1) + 3$，$T(0)=1$

令 $G(x) = \sum_{n=0}^{\infty} T(n)x^n$

将递推关系乘以 $x^n$ 并对 $n \ge 1$ 求和：

$$
G(x) - T(0) = 2xG(x) + 3x \cdot \frac{1}{1-x}
$$

简化：

$$
G(x)(1 - 2x) = 1 + \frac{3x}{1-x}
$$

求解并使用部分分式展开 → 恢复闭式：

$$
T(n) = 4 \cdot 2^n - 3
$$

#### 微型代码（Python）

```python
from sympy import symbols, Function, Eq, rsolve

n = symbols('n', integer=True)
T = Function('T')
recurrence = Eq(T(n), 2*T(n-1) + 3)
solution = rsolve(recurrence, T(n), {T(0): 1})
print(solution)  # 4*2n - 3
```

使用 `sympy.rsolve` 符号化地计算闭式。

#### 重要性

*   将递推关系转化为代数方程
*   揭示精确的闭式，而不仅仅是渐近形式
*   适用于非齐次和常系数递推关系
*   连接组合数学、离散数学和算法分析

#### 一个温和的证明（为什么它有效）

给定线性递推关系：

$$
T(n) - a_1T(n-1) - \cdots - a_kT(n-k) = f(n)
$$

乘以 $x^n$ 并从 $n=k$ 到 $\infty$ 求和：

$$
\sum_{n=k}^{\infty} T(n)x^n = a_1x \sum_{n=k}^{\infty} T(n-1)x^{n-1} + \cdots + f(x)
$$

利用下标平移，每一项都可以用 $G(x)$ 表示，从而得到：

$$
P(x)G(x) = Q(x)
$$

其中 $P(x)$ 和 $Q(x)$ 是多项式。求解 $G(x)$ 即可得到序列的结构。

#### 动手试试

1.  求解 $T(n)=3T(n-1)-2T(n-2)$，初始条件为 $T(0)=2, T(1)=3$。
2.  如果 $T(n)=T(n-1)+1$，$T(0)=0$，求 $T(n)$。
3.  将你的生成函数与展开式进行比较。

#### 测试用例

| 递推关系               | 闭式形式                 | 增长阶      |
| ---------------------- | ------------------------ | ----------- |
| $T(n)=2T(n-1)+3$       | $4 \cdot 2^n - 3$        | $O(2^n)$    |
| $T(n)=T(n-1)+1$        | $n$                      | $O(n)$      |
| $T(n)=T(n-1)+T(n-2)$   | $\text{Binet's Formula}$ | $O(\phi^n)$ |

#### 复杂度总结

| 步骤           | 类型                 | 复杂度     |
| -------------- | -------------------- | ---------- |
| 转换           | 代数运算             | O(k) 项    |
| 求解           | 符号化（通过求根）   | O(k^3)     |
| 求值           | 闭式                 | O(1)       |

生成函数法将递推关系转化为代数，求和变为方程，而方程则产生精确的公式。
### 46 矩阵快速幂

矩阵快速幂方法将线性递推关系转化为矩阵形式，利用快速幂算法在 $O(\log n)$ 时间内高效计算项。它非常适合处理斐波那契数列、三阶斐波那契数列以及许多动态规划状态转移问题。

#### 我们要解决什么问题？

许多递推关系遵循前几项之间的线性关系，例如：

$$
T(n) = a_1 T(n-1) + a_2 T(n-2) + \cdots + a_k T(n-k)
$$

朴素地计算 $T(n)$ 需要 $O(n)$ 步。
通过将此递推关系编码到矩阵中，我们可以通过幂运算高效地计算 $T(n)$，将运行时间减少到 $O(k^3 \log n)$。

#### 工作原理（通俗解释）

1.  将递推关系表示为矩阵乘法。
2.  构造转移矩阵 $M$，它将状态从 $n-1$ 移动到 $n$。
3.  使用快速幂（分治法）计算 $M^n$。
4.  将 $M^n$ 乘以初始向量以获得 $T(n)$。

这种方法可以很好地推广到任何具有常系数的线性齐次递推关系。

#### 逐步示例

示例 1：斐波那契数列

$$
F(n) = F(n-1) + F(n-2)
$$

定义状态向量：

$$
\begin{bmatrix}
F(n) \\
F(n-1)
\end{bmatrix}
=
\begin{bmatrix}
1 & 1 \\
1 & 0
\end{bmatrix}
\begin{bmatrix}
F(n-1) \\
F(n-2)
\end{bmatrix}
$$

所以：

$$
M =
\begin{bmatrix}
1 & 1 [6pt]
1 & 0
\end{bmatrix}
$$

因此：

$$
\begin{bmatrix}
F(n) \ F(n-1)
\end{bmatrix}
= M^{n-1}
\begin{bmatrix}
F(1) \ F(0)
\end{bmatrix}
$$

给定 $F(1)=1, F(0)=0$，
$$
F(n) = (M^{n-1})_{0,0}
$$

示例 2：二阶递推关系

$$
T(n) = 2T(n-1) + 3T(n-2)
$$

矩阵形式：

$$
\begin{bmatrix}
T(n) \\[4pt]
T(n-1)
\end{bmatrix}
=
\begin{bmatrix}
2 & 3 \\[4pt]
1 & 0
\end{bmatrix}
\begin{bmatrix}
T(n-1) \\[4pt]
T(n-2)
\end{bmatrix}
$$

所以：

$$
\vec{T}(n) = M^{n-2} \vec{T}(2)
$$

#### 微型代码（Python）

```python
def mat_mult(A, B):
    return [[sum(A[i][k] * B[k][j] for k in range(len(A)))
             for j in range(len(B[0]))] for i in range(len(A))]

def mat_pow(M, n):
    if n == 1:
        return M
    if n % 2 == 0:
        half = mat_pow(M, n // 2)
        return mat_mult(half, half)
    else:
        return mat_mult(M, mat_pow(M, n - 1))

def fib_matrix(n):
    if n == 0:
        return 0
    M = [[1, 1], [1, 0]]
    Mn = mat_pow(M, n - 1)
    return Mn[0][0]
```

`fib_matrix(n)` 以 $O(\log n)$ 的时间复杂度计算 $F(n)$。

#### 为什么它很重要

-   将递归计算转化为线性代数运算
-   使得 $T(n)$ 的计算复杂度为 $O(\log n)$
-   可推广到高阶递推关系
-   常见于动态规划状态转移、类斐波那契数列和组合计数问题

#### 一个温和的证明（为什么它有效）

递推关系：

$$
T(n) = a_1T(n-1) + a_2T(n-2) + \cdots + a_kT(n-k)
$$

可以表示为：

$$
\vec{T}(n) = M \cdot \vec{T}(n-1)
$$

其中 $M$ 是伴随矩阵：

$$
M =
\begin{bmatrix}
a_1 & a_2 & \cdots & a_k \\
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & & \ddots & 0
\end{bmatrix}
$$

重复相乘得到：

$$
\vec{T}(n) = M^{n-k} \vec{T}(k)
$$

因此，$T(n)$ 可以通过计算 $M$ 的幂来得到，指数级的递归变成了对数级的乘法运算。

#### 动手试试

1.  为 $T(n)=3T(n-1)-2T(n-2)$ 写出矩阵形式
2.  给定 $T(0)=2$, $T(1)=3$，计算 $T(10)$
3.  为 $3\times3$ 矩阵（三阶斐波那契）实现矩阵快速幂
4.  与迭代解法比较运行时间

#### 测试用例

| 递推关系                      | 矩阵                                                                 | $T(n)$ / 符号 | 复杂度      |
| ----------------------------- | -------------------------------------------------------------------- | ------------- | ----------- |
| $F(n)=F(n-1)+F(n-2)$          | $\begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix}$                       | $F(n)$        | $O(\log n)$ |
| $T(n)=2T(n-1)+3T(n-2)$        | $\begin{bmatrix} 2 & 3 \\ 1 & 0 \end{bmatrix}$                       | $T(n)$        | $O(\log n)$ |
| $T(n)=T(n-1)+T(n-2)+T(n-3)$   | $\begin{bmatrix} 1 & 1 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}$  | Tribonacci    | $O(\log n)$ |

#### 复杂度总结

| 步骤                 | 时间复杂度      | 空间复杂度 |
| -------------------- | --------------- | ---------- |
| 矩阵快速幂           | $O(k^3 \log n)$ | $O(k^2)$   |
| 迭代递推             | $O(n)$          | $O(k)$     |

矩阵快速幂将递推求解转化为矩阵求幂，架起了递归与线性代数之间的桥梁，以数学的优雅实现了指数级的加速。
### 47 从递推式到动态规划表

从递推式到动态规划表的方法，将递归关系转换为基于迭代的表格方法，消除了冗余计算，并将效率从指数级提升到多项式级。这是动态规划的基石。

#### 我们正在解决什么问题？

递归公式经常会重复计算重叠的子问题。例如：

$$
T(n) = T(n-1) + T(n-2)
$$

朴素的递归调用树会呈指数级增长，因为它会多次重新计算 $T(k)$。
通过将这个递推式转换为动态规划表，我们只需计算每个子问题一次并存储结果，从而实现线性或多项式时间复杂度。

#### 工作原理（通俗解释）

1.  识别递推式和基本情况。
2.  创建一个表（数组或矩阵）来存储子问题的结果。
3.  使用递推公式迭代地填充表格。
4.  从最后一个单元格读取最终答案。

这种技术被称为制表法，是动态规划的一种自底向上形式。

#### 逐步示例

示例 1：斐波那契数列

递归公式：

$$
F(n) = F(n-1) + F(n-2), \quad F(0)=0, , F(1)=1
$$

动态规划版本：

| n    | 0 | 1 | 2 | 3 | 4 | 5 |
| ---- | - | - | - | - | - | - |
| F(n) | 0 | 1 | 1 | 2 | 3 | 5 |

算法：

1.  初始化基本情况：`F[0]=0`, `F[1]=1`
2.  从 2 循环到 n：`F[i] = F[i-1] + F[i-2]`
3.  返回 `F[n]`

示例 2：零钱兑换（计算方式数）

递推式：
$$
\text{ways}(n, c) = \text{ways}(n, c-1) + \text{ways}(n-\text{coin}[c], c)
$$

转换为按 (n, c) 索引的二维动态规划表。

示例 3：网格路径

递推式：
$$
P(i,j) = P(i-1,j) + P(i,j-1)
$$

动态规划表：

| i\j | 0 | 1 | 2 |
| --- | - | - | - |
| 0   | 1 | 1 | 1 |
| 1   | 1 | 2 | 3 |
| 2   | 1 | 3 | 6 |

每个单元格 = 上方单元格与左侧单元格之和。

#### 微型代码（Python）

```python
def fib_dp(n):
    if n == 0:
        return 0
    dp = [0] * (n + 1)
    dp[1] = 1
    for i in range(2, n + 1):
        dp[i] = dp[i - 1] + dp[i - 2]
    return dp[n]
```

#### 为什么这很重要

-   将指数级递归转换为多项式级迭代
-   避免重复计算子问题
-   支持空间和时间优化
-   构成自底向上动态规划的基础

#### 一个温和的证明（为什么它有效）

给定递推式：

$$
T(n) = a_1 T(n-1) + a_2 T(n-2) + \cdots + a_k T(n-k)
$$

每一项仅依赖于先前已计算的子问题。
因此，通过按递增顺序填充表格，我们确保所有依赖项都已准备就绪。

通过归纳法，如果基本情况正确，则每个计算出的单元格都是正确的。

#### 自己动手试试

1.  将 $F(n)=F(n-1)+F(n-2)$ 转换为一个一维动态规划数组
2.  为网格路径 $P(i,j)=P(i-1,j)+P(i,j-1)$ 构建一个二维表
3.  为阶乘 $n! = n \times (n-1)!$ 编写一个动态规划表
4.  优化空间（只保留最后 k 项）

#### 测试用例

| 输入               | 递推式                                                                 | 预期结果 |
| ------------------ | ---------------------------------------------------------------------- | -------- |
| $F(5)$             | $F(n)=F(n-1)+F(n-2)$                                                   | 5        |
| Grid(2,2)          | $P(i,j)=P(i-1,j)+P(i,j-1)$                                             | 6        |
| $n=3, coins=[1,2]$ | $\text{ways}(n,c)=\text{ways}(n,c-1)+\text{ways}(n-\text{coin}[c],c)$ | 2        |

#### 复杂度总结

| 方法               | 时间复杂度 | 空间复杂度 |
| ------------------ | ---------- | ---------- |
| 递归               | $O(2^n)$   | $O(n)$     |
| 动态规划表         | $O(n)$     | $O(n)$     |
| 空间优化的动态规划 | $O(n)$     | $O(1)$     |

将递推式转换为动态规划表，抓住了动态规划的本质：结构、重用以及相较于蛮力重复的清晰性。
### 48 分治模板

分治模板是一种通过将问题分解为更小、相似的子问题，独立解决每个子问题，然后合并它们的结果来解决问题的结构性指导。它是归并排序、快速排序和卡拉楚巴乘法等分治算法背后的核心框架。

#### 我们正在解决什么问题？

许多复杂问题可以分解为自身的较小副本。我们不是一次性解决整个实例，而是将其分解为子问题，递归地解决每个子问题，然后合并它们的结果。

这种方法降低了复杂性，促进了并行性，并产生了如下的递归关系式：

$$
T(n) = aT\left(\frac{n}{b}\right) + f(n)
$$

#### 它是如何工作的（通俗解释）

1.  **分解**：将问题分割成 $a$ 个子问题，每个子问题的大小为 $\frac{n}{b}$。
2.  **解决**：递归地解决子问题。
3.  **合并**：将子问题的结果合并成一个完整的解决方案。
4.  **基本情况**：当子问题变得微不足道地小时，停止分解。

这种递归结构支撑了大多数用于排序、搜索和乘法的高效算法。

#### 逐步示例

**示例 1：归并排序**

-   **分解**：将数组分成两半
-   **解决**：递归地对每一半进行排序
-   **合并**：合并两个已排序的半部分

递归式：
$$
T(n) = 2T\left(\frac{n}{2}\right) + O(n)
$$

**示例 2：卡拉楚巴乘法**

-   将数字分成两半
-   用 3 次递归乘法**解决**
-   使用线性组合**合并**

递归式：
$$
T(n) = 3T\left(\frac{n}{2}\right) + O(n)
$$

**示例 3：二分查找**

-   通过中点**分解**数组
-   对其中一半**解决**
-   **合并**是平凡的（返回结果）

递归式：
$$
T(n) = T\left(\frac{n}{2}\right) + O(1)
$$
### 通用模板（伪代码）

```python
def divide_and_combine(problem):
    if is_small(problem):
        return solve_directly(problem)
    subproblems = divide(problem)
    results = [divide_and_combine(p) for p in subproblems]
    return combine(results)
```

这个通用模板可以适应许多问题领域：数组、树、图、几何和代数。

#### 重要性

- 阐明递归结构和基本情况推理
- 通过递推关系实现渐进分析
- 为并行和缓存高效算法奠定基础
- 促进清晰的分解和可重用性

#### 一个温和的证明（为何有效）

如果一个问题可以分解为独立的子问题，并且其结果可以确定性地合并，那么递归分解就是有效的。
通过归纳法证明：

- 基本情况：小规模输入直接求解。
- 归纳步骤：如果每个子问题都正确求解，并且合并步骤正确合并，则最终解是正确的。

因此，正确性源于结构分解。

#### 动手实践

1.  实现数组的分治求和。
2.  为最大子数组问题（Kadane 的分治形式）编写递归结构。
3.  表达递推关系 $T(n)=2T(n/2)+n$ 并通过主定理求解。
4.  修改模板以支持并行处理（例如，线程池）。

#### 测试用例

| 问题             | 分解               | 合并                         | 复杂度           |
| ---------------- | ------------------ | ---------------------------- | ---------------- |
| 归并排序         | 对半分数组         | 合并已排序的两半             | $O(n \log n)$    |
| 二分查找         | 对半分搜索空间     | 返回结果                     | $O(\log n)$      |
| Karatsuba 乘法   | 拆分数字           | 合并线性部分                 | $O(n^{1.585})$   |
| 最近点对（二维） | 分割点集           | 合并跨边界点对               | $O(n \log n)$    |

#### 复杂度总结

给定：
$$
T(n) = aT\left(\frac{n}{b}\right) + f(n)
$$

根据主定理：

- 如果 $f(n) = O(n^{\log_b a - \epsilon})$，则 $T(n) = \Theta(n^{\log_b a})$
- 如果 $f(n) = \Theta(n^{\log_b a})$，则 $T(n) = \Theta(n^{\log_b a} \log n)$
- 如果 $f(n) = \Omega(n^{\log_b a + \epsilon})$，则 $T(n) = \Theta(f(n))$

分治与合并模板为递归问题求解提供了蓝图，它简单、优雅，并且是所有算法领域的基础。
### 49 记忆化递归求解器

记忆化递归求解器通过缓存中间结果，将朴素的递归解法转化为高效的解法。它是动态规划的自顶向下版本，既保留了递归的清晰性，又避免了重复计算。

#### 我们正在解决什么问题？

递归算法经常会多次重复计算相同的子问题。
示例：

$$
F(n) = F(n-1) + F(n-2)
$$

一个朴素的递归调用树会重复计算 $F(3)$、$F(2)$ 等子问题，次数呈指数级增长。
通过记忆化（存储）第一次计算后的结果，我们可以在后续以 $O(1)$ 的时间复用它们。

#### 工作原理（通俗解释）

1.  清晰地定义递推关系。
2.  添加一个缓存（字典或数组）来存储已计算的结果。
3.  在每次递归调用前，检查缓存。
4.  如果存在，则返回缓存值。
5.  否则，进行计算、存储并返回。

这种方法保留了递归的优雅性，同时达到了与迭代动态规划相当的性能。

#### 逐步示例

示例 1：斐波那契数列

朴素递归：
$$
F(n) = F(n-1) + F(n-2)
$$

记忆化版本：

| n | F(n) | 是否已缓存？      |
| - | ---- | ----------------- |
| 0 | 0    | 基本情况          |
| 1 | 1    | 基本情况          |
| 2 | 1    | 计算得到          |
| 3 | 2    | 计算得到          |
| 4 | 3    | 通过缓存查找得到  |

时间复杂度从 $O(2^n)$ 降至 $O(n)$。

示例 2：二项式系数

递推关系：
$$
C(n, k) = C(n-1, k-1) + C(n-1, k)
$$

无记忆化：指数级
有记忆化：$O(nk)$

示例 3：零钱兑换

$$
\text{ways}(n) = \text{ways}(n-\text{coin}) + \text{ways}(n, \text{next})
$$

通过 $(n, \text{index})$ 进行记忆化，以避免重复计算状态。

#### 微型代码（Python）

```python
def fib_memo(n, memo={}):
    if n in memo:
        return memo[n]
    if n <= 1:
        return n
    memo[n] = fib_memo(n-1, memo) + fib_memo(n-2, memo)
    return memo[n]
```

或者显式传递缓存：

```python
def fib_memo(n):
    memo = {}
    def helper(k):
        if k in memo:
            return memo[k]
        if k <= 1:
            return k
        memo[k] = helper(k-1) + helper(k-2)
        return memo[k]
    return helper(n)
```

#### 为什么它很重要

-   保留了直观的递归结构
-   极大地降低了时间复杂度
-   是通向制表法（自底向上动态规划）的自然过渡
-   使得高效解决具有重叠子问题的递推关系成为可能

#### 一个温和的证明（为什么它有效）

令 $S$ 为所有不同子问题的集合。
在没有记忆化的情况下，每个子问题都会被重复计算指数次。
使用记忆化后，每个 $s \in S$ 只被精确计算一次。
因此，总时间 = $O(|S|)$。

#### 亲自尝试

1.  为朴素的斐波那契算法添加记忆化。
2.  为二项式系数 $C(n,k)$ 添加记忆化。
3.  将记忆化应用于背包问题的递归解法。
4.  统计使用和不使用记忆化时的总递归调用次数。

#### 测试用例

| 问题              | 朴素算法时间复杂度 | 记忆化算法时间复杂度 |
| ----------------- | ------------------ | -------------------- |
| Fibonacci(40)     | $O(2^{40})$        | $O(40)$              |
| Binomial(20,10)   | $O(2^{20})$        | $O(200)$             |
| Coin Change(100)  | $O(2^n)$           | $O(n \cdot k)$       |

#### 复杂度总结

| 方法       | 时间复杂度                      | 空间复杂度      |
| ---------- | ------------------------------- | --------------- |
| 递归       | 指数级                          | $O(n)$ 栈空间   |
| 记忆化递归 | 多项式级（不同子问题的数量）    | 缓存 + 栈空间   |

记忆化融合了清晰性和效率，是"会记忆"的递归。它通过一个简单的洞见——永远不要重复解决同一个问题——将朴素的指数级算法转变为优雅的线性或多项式级解法。
### 50 特征多项式求解器

特征多项式求解器是一种强大的代数技术，用于求解常系数线性齐次递推关系。它通过多项式根来表达递推关系，从而给出闭式解。

#### 我们要解决什么问题？

当面对如下形式的递推关系时：

$$
T(n) = a_1 T(n-1) + a_2 T(n-2) + \cdots + a_k T(n-k)
$$

我们希望得到 $T(n)$ 的闭式表达式，而不是一步步计算。
特征多项式捕捉了递推关系的结构，其根决定了解的一般形式。

#### 工作原理（通俗解释）

1.  将递推关系写成标准形式：
   $$
   T(n) - a_1 T(n-1) - a_2 T(n-2) - \cdots - a_k T(n-k) = 0
   $$
2.  将 $T(n-i)$ 替换为 $r^{n-i}$，形成多项式方程：
   $$
   r^k - a_1 r^{k-1} - a_2 r^{k-2} - \cdots - a_k = 0
   $$
3.  求解根 $r_1, r_2, \ldots, r_k$。
4.  通解为：
   $$
   T(n) = c_1 r_1^n + c_2 r_2^n + \cdots + c_k r_k^n
   $$
5.  使用初始条件求解常数 $c_i$。

如果有重根，对于重数 $p$，乘以 $n^p$。

#### 逐步示例

示例 1：斐波那契数列

递推关系：
$$
F(n) = F(n-1) + F(n-2)
$$

特征多项式：
$$
r^2 - r - 1 = 0
$$

根：
$$
r_1 = \frac{1+\sqrt{5}}{2}, \quad r_2 = \frac{1-\sqrt{5}}{2}
$$

通解：
$$
F(n) = c_1 r_1^n + c_2 r_2^n
$$

使用 $F(0)=0$, $F(1)=1$：
$$
c_1 = \frac{1}{\sqrt{5}}, \quad c_2 = -\frac{1}{\sqrt{5}}
$$

因此：
$$
F(n) = \frac{1}{\sqrt{5}}\left(\left(\frac{1+\sqrt{5}}{2}\right)^n - \left(\frac{1-\sqrt{5}}{2}\right)^n\right)
$$

这就是比内公式。

示例 2：$T(n) = 3T(n-1) - 2T(n-2)$

特征多项式：
$$
r^2 - 3r + 2 = 0 \implies (r-1)(r-2)=0
$$

根：$r_1=1, , r_2=2$

解：
$$
T(n) = c_1(1)^n + c_2(2)^n = c_1 + c_2 2^n
$$

使用基本情况求 $c_1, c_2$。

示例 3：重根

$$
T(n) = 2T(n-1) - T(n-2)
$$

特征多项式：
$$
r^2 - 2r + 1 = 0 \implies (r-1)^2 = 0
$$

解：
$$
T(n) = (c_1 + c_2 n) \cdot 1^n = c_1 + c_2 n
$$

#### 微型代码（Python）

```python
import sympy as sp

def solve_recurrence(coeffs, initials):
    n = len(coeffs)
    r = sp.symbols('r')
    poly = rn - sum(coeffs[i]*r(n-i-1) for i in range(n))
    roots = sp.roots(poly, r)
    r_syms = list(roots.keys())
    c = sp.symbols(' '.join([f'c{i+1}' for i in range(n)]))
    Tn = sum(c[i]*r_syms[i]sp.symbols('n') for i in range(n))
    equations = []
    for i, val in enumerate(initials):
        equations.append(Tn.subs(sp.symbols('n'), i) - val)
    sol = sp.solve(equations, c)
    return Tn.subs(sol)
```

调用 `solve_recurrence([1, 1], [0, 1])` → 比内公式。

#### 为什么它很重要

-   为线性递推关系提供闭式解
-   消除了迭代或递归的需要
-   将算法分析与代数和特征值联系起来
-   用于运行时分析、组合数学和离散建模

#### 一个温和的证明（为什么它有效）

假设递推关系：
$$
T(n) = a_1 T(n-1) + \cdots + a_k T(n-k)
$$

假设 $T(n) = r^n$：

$$
r^n = a_1 r^{n-1} + \cdots + a_k r^{n-k}
$$

除以 $r^{n-k}$：

$$
r^k = a_1 r^{k-1} + \cdots + a_k
$$

求解多项式根。每个根对应一个独立解。
根据线性性质，独立解的和也是一个解。

#### 动手试试

1.  求解 $T(n)=2T(n-1)+T(n-2)$，已知 $T(0)=0, T(1)=1$。
2.  求解 $T(n)=T(n-1)+2T(n-2)$，已知 $T(0)=2, T(1)=3$。
3.  求解具有重根 $r=1$ 的情况。
4.  对 $n=0\ldots5$ 进行数值验证。

#### 测试用例

| 递推关系               | 特征多项式    | 根                         | 闭式形式          |
| ---------------------- | ------------ | ------------------------ | ------------- |
| $F(n)=F(n-1)+F(n-2)$   | $r^2-r-1=0$  | $\frac{1\pm\sqrt{5}}{2}$ | 比内公式         |
| $T(n)=3T(n-1)-2T(n-2)$ | $r^2-3r+2=0$ | 1, 2                     | $c_1+c_2 2^n$ |
| $T(n)=2T(n-1)-T(n-2)$  | $(r-1)^2=0$  | 1 (双重)               | $c_1+c_2 n$   |

#### 复杂度总结

| 步骤                 | 时间复杂度     | 空间复杂度  |
| -------------------- | -------- | ------ |
| 求解多项式     | $O(k^3)$ | $O(k)$ |
| 计算闭式形式 | $O(1)$   | $O(1)$ |

特征多项式求解器是求解递推关系的代数核心，它通过根和对称性的力量，将重复的模式转化为精确的公式。

## 第 6 章 搜索基础
### 51 搜索空间可视化工具

搜索空间可视化工具是一种概念性工具，用于映射和理解算法探索的整个可能性空间。通过将搜索过程建模为树或图，你可以在深入代码之前，对完备性、最优性和复杂性获得直观理解。

#### 我们要解决什么问题？

当处理优化、约束满足或路径查找等问题时，解决方案并非立即可得，我们必须探索一个可能性空间。
理解这个空间有多大、它如何增长以及如何对其进行剪枝，对于算法设计至关重要。

可视化搜索空间有助于回答以下问题：

-   有多少状态是可达的？
-   搜索的深度或广度如何？
-   分支因子是多少？
-   目标位于何处？

#### 它是如何工作的（通俗解释）

1.  将状态建模为节点。每个节点代表一个部分或完整的解决方案。
2.  将状态转移建模为边。每次移动或决策都会带你进入一个新状态。
3.  定义起始节点和目标节点。通常，根节点（起始）会向一个或多个目标扩展。
4.  追踪探索过程。广度优先逐层探索；深度优先深入探索。
5.  如果适用（例如对于 A*、分支定界等算法），用成本或启发式值为节点添加标签。

这种结构不仅能揭示正确性，还能揭示效率和复杂性。

#### 逐步示例

示例 1：二叉搜索树遍历

对于数组 `[1, 2, 3, 4, 5, 6, 7]` 和目标值 = 6：

搜索空间（比较过程）：

```
        4
       / \
      2   6
     / \ / \
    1  3 5  7
```

探索路径：4 → 6（找到）

搜索空间深度：$\log_2 7 \approx 3$

示例 2：八皇后问题

每一层代表在新的一行放置一个皇后。
随着约束条件减少可能性，分支因子逐渐减小。

可视化显示总共有 8! 条路径，但剪枝会剪掉大部分。

示例 3：迷宫求解器

状态 = 网格单元；边 = 可能的移动。

可视化有助于你看到 BFS 的波前扩展与 DFS 的深度优先路径之间的区别。

#### 微型代码（Python）

```python
from collections import deque

def visualize_bfs(graph, start):
    visited = set()
    queue = deque([(start, [start])])
    while queue:
        node, path = queue.popleft()
        print(f"访问节点: {node}, 路径: {path}")
        visited.add(node)
        for neighbor in graph[node]:
            if neighbor not in visited:
                queue.append((neighbor, path + [neighbor]))
```

在一个小的邻接表上使用此代码，可以看到 BFS 的层次展开过程。

#### 为什么它很重要

-   建立对算法行为的直觉
-   展示广度与深度之间的权衡
-   揭示冗余路径和剪枝机会
-   对教学、调试和复杂性估算很有用

#### 一个温和的证明（为什么它有效）

令每个状态 $s \in S$ 通过转移 $E$ 连接。
搜索算法定义了节点扩展的顺序（DFS、BFS、基于启发式的）。
将 $S$ 可视化为图保留了以下特性：

-   完备性：BFS 探索所有有限路径
-   最优性：在成本一致的情况下，最短路径 = 最先找到的路径
-   复杂性：与生成的节点数成正比（通常是 $O(b^d)$）

#### 自己动手试试

1.  为 7 个元素的二分搜索绘制搜索树。
2.  在迷宫中可视化 DFS 与 BFS。
3.  为在 $4\times4$ 棋盘上放置 4 个皇后构建搜索空间。
4.  比较有剪枝和无剪枝时的路径数量。

#### 测试用例

| 问题         | 搜索空间大小 | 可视化见解               |
| ------------ | ------------ | ------------------------ |
| 二分搜索     | $\log_2 n$   | 狭窄、平衡               |
| 八皇后       | $8!$         | 需要大量剪枝             |
| 迷宫 (10x10) | $100$ 个节点 | BFS = 波，DFS = 路径     |
| 数独         | $9^{81}$     | 利用约束进行剪枝         |

#### 复杂性总结

| 算法 | 探索的节点数 | 内存使用   | 可视化特征       |
| ---- | ------------ | ---------- | ---------------- |
| BFS  | $O(b^d)$     | $O(b^d)$   | 树层             |
| DFS  | $O(bd)$      | $O(d)$     | 深度路径         |
| A*   | $O(b^d)$     | $O(b^d)$   | 成本引导的前沿   |

搜索空间可视化工具将抽象的计算转化为几何图形，使不可见的探索过程变得可见，帮助你在编码之前对复杂性进行推理。
### 52 决策树深度估计器

决策树深度估计器帮助你推理算法在最坏、最好或平均情况下必须进行多少次提问、比较或分支选择。它将决策过程建模为一棵树，其中每个节点代表一次测试，每个叶子代表一个结果。

#### 我们解决什么问题？

任何通过比较或条件分支进行的算法（如排序、搜索或分类）都可以用决策树来表示。
分析其深度有助于理解：

- 最坏情况时间复杂度（最长路径）
- 最好情况时间复杂度（最短路径）
- 平均情况复杂度（加权路径长度）

通过研究深度，我们可以理解解决问题所需的最小信息量。

#### 工作原理（通俗解释）

1.  将每次比较或条件表示为一个分支节点。
2.  根据真/假或小于/大于的结果沿着每个分支前进。
3.  每个叶子代表一个已解决的实例（例如，已排序的数组、找到的键）。
4.  深度 = 一条路径上的决策次数。
5.  最大深度 → 最坏情况成本。

这个模型抽象掉了细节，只关注信息流。

#### 逐步示例

示例 1：二分查找

- 每次比较将搜索空间减半。
- 决策树深度为 $\log_2 n$。
- 最坏情况下的最小比较次数：$\lceil \log_2 n \rceil$。

$n=8$ 个元素的树：

```
          [mid=4]
         /       \
     [mid=2]     [mid=6]
     /   \       /   \
   [1]   [3]   [5]   [7]
```

深度：$3 = \log_2 8$

示例 2：比较排序

每个叶子代表一种可能的排序。
一个有效的排序树必须能区分所有 $n!$ 种顺序。

因此：

$$
2^h \ge n! \implies h \ge \log_2(n!)
$$

所以，任何比较排序的下界是：
$$
\Omega(n \log n)
$$

示例 3：决策算法

如果用 $b$ 个可能的结果来解决一个是/否分类问题，
所需的最小比较次数 = $\lceil \log_2 b \rceil$。

#### 微型代码（Python）

```python
import math

def decision_tree_depth(outcomes):
    # 区分 outcomes 个结果所需的最小比较次数
    return math.ceil(math.log2(outcomes))

print(decision_tree_depth(8))  # 3
print(decision_tree_depth(120))  # ~7 (对应 5!)
```

#### 为什么重要

- 揭示理论极限（通过比较的排序算法不可能快于 $O(n \log n)$）
- 建模搜索和优化中的决策复杂度
- 连接信息论和算法设计
- 帮助比较分支策略

#### 一个温和的证明（为什么有效）

每次比较将搜索空间一分为二。
要区分 $N$ 种可能的结果，至少需要 $h$ 次比较，使得：
$$
2^h \ge N
$$

因此：
$$
h \ge \lceil \log_2 N \rceil
$$

对于排序：
$$
N = n! \implies h \ge \log_2 (n!) = \Omega(n \log n)
$$

这个界限独立于具体实现，它是所需信息量的下界。

#### 动手试试

1.  为 3 个元素的排序构建决策树。
2.  计算在 $n=16$ 上进行二分查找的比较次数。
3.  估计 4 个元素比较排序的下界。
4.  可视化具有 8 个类别的分类树。

#### 测试用例

| 问题                 | 可能结果    | 深度下界     |
| -------------------- | ----------- | ------------ |
| 二分查找 (n=8)       | 8           | 3            |
| 排序 3 个元素        | $3! = 6$    | $\ge 3$      |
| 排序 5 个元素        | $5! = 120$  | $\ge 7$      |
| 分类 8 种结果        | 8           | 3            |

#### 复杂度总结

| 算法         | 搜索空间 | 深度         | 含义                     |
| ------------ | -------- | ------------ | ------------------------ |
| 二分查找     | $n$      | $\log_2 n$   | 最坏情况比较次数         |
| 比较排序     | $n!$     | $\log_2 n!$  | 信息论极限               |
| 分类器       | $b$      | $\log_2 b$   | 区分 $b$ 个类别的最小测试次数 |

决策树深度估计器帮助揭示每个算法背后看不见的“提问复杂度”，即无论你的代码多么巧妙，都必须做出多少次决策。
### 53 比较计数器

比较计数器用于测量算法比较元素或条件的次数，这是理解其时间复杂度、效率和实际性能的直接方法。通过统计比较次数，我们可以深入了解真正驱动运行时间的因素，特别是在基于比较的算法中。

#### 我们要解决什么问题？

许多算法，如排序、搜索、选择、优化，都围绕着比较展开。
每一个 `if`、`<` 或 `==` 都是一个消耗时间的决策。

通过统计比较次数，我们可以：

*   估算小规模输入的确切步骤数
*   验证渐进界限（$O(n^2)$、$O(n \log n)$ 等）
*   经验性地比较不同算法
*   识别实现中的热点

这使得性能从一个模糊的概念转变为可测量的数据。

#### 工作原理（通俗解释）

1.  对算法进行插装：将每个比较操作包装在一个计数器中。
2.  每次比较发生时，计数器加一。
3.  使用样本输入运行算法。
4.  观察输入规模增长时的模式。
5.  将结果拟合到复杂度函数（$n$、$n \log n$、$n^2$ 等）。

这既提供了经验证据，也提供了分析洞察。

#### 逐步示例

示例 1：线性搜索

在大小为 $n$ 的数组中搜索。
每次比较检查一个元素。

| 情况     | 比较次数          |
| -------- | ----------------- |
| 最佳情况 | 1                 |
| 最坏情况 | n                 |
| 平均情况 | $\frac{n+1}{2}$   |

因此：
$$
T(n) = O(n)
$$

示例 2：二分搜索

每一步都将搜索空间减半。

| 情况     | 比较次数                     |
| -------- | ---------------------------- |
| 最佳情况 | 1                            |
| 最坏情况 | $\lceil \log_2 n \rceil$     |
| 平均情况 | $\approx \log_2 n - 1$       |

因此：
$$
T(n) = O(\log n)
$$

示例 3：冒泡排序

对于长度为 $n$ 的数组，每一轮比较相邻元素。

| 轮次 | 比较次数 |
| ---- | -------- |
| 1    | $n-1$    |
| 2    | $n-2$    |
| ...  | ...      |
| n-1  | 1        |

总计：
$$
C(n) = (n-1)+(n-2)+\cdots+1 = \frac{n(n-1)}{2}
$$

因此：
$$
T(n) = O(n^2)
$$

#### 微型代码（Python）

```python
class Counter:
    def __init__(self):
        self.count = 0
    def compare(self, a, b, op):
        self.count += 1
        if op == '<': return a < b
        if op == '>': return a > b
        if op == '==': return a == b

# 示例：冒泡排序
def bubble_sort(arr):
    c = Counter()
    n = len(arr)
    for i in range(n):
        for j in range(n - i - 1):
            if c.compare(arr[j], arr[j + 1], '>'):
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
    return arr, c.count
```

在小数组上运行以记录确切的比较次数。

#### 为什么这很重要

*   将抽象的复杂度转化为可测量的数据
*   揭示隐藏的常数因子和实际性能
*   对算法性能分析和教学很有用
*   有助于确认理论分析

#### 一个温和的证明（为什么它有效）

每次比较对应于算法决策树中的一个节点。
比较次数 = 访问的节点数。
因此，统计比较次数就是测量路径长度，这与基于比较的算法的运行时间相关。

通过对所有路径求和，我们可以得到确切的代价函数 $C(n)$。

#### 亲自尝试

1.  统计当 $n=5$ 时，冒泡排序与插入排序的比较次数。
2.  测量 $n=16$ 时二分搜索的比较次数。
3.  比较选择排序和归并排序。
4.  将测量值拟合到理论上的 $O(n^2)$ 或 $O(n \log n)$。

#### 测试用例

| 算法         | 输入规模 | 比较次数 | 模式                |
| ------------ | -------- | -------- | ------------------- |
| 线性搜索     | 10       | 10       | $O(n)$              |
| 二分搜索     | 16       | 4        | $O(\log n)$         |
| 冒泡排序     | 5        | 10       | $\frac{n(n-1)}{2}$  |
| 归并排序     | 8        | 17       | $\approx n \log n$  |

#### 复杂度总结

| 算法         | 最佳情况 | 最坏情况            | 平均情况            |
| ------------ | -------- | ------------------- | ------------------- |
| 线性搜索     | 1        | n                   | $\frac{n+1}{2}$     |
| 二分搜索     | 1        | $\log_2 n$          | $\log_2 n - 1$      |
| 冒泡排序     | $n-1$    | $\frac{n(n-1)}{2}$  | $\frac{n(n-1)}{2}$  |

比较计数器使复杂度理论变得生动，每一个 `if` 都成为一个数据点，每一个循环都揭示出其真实的代价。
### 54 提前终止启发式

提前终止启发式是一种策略，当所需结果已经得到保证或进一步的工作不会改变结果时，在算法完全完成之前就停止它。这是一种简单而强大的优化方法，在最佳情况和平均情况下可以节省时间。

#### 我们在解决什么问题？

许多算法在有效找到解之后，或者在额外步骤不再改善结果时，仍然执行冗余工作。
通过及早检测这些条件，我们可以切断不必要的计算，在不影响正确性的情况下减少运行时间。

关键问题：*"我们现在停止会改变答案吗？"*

#### 它是如何工作的（通俗解释）

1.  确定一个超出常规循环限制的停止条件。
2.  在每一步检查结果是否已经确定。
3.  当条件满足时提前退出。
4.  如果部分结果被保证是最终结果，则返回它。

这种优化在搜索、排序、模拟和迭代收敛算法中很常见。

#### 逐步示例

示例 1：冒泡排序

通常运行 $n-1$ 轮，即使数组早已排好序。
添加一个标志来跟踪交换；如果没有发生交换，则终止。

```python
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        swapped = False
        for j in range(n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
                swapped = True
        if not swapped:
            break  # 提前终止
    return arr
```

最佳情况：已排序 → 仅需 1 轮
$$
T(n) = O(n)
$$
最坏情况：逆序 → 仍然是 $O(n^2)$

示例 2：线性搜索

在数组 `A` 中搜索键值 $k$：

-   找到时停止（不扫描整个数组）。
-   平均情况比较次数从 $O(n)$ 改善到 $\frac{n}{2}$。

示例 3：收敛算法

在迭代求解器中：

-   当误差 < ε（容差阈值）时停止。
-   避免不必要的额外迭代。

示例 4：约束搜索

在回溯或分支定界中：

-   当解无法改善当前最优解时，停止探索。
-   显著减少搜索空间。

#### 为什么它很重要

-   改善了平均情况性能
-   减少了实际系统中的能耗和时间
-   保持了正确性（永远不会过早停止）
-   为近似算法实现了优雅降级

#### 一个温和的证明（为什么它有效）

令 $f(i)$ 表示第 $i$ 次迭代后的进度度量。
如果 $f(i)$ 满足停止不变量 $P$，那么继续下去不会改变最终答案。
因此：
$$
\exists i < n ;|; P(f(i)) = \text{True} \implies T(n) = i
$$
在有利情况下，将总操作数从 $n$ 减少到 $i$。

#### 动手试试

1.  为选择排序添加提前停止（当前缀已排序时）。
2.  对牛顿法应用容差检查。
3.  实现立即退出的线性搜索。
4.  比较使用和不使用提前终止的运行时间。

#### 测试用例

| 算法         | 条件               | 最佳情况   | 最坏情况   |             |        |
| ------------ | ------------------ | ---------- | ---------- | ----------- | ------ |
| 冒泡排序     | 一轮中无交换       | $O(n)$     | $O(n^2)$   |             |        |
| 线性搜索     | 提前找到           | $O(1)$     | $O(n)$     |             |        |
| 牛顿法       | $                  | x_{i+1}-x_i | <\epsilon$ | $O(\log n)$ | $O(n)$ |
| 深度优先搜索 | 提前找到目标       | $O(d)$     | $O(b^d)$   |             |        |

#### 复杂度总结

| 情况     | 描述               | 时间复杂度          |
| -------- | ------------------ | ------------------- |
| 最佳     | 触发提前停止       | 从 $n$ 减少到 $k$   |
| 平均     | 取决于数据顺序     | 通常为亚线性        |
| 最坏     | 条件从未满足       | 与原算法相同        |

提前终止启发式增加了一种简单而深刻的优化，它教会算法何时停止，而不仅仅是如何运行。
### 55 哨兵技术

哨兵技术是一种简单而优雅的优化方法，它通过在数据结构的末尾放置一个*特殊标记*（哨兵）来消除循环中冗余的边界检查。这是一个巧妙的技巧，能使代码更快、更简洁、更安全。

#### 我们要解决什么问题？

在许多算法中，尤其是搜索和扫描循环，我们反复检查两件事：

1.  元素是否与目标匹配
2.  是否已到达结构的末尾

这种双重条件在每次迭代中都需要额外的比较。
通过添加一个哨兵值，我们可以保证循环终止并移除一项检查。

#### 工作原理（通俗解释）

1.  在数组末尾追加一个哨兵值（例如目标值或无穷大）。
2.  循环直到找到匹配项，期间不检查边界。
3.  当遇到哨兵时自动停止。
4.  之后检查匹配是真实的还是由哨兵触发的。

这可以将：

```python
while i < n and A[i] != key:
    i += 1
```

替换为更简单的循环：

```python
A[n] = key
while A[i] != key:
    i += 1
```

循环内部不再有边界检查。

#### 逐步示例

示例 1：使用哨兵的线性搜索

不使用哨兵：

```python
def linear_search(A, key):
    for i in range(len(A)):
        if A[i] == key:
            return i
    return -1
```

每一步都检查两个条件。

使用哨兵：

```python
def linear_search_sentinel(A, key):
    n = len(A)
    A.append(key)  # 添加哨兵
    i = 0
    while A[i] != key:
        i += 1
    return i if i < n else -1
```

-   循环内部只有一个条件
-   对找到和未找到的情况都有效

成本降低：从 `2n+1` 次比较减少到 `n+1` 次

示例 2：合并有序列表

在每个列表末尾添加无穷大哨兵：

-   避免了重复的数组结束检查
-   简化了内层循环逻辑

例如，在归并排序中，使用哨兵值可以避免 `if i < n` 检查。

示例 3：字符串解析

追加 `'\0'`（空终止符）作为哨兵，使循环可以自动在哨兵处停止。
在 C 语言字符串中广泛使用。

#### 为什么它很重要

-   移除冗余检查
-   简化循环逻辑
-   提高效率和可读性
-   常用于系统编程、解析、搜索

#### 一个温和的证明（为什么有效）

设 $n$ 为数组长度。
通常，每次迭代执行：

-   1 次与边界的比较
-   1 次与键值的比较

因此总共约 $2n+1$ 次比较。

使用哨兵：

-   每个元素 1 次比较
-   循环后 1 次最终检查

因此总共约 $n+1$ 次比较。

对于长列表，改进因子约为 2 倍加速。

#### 亲自尝试

1.  实现哨兵线性搜索并统计比较次数。
2.  在合并例程中添加无穷大哨兵。
3.  编写一个在哨兵 `'\0'` 处停止的解析器。
4.  与标准实现比较运行时间。

#### 测试用例

| 输入         | 键 | 输出 | 比较次数 |
| ------------ | -- | ---- | -------- |
| [1,2,3,4], 3 | 3  | 2    | 3        |
| [1,2,3,4], 5 | -1 | 4    | 5        |
| [] , 1       | -1 | 0    | 1        |

#### 复杂度总结

| 情况         | 时间复杂度        | 空间复杂度 | 备注                     |
| ------------ | ----------------- | ---------- | ------------------------ |
| 最佳         | $O(1)$            | $O(1)$     | 立即找到                 |
| 最差         | $O(n)$            | $O(1)$     | 在末尾找到 / 未找到      |
| 改进         | 比较次数减少约 2 倍 | +1 个哨兵  | 总是安全的               |

哨兵技术是算法设计中一个低调的杰作，证明有时一个微小的标记也能带来巨大的改变。
### 56 二元谓词测试器

二元谓词测试器是一种简单而基础的工具，用于检查涉及两个操作数的条件是否成立，它是算法中比较、排序、过滤和搜索逻辑的构建块。它通过抽象条件检查来澄清逻辑并促进重用。

#### 我们要解决什么问题？

每个算法都依赖于决策，"这个元素更小吗？"、"这两个相等吗？"、"这满足约束吗？"。
这些是/否问题就是二元谓词：返回 `True` 或 `False` 的函数。

通过将它们形式化为可重用的测试器，我们获得了：

- **清晰性**，将逻辑与控制流分离
- **可重用性**，可以作为参数传递给算法
- **灵活性**，可以轻松地从 `<` 切换到 `>` 或 `==`

这是排序、搜索和函数式风格算法的基础。

#### 工作原理（通俗解释）

1.  定义一个接受两个参数的谓词函数。
2.  如果条件满足则返回 `True`，否则返回 `False`。
3.  在循环、过滤器或算法决策中使用该谓词。
4.  动态交换谓词以改变算法行为。

谓词作为比较层，它们不控制流程，而是为流程提供信息。

#### 逐步示例

示例 1：通过谓词排序

定义不同的谓词：

```python
def less(a, b): return a < b
def greater(a, b): return a > b
def equal(a, b): return a == b
```

传递给排序例程：

```python
def compare_sort(arr, predicate):
    n = len(arr)
    for i in range(n):
        for j in range(0, n - i - 1):
            if predicate(arr[j + 1], arr[j]):
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
    return arr
```

现在，只需更改谓词，就可以按升序或降序排序。

示例 2：二分查找条件

二分查找依赖于谓词 `is_less(mid_value, key)` 来决定方向：

```python
def is_less(a, b): return a < b
```

因此，决策步骤变为：

```python
if is_less(arr[mid], key):
    left = mid + 1
else:
    right = mid - 1
```

这使得比较逻辑变得明确，而不是隐藏在控制结构中。

示例 3：过滤或匹配

```python
def between(a, b): return a < b
filtered = [x for x in data if between(x, 10)]
```

可以轻松地交换谓词以进行大于或等于检查。

#### 为什么它很重要

- 清晰地封装决策逻辑
- 支持高阶算法（将函数作为参数传递）
- 简化测试和定制
- 是泛型编程和模板（C++、Python 的 `key` 函数）的核心

#### 一个温和的证明（为什么它有效）

谓词抽象了排序或关系的概念。
如果一个谓词满足：

- 自反性（$P(x,x)=\text{False}$ 或 True，根据定义）
- 反对称性（$P(a,b) \Rightarrow \neg P(b,a)$）
- 传递性（$P(a,b)\wedge P(b,c) \Rightarrow P(a,c)$）

那么它就定义了一个严格弱序，这对于排序和搜索算法来说已经足够了。

因此，算法的正确性取决于谓词的一致性。

#### 亲自尝试

1.  为 `<`、`>`、`==` 和 `divisible(a,b)` 编写谓词。
2.  在一种选择算法中使用它们。
3.  使用相同的代码测试升序和降序排序。
4.  验证谓词的正确性（反对称性、传递性）。

#### 测试用例

| 谓词      | a   | b   | 结果  | 含义         |
| --------- | --- | --- | ----- | ------------ |
| less      | 3   | 5   | True  | 3 < 5        |
| greater   | 7   | 2   | True  | 7 > 2        |
| equal     | 4   | 4   | True  | 4 == 4       |
| divisible | 6   | 3   | True  | 6 % 3 == 0   |

#### 复杂度总结

| 操作                     | 时间                 | 空间  | 注释                 |
| ------------------------ | -------------------- | ----- | -------------------- |
| 谓词调用                 | $O(1)$               | $O(1)$ | 每次检查为常数       |
| 使用谓词的算法           | 取决于结构           |       | 例如，排序：$O(n^2)$ |

二元谓词测试器将隐藏的条件转化为可见的设计，澄清了逻辑，鼓励了重用，并为那些*基于关系而非指令*进行思考的通用算法奠定了基础。
### 57 范围测试函数

范围测试函数用于检查给定值是否落在指定边界内，这是处理区间、数组索引、数值域或搜索约束时的一种通用操作。它虽小但功能强大，能在无数应用中提供正确性和安全性。

#### 我们要解决什么问题？

许多算法都在某个范围内操作，无论是扫描数组、迭代循环、搜索区间还是强制执行约束。
反复检查 `if low <= x <= high` 会使代码混乱，并可能导致细微的差一错误。

通过定义一个可重用的范围测试，我们可以使此类检查：

- 集中化（一个定义，语义一致）
- 可读性强（调用处的意图清晰）
- 安全（避免不一致的不等式）

#### 工作原理（通俗解释）

1. 将边界逻辑封装到单个函数中。
2. 输入：一个值 `x` 和边界 `(low, high)`。
3. 返回：如果 `x` 满足范围条件则返回 `True`，否则返回 `False`。
4. 可以处理开区间、闭区间或半开区间。

变体：

- 闭区间：`[low, high]` → `low ≤ x ≤ high`
- 半开区间：`[low, high)` → `low ≤ x < high`
- 开区间：`(low, high)` → `low < x < high`

#### 逐步示例

示例 1：数组索引边界

防止越界访问：

```python
def in_bounds(i, n):
    return 0 <= i < n

if in_bounds(idx, len(arr)):
    value = arr[idx]
```

无需再手动处理范围逻辑。

示例 2：范围过滤

过滤在范围 `[a, b]` 内的值：

```python
def in_range(x, low, high):
    return low <= x <= high

data = [1, 3, 5, 7, 9]
filtered = [x for x in data if in_range(x, 3, 7)]
# → [3, 5, 7]
```

示例 3：约束检查

用于搜索或优化算法：

```python
if not in_range(candidate, min_val, max_val):
    continue  # 跳过无效候选值
```

保持循环简洁，避免边界错误。

示例 4：几何/区间问题

检查区间重叠：

```python
def overlap(a1, a2, b1, b2):
    return in_range(a1, b1, b2) or in_range(b1, a1, a2)
```

#### 为什么它很重要？

- 防止差一错误
- 提高代码清晰度和一致性
- 在循环守卫、搜索边界和有效性检查中必不可少
- 支持参数验证和防御性编程

#### 一个温和的证明（为什么它有效）

范围测试表达了一个逻辑合取：
$$
P(x) = (x \ge \text{low}) \land (x \le \text{high})
$$
对于闭区间，该谓词在集合 $[\text{low}, \text{high}]$ 内是自反且传递的。
通过将此谓词编码为一个函数，其正确性源于不等式的基本性质。

半开区间变体保留了定义良好的迭代边界（对数组索引很重要）。

#### 亲自尝试

1. 为 $(low, high)$ 实现 `in_open_range(x, low, high)`。
2. 为循环编写 `in_half_open_range(i, 0, n)`。
3. 在二分查找终止条件中使用范围测试。
4. 在矩阵遍历中检查索引有效性。

#### 测试用例

| 输入 | 范围     | 类型      | 结果   |
| ---- | -------- | --------- | ------ |
| 5    | [1, 10]  | 闭区间    | True   |
| 10   | [1, 10)  | 半开区间  | False  |
| 0    | (0, 5)   | 开区间    | False  |
| 3    | [0, 3]   | 闭区间    | True   |

#### 复杂度总结

| 操作         | 时间     | 空间     | 备注                     |
| ------------ | -------- | -------- | ------------------------ |
| 范围检查     | $O(1)$   | $O(1)$   | 常数时间比较             |
| 每次循环使用 | $O(n)$   | $O(1)$   | 总体线性                 |

范围测试函数是一个影响巨大的小护栏，它在每个边界处保护正确性，并使算法更易于推理。
### 58 搜索不变式检查器

搜索不变式检查器确保在搜索算法执行过程中，关键条件（不变式）始终成立。通过维护这些不变式，我们可以保证算法的正确性，防止细微的错误，并为证明和推理提供基础。

#### 我们正在解决什么问题？

在执行迭代搜索（如二分查找或插值查找）时，我们维护一些必须始终为真的条件，例如：

- 如果目标存在，它始终在当前边界内。
- 搜索区间每一步都在缩小。
- 索引保持有效且有序。

失去这些不变式可能导致无限循环、错误结果或索引错误。
通过显式地检查不变式，我们使正确性变得可见且可测试。

#### 工作原理（通俗解释）

1.  定义不变式，即每次迭代期间必须保持为真的条件。
2.  在每次更新步骤后，验证这些条件。
3.  如果某个不变式不成立，则断言或记录错误。
4.  将不变式同时用于调试和证明。

常见的搜索不变式：

- $ \text{low} \le \text{high} $
- $ \text{target} \in [\text{low}, \text{high}] $
- 区间大小递减：$ (\text{high} - \text{low}) $ 每一步都在缩小

#### 逐步示例

示例：二分查找不变式

目标：在 $[\text{low}, \text{high}]$ 中维护正确的搜索窗口。

1.  初始化：
   $ \text{low} = 0 $, $ \text{high} = n - 1 $
2.  不变式 1：
   $ \text{target} \in [\text{low}, \text{high}] $
3.  不变式 2：
   $ \text{low} \le \text{high} $
4.  步骤：
   计算 mid，缩小范围
5.  检查：
   每次迭代，断言这些不变式

#### 微型代码（Python）

```python
def binary_search(arr, target):
    low, high = 0, len(arr) - 1
    while low <= high:
        assert 0 <= low <= high < len(arr), "不变式被破坏！"
        mid = (low + high) // 2

        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            low = mid + 1
        else:
            high = mid - 1
    return -1
```

如果不变式不成立，我们可以及早捕获逻辑错误。

#### 为何重要

- 正确性证明：每次迭代都保持真理
- 调试辅助：立即检测逻辑缺陷
- 安全保障：防止无效访问或无限循环
- 文档说明：阐明算法意图

#### 一个温和的证明（为何有效）

假设不变式 $P$ 在迭代前成立。
更新步骤将状态 $(\text{low}, \text{high})$ 转换为 $(\text{low}', \text{high}')$。

我们证明：

1.  基本情况：$P$ 在第一次迭代前成立（初始化）
2.  归纳步骤：如果 $P$ 在迭代前成立，并且更新规则保持了 $P$，那么 $P$ 在迭代后也成立

因此，通过归纳法，$P$ 始终成立。这确保了算法的正确性。

#### 动手尝试

1.  为三分查找添加不变式
2.  使用不变式保持来证明二分查找的正确性
3.  测试边界情况（空数组、单个元素）
4.  可视化缩小区间，并在每一步检查不变式是否成立

#### 测试用例

| 输入数组        | 目标值 | 不变式成立 | 结果       |
| --------------- | ------ | ---------- | ---------- |
| [1, 3, 5, 7, 9] | 5      | 是         | 索引 2     |
| [2, 4, 6]       | 3      | 是         | 未找到     |
| [1]             | 1      | 是         | 索引 0     |
| []              | 10     | 是         | 未找到     |

#### 复杂度

| 操作             | 时间        | 空间  | 备注                 |
| ---------------- | ----------- | ----- | -------------------- |
| 检查不变式       | $O(1)$      | $O(1)$ | 常数时间检查         |
| 总搜索           | $O(\log n)$ | $O(1)$ | 保持正确性           |

搜索不变式检查器将隐式假设转化为显式保证，使您的搜索算法不仅快速，而且可证明是正确的。
### 59 探测计数器

探测计数器用于追踪搜索算法执行了多少次探测或查找尝试。它是一种诊断工具，用于理解效率并比较不同搜索策略或数据结构之间的性能。

#### 我们正在解决什么问题？

在搜索中（尤其是在哈希表、线性探测或开放寻址中），性能不仅取决于复杂度，还取决于查找或未找到某个元素所需的探测次数。

通过计数探测次数，我们可以：

- 揭示每次搜索的成本
- 比较不同负载因子下的性能
- 诊断聚集现象或低效的探测模式

#### 工作原理（通俗解释）

1.  初始化一个计数器 `probes = 0`。
2.  每次算法检查一个位置或节点时，递增 `probes`。
3.  当搜索结束时，记录或返回探测次数。
4.  使用统计量（平均值、最大值、方差）来衡量性能。

#### 逐步示例

示例：哈希表中的线性探测

1.  计算哈希值：$h = \text{key} \bmod m$
2.  从 $h$ 开始，检查槽位
3.  如果发生冲突，移动到下一个槽位
4.  每次检查都递增 `probes`
5.  当槽位为空或找到键时停止

如果表几乎已满，探测次数会增加，从而揭示效率损失。

#### 微型代码（Python）

```python
def linear_probe_search(table, key):
    m = len(table)
    h = key % m
    probes = 0
    i = 0

    while table[(h + i) % m] is not None:
        probes += 1
        if table[(h + i) % m] == key:
            return (h + i) % m, probes
        i += 1
        if i == m:
            break  # 表已满
    return None, probes
```

运行示例：

```python
table = [10, 21, 32, None, None]
index, probes = linear_probe_search(table, 21)
# probes = 1
```

#### 为何重要

-   **性能洞察**：理解渐进复杂度之外的搜索成本
-   **聚集检测**：揭示分布不均或冲突过多的问题
-   **负载因子调优**：在性能下降前找到阈值
-   **算法比较**：评估二次探测与线性探测

#### 一个温和的证明（为何有效）

令 $L$ 为负载因子（表中已填充的比例）。
线性探测中成功搜索的期望探测次数：

$$
E[P_{\text{success}}] = \frac{1}{2}\left(1 + \frac{1}{1 - L}\right)
$$

不成功搜索的期望探测次数：

$$
E[P_{\text{fail}}] = \frac{1}{2}\left(1 + \frac{1}{(1 - L)^2}\right)
$$

当 $L \to 1$ 时，探测次数快速增长，性能急剧下降。

#### 亲自尝试

1.  创建一个使用线性探测的哈希表
2.  在不同的负载因子下插入键
3.  测量命中和未命中的探测次数
4.  比较线性探测与二次探测

#### 测试用例

| 表（大小 7）          | 键  | 负载因子 | 期望探测次数 | 说明               |
| --------------------- | --- | -------- | ------------ | ------------------ |
| [10, 21, 32, None...] | 21  | 0.4      | 1            | 直接命中           |
| [10, 21, 32, 43, 54]  | 43  | 0.7      | 3            | 聚集区域           |
| [10, 21, 32, 43, 54]  | 99  | 0.7      | 5            | 探测后未命中       |

#### 复杂度

| 操作       | 时间（期望） | 时间（最坏） | 空间   |
| ---------- | ------------ | ------------ | ------ |
| 探测计数   | $O(1)$ 每步  | $O(n)$       | $O(1)$ |
| 总搜索时间 | $O(1)$ 平均  | $O(n)$       | $O(1)$ |

通过计数探测次数，我们从理论走向可测量的理解。这是一个简单的度量标准，揭示了冲突、负载因子和搜索效率背后隐藏的成本。
### 60 成本曲线绘制器

成本曲线绘制器能够可视化算法运行成本如何随着输入规模的增加而增长。它将抽象的复杂度转化为有形的曲线，帮助你并排比较理论和实际的性能表现。

#### 我们正在解决什么问题？

大 O 符号告诉我们成本如何缩放，但没有告诉我们具体是多少，或者性能在何处开始崩溃。
成本曲线让你可以：

- 看到实际增长与理论模型的对比
- 识别算法之间的交叉点
- 检测异常或开销
- 建立关于效率和扩展性的直觉

#### 工作原理（通俗解释）

1.  选择一个算法和一个输入规模范围。
2.  对于每个 $n$，运行算法并记录：
    *   时间成本（运行时间）
    *   空间成本（内存使用量）
    *   操作计数
3.  绘制 $(n, \text{cost}(n))$ 点
4.  叠加理论曲线（$O(n)$, $O(n \log n)$, $O(n^2)$）进行比较

这就创建了一个关于性能和规模关系的可视化地图。

#### 分步示例

让我们测量不同输入规模下的排序成本：

| n    | 时间 (毫秒) |
| ---- | ----------- |
| 100  | 0.3         |
| 500  | 2.5         |
| 1000 | 5.2         |
| 2000 | 11.3        |
| 4000 | 23.7        |

绘制这些点。曲线的形状表明是 $O(n \log n)$ 行为。

#### 微型代码 (Python + Matplotlib)

```python
import time, random, matplotlib.pyplot as plt

def measure_cost(algorithm, sizes):
    results = []
    for n in sizes:
        arr = [random.randint(0, 100000) for _ in range(n)]
        start = time.time()
        algorithm(arr)
        end = time.time()
        results.append((n, end - start))
    return results

def plot_cost_curve(results):
    xs, ys = zip(*results)
    plt.plot(xs, ys, marker='o')
    plt.xlabel("输入规模 (n)")
    plt.ylabel("时间 (秒)")
    plt.title("算法成本曲线")
    plt.grid(True)
    plt.show()
```

#### 为何重要

-   让大 O 符号变得生动
-   可视化调试，检测意外的峰值
-   经验性地比较算法
-   调整阈值，知道何时切换策略

#### 一个温和的证明（为何有效）

如果理论成本是 $f(n)$，经验成本是 $g(n)$，那么我们期望：

$$
\lim_{n \to \infty} \frac{g(n)}{f(n)} = c
$$

其中 $c$ 是一个常数缩放因子。

绘制的曲线在视觉上近似于 $g(n)$；将其形状与 $f(n)$ 进行比较，可以揭示复杂度类别是否符合预期。

#### 亲自尝试

1.  比较冒泡排序、归并排序和快速排序。
2.  叠加 $n$、$n \log n$ 和 $n^2$ 参考曲线。
3.  尝试不同的数据分布（已排序、逆序）。
4.  同时绘制时间和内存成本曲线。

#### 测试用例

| 算法       | 输入规模 | 时间 (毫秒) | 形状         | 匹配           |
| ---------- | -------- | ----------- | ------------ | -------------- |
| 冒泡排序   | 1000     | 80          | 二次型       | $O(n^2)$       |
| 归并排序   | 1000     | 5           | 线性对数型   | $O(n \log n)$  |
| 快速排序   | 1000     | 3           | 线性对数型   | $O(n \log n)$  |

#### 复杂度

| 方面       | 成本              | 备注                       |
| ---------- | ----------------- | -------------------------- |
| 测量       | $O(k \cdot T(n))$ | 测量 $k$ 个样本规模        |
| 绘图       | $O(k)$            | 根据 $k$ 个点绘制曲线      |
| 空间       | $O(k)$            | 存储测量数据               |

成本曲线绘制器将理论转化为形状，一个简单的图表使得扩展行为和权衡取舍一目了然。

## 第 7 节. 排序基础
### 61 交换计数器

交换计数器用于跟踪排序过程中执行元素交换的次数。它帮助我们理解算法执行了多少重排操作，并可作为效率、稳定性和输入敏感性的诊断工具。

#### 我们正在解决什么问题？

许多排序算法（如冒泡排序、选择排序或快速排序）通过交换来重排元素。统计交换次数显示了算法的“活跃”程度：

- 冒泡排序 → 交换次数多
- 插入排序 → 在接近有序的输入上交换次数较少
- 选择排序 → 交换次数固定

通过跟踪交换，我们可以比较算法在数据移动成本上的表现，而不仅仅是比较次数。

#### 工作原理（通俗解释）

1. 初始化 `swap_count = 0`。
2. 每次两个元素交换位置时，计数器加一。
3. 最后，报告 `swap_count` 以衡量重排的工作量。
4. 使用结果来比较排序策略或分析输入模式。

#### 逐步示例

示例：对 [3, 2, 1] 进行冒泡排序

1. 比较 3 和 2 → 交换 → 计数 = 1 → [2, 3, 1]
2. 比较 3 和 1 → 交换 → 计数 = 2 → [2, 1, 3]
3. 比较 2 和 1 → 交换 → 计数 = 3 → [1, 2, 3]

总交换次数：3

如果输入是 [1, 2, 3]，则不会发生交换，成本反映了有序程度。

#### 微型代码（Python）

```python
def bubble_sort_with_swaps(arr):
    n = len(arr)
    swaps = 0
    for i in range(n):
        for j in range(0, n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
                swaps += 1
    return arr, swaps
```

示例：

```python
arr, swaps = bubble_sort_with_swaps([3, 2, 1])
# swaps = 3
```

#### 为什么这很重要

- 量化数据移动成本
- 衡量输入的无序程度（零交换 → 已排序）
- 比较算法的交换效率
- 揭示算法在真实数据中的适应性行为

#### 一个温和的证明（为什么它有效）

每次交换将逆序数减少一个。
逆序是指一对 $(i, j)$，满足 $i < j$ 且 $a_i > a_j$。

如果初始逆序数 = $I$，并且每次交换修复一个逆序：

$$
\text{总交换次数} = I_{\text{初始}}
$$

因此，交换次数直接等于无序度的度量，这是一个有意义的成本指标。

#### 亲自尝试

1.  统计冒泡排序、插入排序和选择排序的交换次数。
2.  在已排序、逆序和随机列表上运行。
3.  比较计数，哪种算法对接近有序的数据适应性最好？
4.  绘制交换次数与输入大小的关系图。

#### 测试用例

| 输入       | 算法         | 交换次数 | 观察结果               |
| ---------- | ------------ | -------- | ---------------------- |
| [3, 2, 1]  | 冒泡排序     | 3        | 完全逆序               |
| [1, 2, 3]  | 冒泡排序     | 0        | 已排序                 |
| [2, 3, 1]  | 插入排序     | 2        | 移动最小元素           |
| [3, 1, 2]  | 选择排序     | 2        | 每个位置交换一次       |

#### 复杂度

| 指标          | 成本       | 备注                           |
| ------------- | ---------- | ------------------------------ |
| 时间（跟踪）  | $O(1)$     | 每次交换递增计数器             |
| 总交换次数    | $O(n^2)$   | 冒泡排序的最坏情况             |
| 空间          | $O(1)$     | 恒定的额外内存                 |

交换计数器提供了一个清晰的窗口来观察排序动态，揭示了算法工作的“努力”程度以及输入距离有序状态有多远。
### 62 逆序对计数器

逆序对计数器通过统计所有顺序错误的数对，来衡量一个序列距离有序状态有多远。它是无序程度的一种数值度量：有序列表的逆序对数为零，完全逆序的列表则达到最大值。

#### 我们要解决什么问题？

排序算法修正的是*逆序对*。每个逆序对是一个数对 $(i, j)$，满足 $i < j$ 且 $a_i > a_j$。
统计逆序对可以为我们提供：

- 无序程度的量化度量
- 分析算法进展的一种方法
- 对最好情况与最坏情况行为的洞察

这个度量也用于肯德尔 tau 距离、排名比较和自适应排序研究。

#### 工作原理（通俗解释）

1.  取一个数组 $A = [a_1, a_2, \ldots, a_n]$。
2.  对于每个满足 $i < j$ 的数对 $(i, j)$，检查是否 $a_i > a_j$。
3.  每找到一个逆序对，计数加一。
4.  有序数组的逆序对数为 $0$；完全逆序的数组为 $\frac{n(n-1)}{2}$。

#### 逐步示例

数组: [3, 1, 2]

- (3, 1): 是逆序对
- (3, 2): 是逆序对
- (1, 2): 不是逆序对

逆序对总数: 2

一个完美的诊断工具：计数小 → 接近有序。

#### 简单代码（暴力法）

```python
def count_inversions_bruteforce(arr):
    count = 0
    n = len(arr)
    for i in range(n):
        for j in range(i + 1, n):
            if arr[i] > arr[j]:
                count += 1
    return count
```

输出：
`count_inversions_bruteforce([3, 1, 2])` → `2`

#### 优化方法（归并排序）

通过修改归并排序，可以在 $O(n \log n)$ 时间内统计逆序对。

```python
def count_inversions_merge(arr):
    def merge_count(left, right):
        i = j = inv = 0
        merged = []
        while i < len(left) and j < len(right):
            if left[i] <= right[j]:
                merged.append(left[i])
                i += 1
            else:
                merged.append(right[j])
                inv += len(left) - i
                j += 1
        merged += left[i:]
        merged += right[j:]
        return merged, inv

    def sort_count(sub):
        if len(sub) <= 1:
            return sub, 0
        mid = len(sub) // 2
        left, invL = sort_count(sub[:mid])
        right, invR = sort_count(sub[mid:])
        merged, invM = merge_count(left, right)
        return merged, invL + invR + invM

    _, total = sort_count(arr)
    return total
```

结果：时间复杂度为 $O(n \log n)$，而不是 $O(n^2)$。

#### 为什么它很重要

- 精确量化无序程度
- 用于排序网络分析
- 预测自适应排序算法在最好情况下的改进
- 与排名相关性度量相关联

#### 一个温和的证明（为什么它有效）

在稳定排序中，每次交换恰好修正一个逆序对。
如果我们用 $I$ 表示逆序对总数：

$$
I_{\text{有序}} = 0, \quad I_{\text{逆序}} = \frac{n(n-1)}{2}
$$

因此，逆序对计数衡量的是*距离有序状态的距离*，这是任何基于比较的排序算法所需交换次数的一个下界。

#### 亲自尝试

1.  统计有序、逆序和随机数组的逆序对。
2.  绘制逆序对计数与交换次数的关系图。
3.  测试归并排序计数器与暴力计数器的性能。
4.  测量逆序对计数如何影响自适应算法。

#### 测试用例

| 输入        | 逆序对数 | 解释                 |
| ----------- | -------- | -------------------- |
| [1, 2, 3]   | 0        | 已经有序             |
| [3, 2, 1]   | 3        | 完全逆序             |
| [2, 3, 1]   | 2        | 两个数对顺序错误     |
| [1, 3, 2]   | 1        | 轻微无序             |

#### 复杂度

| 方法           | 时间复杂度  | 空间复杂度 | 备注                     |
| -------------- | ----------- | ---------- | ------------------------ |
| 暴力法         | $O(n^2)$    | $O(1)$     | 简单但慢                 |
| 基于归并排序   | $O(n \log n)$ | $O(n)$     | 适用于大型数组的高效方法 |

逆序对计数器将“这个列表有多有序？”转化为一个精确的数字，非常适合用于分析、比较和设计更智能的排序算法。
### 63 稳定性检查器

稳定性检查器用于验证排序算法是否保持相等元素的相对顺序。在对具有多个键的复杂记录进行排序时，稳定性至关重要，它能确保在以主键排序后，次要属性仍保持有序。

#### 我们要解决什么问题？

在排序时，有时值会"打平"，即它们在主键下是相等的。
稳定的排序会保持这些打平元素的原始顺序。
例如，按成绩对学生排序，同时保持之前输入姓名的顺序。

如果没有稳定性，按多个键排序就容易出错，并且链式排序可能会失去意义。

#### 工作原理（通俗解释）

1.  用其原始位置标记每个元素。
2.  执行排序。
3.  排序后，对于所有具有相等键的元素对，检查原始索引是否保持升序。
4.  如果是，则算法是稳定的。否则，就是不稳定的。

#### 逐步示例

带标签的数组：
`[(A, 3), (B, 1), (C, 3)]`
按值排序 → `[ (B, 1), (A, 3), (C, 3) ]`

检查打平的元素：

*   值为 `3` 的元素：A 在 C 之前，并且 A 的原始索引 < C 的原始索引 → 稳定。

如果结果是 `[ (B, 1), (C, 3), (A, 3) ]`，相等元素的顺序颠倒了 → 不稳定。

#### 微型代码（Python）

```python
def is_stable_sort(original, sorted_arr, key=lambda x: x):
    positions = {}
    for idx, val in enumerate(original):
        positions.setdefault(key(val), []).append(idx)
    
    last_seen = {}
    for val in sorted_arr:
        k = key(val)
        pos = positions[k].pop(0)
        if k in last_seen and last_seen[k] > pos:
            return False
        last_seen[k] = pos
    return True
```

用法：

```python
data = [('A', 3), ('B', 1), ('C', 3)]
sorted_data = sorted(data, key=lambda x: x[1])
is_stable_sort(data, sorted_data, key=lambda x: x[1])  # True
```

#### 为什么它很重要

*   保持次要顺序：对于多键排序至关重要
*   链式排序安全：逐步按多个字段排序
*   结果可预测：避免相等元素的随机重排
*   常见属性：归并排序、插入排序稳定；快速排序不稳定（默认情况下）

#### 一个温和的证明（为什么它有效）

令 $a_i$ 和 $a_j$ 为具有相等键 $k$ 的元素。
如果在输入中 $i < j$，并且排序后 $a_i$ 和 $a_j$ 的位置是 $p_i$ 和 $p_j$，
那么当且仅当满足以下条件时，算法是稳定的：

$$
i < j \implies p_i < p_j \text{ whenever } k_i = k_j
$$

检查所有打平键的这个性质即可确认稳定性。

#### 自己动手试试

1.  比较稳定排序（归并排序）与非稳定排序（选择排序）。
2.  按一个键对元组列表排序，检查打平元素的保持情况。
3.  进行链式排序（先按姓，再按名）。
4.  运行检查器以确认最终的稳定性。

#### 测试用例

| 输入                    | 排序结果                | 稳定？ | 解释                       |
| ----------------------- | ----------------------- | ------ | -------------------------- |
| [(A,3),(B,1),(C,3)]     | [(B,1),(A,3),(C,3)]     | 是     | A 在 C 之前得以保持        |
| [(A,3),(B,1),(C,3)]     | [(B,1),(C,3),(A,3)]     | 否     | A 和 C 的顺序颠倒了        |
| [(1,10),(2,10),(3,10)]  | [(1,10),(2,10),(3,10)]  | 是     | 全部打平，全部保持         |

#### 复杂度

| 操作       | 时间     | 空间    | 备注                          |
| ---------- | -------- | ------- | ----------------------------- |
| 检查       | $O(n)$   | $O(n)$  | 遍历排序后的数组一次           |
| 排序       | 取决于   |,        | 检查器独立于排序算法本身       |

稳定性检查器确保你的排序尊重相等元素之间的顺序，这是保障多键排序正确性和可解释性的一小步。
### 64 比较网络可视化工具

比较网络可视化工具展示了固定的比较序列如何对元素进行排序，揭示了排序网络的结构。这些图表帮助我们逐步理解并行排序的工作原理，且独立于输入数据。

#### 我们正在解决什么问题？

排序网络是数据无关的，它们的比较序列是固定的，不由数据驱动。
为了理解或设计它们，我们需要一个清晰的视觉图来展示哪些元素在何时进行比较。
可视化工具将抽象的序列比较转化为分层的网络图。

这对于以下方面至关重要：

- 分析并行排序
- 设计基于硬件的排序器
- 研究双调或奇偶归并

#### 工作原理（通俗解释）

1.  将每个元素表示为一条水平线（导线）。
2.  画一条垂直的比较器线，连接正在进行比较的两条导线。
3.  将比较器分组到可以并行运行的层中。
4.  网络逐层执行，如果元素顺序不对则进行交换。

结果：得到排序逻辑的视觉图。

#### 逐步示例

使用双调排序网络对 4 个元素进行排序：

```
第一层：比较 (0,1), (2,3)
第二层：比较 (0,2), (1,3)
第三层：比较 (1,2)
```

可视化：

```
0 ──●────┐─────●───
1 ──●─┐──┼──●──┼───
2 ───┼─●──●─┘──●───
3 ───┼────●────┘───
```

每个点对 = 一个比较器。结构是静态的，与数值无关。

#### 微型代码（Python）

```python
def visualize_network(n, layers):
    wires = [['─'] * (len(layers) + 1) for _ in range(n)]

    for layer_idx, layer in enumerate(layers):
        for (i, j) in layer:
            wires[i][layer_idx] = '●'
            wires[j][layer_idx] = '●'
    for i in range(n):
        print(f"{i}: " + "─".join(wires[i]))

layers = [[(0,1), (2,3)], [(0,2), (1,3)], [(1,2)]]
visualize_network(4, layers)
```

这段代码会打印出比较器层的符号化可视化图。

#### 为什么这很重要

- 揭示排序逻辑中的并行性
- 帮助调试数据无关算法
- 对硬件和 GPU 设计很有用
- 是双调排序、奇偶归并和 Batcher 网络的基础

#### 一个温和的证明（为什么它有效）

如果一个排序网络能正确排序所有长度为 $n$ 的二进制序列，那么它就保证了正确性。

根据 0-1 原理：

> 如果一个比较网络能正确排序所有由 0 和 1 组成的序列，那么它就能正确排序所有任意数字的序列。

因此，可视化比较器确保了完整性和层的正确性。

#### 亲自尝试

1.  画一个 4 输入的双调排序网络。
2.  可视化比较器如何"流经"各层。
3.  检查有多少层可以并行运行。
4.  手动测试 0/1 序列通过网络后的排序情况。

#### 测试用例

| 输入       | 网络类型       | 层数 | 排序后输出   |
| ---------- | -------------- | ---- | ------------ |
| [3,1,4,2] | 双调排序       | 3    | [1,2,3,4]    |
| [1,0,1,0] | 奇偶归并排序   | 3    | [0,0,1,1]    |

#### 复杂度

| 指标       | 值               | 备注                           |
| ---------- | ---------------- | ------------------------------ |
| 比较器数量 | $O(n \log^2 n)$  | Batcher 网络的复杂度           |
| 深度       | $O(\log^2 n)$    | 并行执行的层数                 |
| 空间       | $O(n)$           | 每个输入对应一条导线           |

比较网络可视化工具使并行排序变得具体，每个比较器和每一层都清晰可见，将抽象的硬件逻辑转化为清晰、具有教育意义的蓝图。
### 65 自适应排序检测器

自适应排序检测器用于测量输入序列的“已排序”程度，并预测算法是否能利用这一点。它是一种诊断工具，用于估计预排序程度并指导自适应排序算法的选择。

#### 我们要解决什么问题？

并非所有输入都是随机的，许多输入是部分有序的。
一些算法（如插入排序或 Timsort）在接近有序的数据上表现得更快。
我们需要一种方法，在选择正确的策略之前检测数据的排序程度。

自适应检测器量化了输入接近有序的程度。

#### 工作原理（通俗解释）

1.  定义一个无序度量标准（例如，逆序对数量、游程数量或局部错位数量）。
2.  遍历数组，计算无序指标。
3.  返回一个度量值（例如，0 = 完全有序，1 = 完全逆序）。
4.  使用这个分数来决定应用哪种算法：
    *   简单的类插入排序（适用于接近有序的数据）
    *   通用排序算法（适用于随机数据）

#### 逐步示例

数组：[1, 2, 4, 3, 5, 6]

1.  比较相邻元素对：
    *   1 ≤ 2 (正常)
    *   2 ≤ 4 (正常)
    *   4 > 3 (无序)
    *   3 ≤ 5 (正常)
    *   5 ≤ 6 (正常)
2.  计数 = 1 个局部逆序对

有序度分数：
$$
s = 1 - \frac{\text{无序度}}{n-1} = 1 - \frac{1}{5} = 0.8
$$

80% 有序，是自适应排序的良好候选。

#### 微型代码（Python）

```python
def adaptive_sort_detector(arr):
    disorder = 0
    for i in range(len(arr) - 1):
        if arr[i] > arr[i + 1]:
            disorder += 1
    return 1 - disorder / max(1, len(arr) - 1)

arr = [1, 2, 4, 3, 5, 6]
score = adaptive_sort_detector(arr)
# score = 0.8
```

你可以使用这个分数来动态选择算法。

#### 为什么它很重要

-   高效检测接近有序的输入
-   支持在运行时选择算法
-   节省处理真实世界数据（日志、流、合并结果）的时间
-   Timsort 中游程检测背后的核心思想

#### 一个温和的证明（为什么它有效）

如果一个算法的时间复杂度依赖于无序度 $d$，例如 $O(n + d)$，
并且对于接近有序的数组有 $d = O(1)$，
那么自适应算法就接近线性时间。

检测器近似估计 $d$，帮助我们判断何时 $O(n + d)$ 优于 $O(n \log n)$。

#### 自己动手试试

1.  测试无序度为 0%、10%、50% 和 100% 的数组。
2.  比较插入排序与归并排序的运行时间。
3.  使用逆序对计数进行更精确的检测。
4.  将检测器集成到混合排序例程中。

#### 测试用例

| 输入          | 无序度 | 分数 | 推荐算法         |
| ------------- | ------ | ---- | ---------------- |
| [1,2,3,4,5]   | 0      | 1.0  | 插入排序         |
| [1,3,2,4,5]   | 1      | 0.8  | 自适应排序       |
| [3,2,1]       | 2      | 0.0  | 归并 / 快速排序  |
| [2,1,3,5,4]   | 2      | 0.6  | 自适应排序       |

#### 复杂度

| 操作           | 时间     | 空间    | 备注                         |
| -------------- | -------- | ------- | ---------------------------- |
| 无序度检查     | $O(n)$   | $O(1)$  | 单次扫描                     |
| 排序（所选算法） | 自适应   | ,       | 取决于所选算法               |

自适应排序检测器连接了理论与实践，量化了数据的排序程度，并为现实世界的性能表现指导更智能的算法选择。
### 66 排序不变式检查器

排序不变式检查器用于验证排序算法执行过程中关键排序条件是否始终成立。它用于逐步推理算法的正确性，确保每次迭代都朝着完全有序数组的方向保持进展。

#### 我们要解决什么问题？

在调试或证明排序算法的正确性时，我们需要确保某些不变式（必须始终成立的条件）保持为真。
如果任何不变式被破坏，即使算法乍一看"似乎"正确，也可能产生错误的输出。

排序不变式形式化了"部分进展"的含义。
例如：

- "索引 `i` 之前的所有元素都处于有序状态。"
- "基准（pivot）之后的所有元素都大于或等于它。"
- "每个节点都满足堆性质。"

#### 它是如何工作的？（通俗解释）

1.  定义一个或多个描述正确性的不变式。
2.  在每次迭代或递归步骤之后，检查这些不变式是否仍然成立。
3.  如果任何不变式失败，则停止并调试，说明算法逻辑有误。
4.  排序完成后，全局不变式（数组已排序）必须成立。

这种方法对于形式化验证和可调试代码至关重要。

#### 逐步示例

插入排序的不变式：

> 在处理元素 `i` 之前，子数组 `arr[:i]` 是已排序的。

-   初始时 `i = 1`：子数组 `[arr[0]]` 是已排序的。
-   插入 `arr[1]` 后，子数组 `[arr[0:2]]` 是已排序的。
-   根据归纳法，最终整个数组排序完成。

每次插入后进行检查：
`assert arr[:i] == sorted(arr[:i])`

#### 微型代码（Python）

```python
def insertion_sort_with_invariant(arr):
    for i in range(1, len(arr)):
        key = arr[i]
        j = i - 1
        while j >= 0 and arr[j] > key:
            arr[j + 1] = arr[j]
            j -= 1
        arr[j + 1] = key
        # 检查不变式
        assert arr[:i+1] == sorted(arr[:i+1]), f"不变式在 i={i} 处被破坏"
    return arr
```

如果不变式失败，断言错误会揭示出具体的迭代步骤。

#### 为什么它很重要？

-   通过归纳法构建正确性证明
-   早期错误检测，精确定位迭代错误
-   阐明算法意图
-   教授关于程序逻辑的结构化推理

应用于：

-   形式化证明（循环不变式）
-   算法验证
-   教学与分析

#### 一个温和的证明（为什么它有效）

令 $P(i)$ 表示不变式"长度为 $i$ 的前缀是已排序的。"

-   基本情况：$P(1)$ 显然成立。
-   归纳步骤：如果 $P(i)$ 成立，则插入下一个元素后 $P(i+1)$ 仍为真。

根据归纳法，$P(n)$ 成立，即整个数组已排序。

因此，如果每个步骤都保持真值，则不变式框架保证了正确性。

#### 动手尝试

1.  为选择排序添加不变式（"最小元素已放置在索引 i 处"）。
2.  为堆排序添加堆性质不变式。
3.  在测试套件中运行断言。
4.  当不变式失败时，使用 `try/except` 记录日志而不是直接停止。

#### 测试用例

| 算法         | 不变式                                   | 成立？ | 备注                       |
| ------------ | ---------------------------------------- | ------ | -------------------------- |
| 插入排序     | 每个步骤的前缀已排序                     | 是     | 经典的归纳不变式           |
| 选择排序     | 最小元素已放置在位置 i                   | 是     | 迭代验证                   |
| 快速排序     | 基准划分后满足 左 ≤ 基准 ≤ 右            | 是     | 必须在划分后成立           |
| 冒泡排序     | 最大元素冒泡到正确位置                   | 是     | 每次完整遍历后             |

#### 复杂度

| 检查类型 | 时间复杂度   | 空间复杂度                 | 备注                 |
| -------- | ------------ | -------------------------- | -------------------- |
| 断言检查 | $O(k)$       | $O(1)$                     | 对于前缀长度 $k$     |
| 总成本   | $O(n^2)$ 最坏 | 对于嵌套的不变式检查       |                      |

排序不变式检查器将正确性从直觉转化为逻辑，强制执行顺序、证明有效性，并逐个不变式地阐明排序算法的结构。
### 67 分布直方图排序演示

分布直方图排序演示可视化在基于分布的排序过程中，元素如何分散到各个桶（buckets）或区间（bins）中。它通过展示在最终排序前如何组织数值，帮助学习者理解计数排序、基数排序或桶排序*为何*以及*如何*实现线性时间行为。

#### 我们要解决什么问题？

基于分布的排序（计数排序、桶排序、基数排序）不依赖于成对比较。
相反，它们根据键值或数字将元素分类到不同的桶中。
理解这些算法需要可视化数据是如何在各个类别中分布的，而直方图正好捕捉了这一过程。

本演示展示了：

-   如何收集计数
-   如何通过前缀和将计数转换为位置
-   如何按顺序重建项目以得到排序结果

#### 工作原理（通俗解释）

1.  初始化桶，每个键值或范围对应一个桶。
2.  遍历输入数组，并在正确的桶中增加计数。
3.  可视化得到的频率直方图。
4.  （可选）应用前缀和来展示累积位置。
5.  按顺序读取桶中的元素来重建输出数组。

这种可视化将计数逻辑与最终的排序数组联系起来。

#### 逐步示例

示例：对 `[2, 1, 2, 0, 1]` 进行计数排序

| 值 | 计数 |
| --- | --- |
| 0 | 1 |
| 1 | 2 |
| 2 | 2 |

前缀和 → `[1, 3, 5]`
重建数组 → `[0, 1, 1, 2, 2]`

直方图清晰地显示了每组值最终会出现在哪里。

#### 微型代码（Python）

```python
import matplotlib.pyplot as plt

def histogram_sort_demo(arr, max_value):
    counts = [0] * (max_value + 1)
    for x in arr:
        counts[x] += 1
    
    plt.bar(range(len(counts)), counts)
    plt.xlabel("值")
    plt.ylabel("频率")
    plt.title("计数排序的分布直方图")
    plt.show()
    
    # 可选的重建步骤
    sorted_arr = []
    for val, freq in enumerate(counts):
        sorted_arr.extend([val] * freq)
    return sorted_arr
```

示例：

```python
histogram_sort_demo([2, 1, 2, 0, 1], 2)
```

#### 为何重要

-   使非比较排序变得直观易懂
-   展示数据频率模式
-   在计数和位置分配之间架起桥梁
-   有助于直观解释 $O(n + k)$ 复杂度

#### 一个温和的证明（为何有效）

每个值的频率 $f_i$ 精确地决定了它出现的次数。
通过对计数进行前缀和计算：

$$
p_i = \sum_{j < i} f_j
$$

我们为每个值分配了唯一的输出位置，从而确保了在线性时间内得到稳定且正确的排序顺序。

因此，排序变成了位置映射，而非比较。

#### 亲自尝试

1.  为随机数组、已排序数组和均匀分布数组绘制直方图。
2.  比较桶排序中的桶大小与基数排序中的数字位置。
3.  在直方图柱状图上添加前缀和标签。
4.  逐步动画演示输出数组的重建过程。

#### 测试用例

| 输入 | 最大值 | 直方图 | 排序输出 |
| --- | --- | --- | --- |
| [2,1,2,0,1] | 2 | [1,2,2] | [0,1,1,2,2] |
| [3,3,3,3] | 3 | [0,0,0,4] | [3,3,3,3] |
| [0,1,2,3] | 3 | [1,1,1,1] | [0,1,2,3] |

#### 复杂度

| 操作 | 时间 | 空间 | 备注 |
| --- | --- | --- | --- |
| 计数 | $O(n)$ | $O(k)$ | $k$ = 桶的数量 |
| 前缀和 | $O(k)$ | $O(k)$ | 对计数进行单次遍历 |
| 重建 | $O(n + k)$ | $O(n + k)$ | 构建排序数组 |

分布直方图排序演示将抽象的计数逻辑转化为具体的视觉呈现，展示了频率如何塑造顺序，并使线性时间排序变得一目了然。
### 68 键提取函数

键提取函数从数据元素中分离出决定其排序位置的具体特征或属性。它是实现灵活、可重用排序逻辑的基础工具，使算法能够处理复杂的记录、元组或自定义对象。

#### 我们要解决什么问题？

对现实世界的数据进行排序通常涉及结构化元素、元组、对象或字典，而不仅仅是数字。
我们很少直接对整个元素进行排序；相反，我们通过一个键来排序：

- 按字母顺序排列姓名
- 按数字大小排列年龄
- 按时间顺序排列日期

键提取器定义了*如何查看*每个项目以进行比较，将*数据*与*排序*解耦。

#### 工作原理（通俗解释）

1.  定义一个键函数：`key(x)` → 提取可排序的属性。
2.  在比较过程中应用键函数。
3.  算法基于这些提取的值进行排序。
4.  原始元素保持不变，只有它们的顺序发生变化。

#### 分步示例

假设你有：

```python
students = [
    ("Alice", 22, 3.8),
    ("Bob", 20, 3.5),
    ("Clara", 21, 3.9)
]
```

要按年龄排序，使用 `key=lambda x: x[1]`。
要按 GPA（降序）排序，使用 `key=lambda x: -x[2]`。

结果：

- 按年龄 → `[("Bob", 20, 3.5), ("Clara", 21, 3.9), ("Alice", 22, 3.8)]`
- 按 GPA → `[("Clara", 21, 3.9), ("Alice", 22, 3.8), ("Bob", 20, 3.5)]`

#### 微型代码（Python）

```python
def sort_by_key(data, key):
    return sorted(data, key=key)

students = [("Alice", 22, 3.8), ("Bob", 20, 3.5), ("Clara", 21, 3.9)]

# 按年龄排序
result = sort_by_key(students, key=lambda x: x[1])
# 按 GPA 降序排序
result2 = sort_by_key(students, key=lambda x: -x[2])
```

这种抽象允许实现简洁、可重用的排序。

#### 为什么它很重要

-   分离逻辑：比较机制与数据结构
-   可重用性：一种算法，多种排序方式
-   可组合性：通过链式键实现多级排序
-   稳定性协同：稳定排序 + 键提取 = 多键排序

#### 一个温和的证明（为什么它有效）

令 $f(x)$ 为键提取器。
我们基于 $f(x)$ 排序，而不是 $x$。
如果比较器满足：

$$
f(x_i) \le f(x_j) \implies x_i \text{ 在 } x_j \text{ 之前}
$$

那么得到的顺序就符合预期的属性。
因为 $f$ 是确定性的，排序的正确性直接源于比较器的正确性。

#### 自己动手试试

1.  按长度对字符串排序：`key=len`
2.  按字段对字典列表排序：`key=lambda d: d['score']`
3.  组合键：`key=lambda x: (x.grade, x.name)`
4.  结合稳定性来模拟 SQL 的 "ORDER BY"

#### 测试用例

| 输入                         | 键                 | 结果                         |
| ---------------------------- | ------------------ | ---------------------------- |
| [("A",3),("B",1),("C",2)]    | `lambda x:x[1]`    | [("B",1),("C",2),("A",3)]    |
| ["cat","a","bird"]           | `len`              | ["a","cat","bird"]           |
| [{"x":5},{"x":2},{"x":4}]    | `lambda d:d["x"]`  | [{"x":2},{"x":4},{"x":5}]    |

#### 复杂度

| 步骤           | 时间             | 空间   | 备注                       |
| -------------- | ---------------- | ------ | -------------------------- |
| 键提取         | $O(n)$           | $O(1)$ | 每个元素调用一次           |
| 排序           | $O(n \log n)$    | $O(n)$ | 取决于使用的算法           |
| 组合           | $O(k \cdot n)$   | $O(1)$ | 对于多键链式操作           |

键提取函数是原始数据与自定义顺序之间的桥梁，使算法不仅能对数字排序，还能对意义进行排序。
### 69 偏序集构建器

偏序集（Poset）构建器构建一种视觉和逻辑模型，用于表示元素之间定义*偏序*的关系，其中一些项目可以比较，而另一些则不能。这是一个用于理解排序约束、依赖图以及优先结构的概念性工具。

#### 我们要解决什么问题？

并非所有集合都具有全序。
有时只有部分比较是有意义的，例如：

- 任务依赖关系（A 在 B 之前，C 独立）
- 版本控制合并
- 有向无环图中的拓扑排序

偏序集捕捉了这些关系：

- 自反性：每个元素 ≤ 自身
- 反对称性：如果 A ≤ B 且 B ≤ A，则 A = B
- 传递性：如果 A ≤ B 且 B ≤ C，则 A ≤ C

构建偏序集有助于我们在尝试排序或调度之前可视化约束。

#### 工作原理（通俗解释）

1.  定义元素间的关系（≤）。
2.  构建一个图，其中边 A → B 表示 "A ≤ B"。
3.  确保自反性、反对称性和传递性。
4.  将结果可视化为哈斯图（省略冗余边）。
5.  使用此结构寻找线性扩展（有效的排序顺序）。

#### 分步示例

示例：假设我们有具有依赖关系的任务：

```
A ≤ B, A ≤ C, B ≤ D, C ≤ D
```

构建偏序集：

- 节点：A, B, C, D
- 边：A→B, A→C, B→D, C→D

哈斯图：

```
   D
  / \
 B   C
  \ /
   A
```

可能的总序（线性扩展）：

- A, B, C, D
- A, C, B, D

#### 微型代码（Python）

```python
from collections import defaultdict

def build_poset(relations):
    graph = defaultdict(list)
    for a, b in relations:
        graph[a].append(b)
    return graph

relations = [('A', 'B'), ('A', 'C'), ('B', 'D'), ('C', 'D')]
poset = build_poset(relations)
for k, v in poset.items():
    print(f"{k} → {v}")
```

输出：

```
A → ['B', 'C']
B → ['D']
C → ['D']
```

你可以使用像 `networkx` 这样的工具来扩展它以进行可视化。

#### 为什么重要

- 建模依赖关系和优先顺序
- 拓扑排序的基础
- 解释为什么全序并非总是可能
- 阐明调度中的约束满足

应用于：

- 构建系统（make, DAGs）
- 任务规划
- 编译器依赖分析

#### 温和的证明（为什么有效）

偏序集 $(P, \le)$ 满足三条公理：

1.  自反性：$\forall x, x \le x$
2.  反对称性：$(x \le y \land y \le x) \implies x = y$
3.  传递性：$(x \le y \land y \le z) \implies x \le z$

这些属性确保了一致的结构。
对偏序集进行排序意味着找到与所有 $\le$ 关系一致的线性扩展，而拓扑排序对有向无环图保证了这一点。

#### 动手尝试

1.  定义具有先决条件的任务。
2.  绘制哈斯图。
3.  执行拓扑排序以列出有效的总序。
4.  添加额外关系，检查是否破坏反对称性。

#### 测试用例

| 关系                         | 偏序集边          | 线性顺序               |
| ---------------------------- | ----------------- | ---------------------- |
| A ≤ B, A ≤ C, B ≤ D, C ≤ D   | A→B, A→C, B→D, C→D | [A,B,C,D], [A,C,B,D]   |
| A ≤ B, B ≤ C, A ≤ C          | A→B, B→C, A→C     | [A,B,C]                |
| A ≤ B, B ≤ A (无效)          |                   | 违反反对称性           |

#### 复杂度

| 操作             | 时间复杂度 | 空间复杂度 | 备注                               |
| ---------------- | ---------- | ---------- | ---------------------------------- |
| 构建关系图       | $O(E)$     | $O(V)$     | $E$ = 关系数量                     |
| 检查反对称性     | $O(E)$     | $O(V)$     | 检测环或双向边                     |
| 拓扑排序         | $O(V + E)$ | $O(V)$     | 用于寻找线性扩展                   |

偏序集构建器将抽象的排序约束转化为结构化的见解，不仅展示了*什么先出现*，还展示了*什么可以共存*。
### 70 复杂度比较器

复杂度比较器通过直接比较算法的时间或空间复杂度函数，帮助我们理解不同算法如何随规模变化。它是一种直观的工具：当 $n$ 增大时，$O(n)$ 与 $O(n \log n)$ 或 $O(2^n)$ 相比如何？

#### 我们要解决什么问题？

当面对多个解决同一问题的算法时，我们必须决定哪个算法对于大规模输入更高效。
与其猜测，不如比较它们复杂度函数的增长率。

示例：
$O(n^2)$ 比 $O(n \log n)$ 慢吗？
对于小的 $n$，可能不是。但随着 $n \to \infty$，$n^2$ 增长更快，因此 $O(n \log n)$ 算法在渐近意义上更好。

#### 工作原理（通俗解释）

1.  定义代表其成本的两个函数 $f(n)$ 和 $g(n)$。
2.  计算当 $n \to \infty$ 时的比值 $\frac{f(n)}{g(n)}$。
3.  解释极限：
    *   如果 $\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0$，则 $f(n) = o(g(n))$（增长更慢）。
    *   如果极限是 $\infty$，则 $f(n) = \omega(g(n))$（增长更快）。
    *   如果极限是常数，则 $f(n) = \Theta(g(n))$（增长相同）。
4.  使用图表或小 $n$ 值表格进行可视化，以理解交叉点。

#### 逐步示例

比较 $f(n) = n \log n$ 和 $g(n) = n^2$：

-   计算比值：$\frac{f(n)}{g(n)} = \frac{n \log n}{n^2} = \frac{\log n}{n}$。
-   当 $n \to \infty$ 时，$\frac{\log n}{n} \to 0$。
    因此，$f(n) = o(g(n))$。

解释：$O(n \log n)$ 比 $O(n^2)$ 增长更慢，因此其可扩展性更好。

#### 微型代码（Python）

```python
import math

def compare_growth(f, g, n_values):
    for n in n_values:
        print(f"n={n:6d} f(n)={f(n):10.2f} g(n)={g(n):10.2f} ratio={f(n)/g(n):10.6f}")

compare_growth(lambda n: n * math.log2(n),
               lambda n: n2,
               [2, 4, 8, 16, 32, 64, 128])
```

输出显示 $\frac{f(n)}{g(n)}$ 如何随 $n$ 减小。

#### 为何重要

-   使渐近比较变得可视化和数值化
-   揭示实际输入规模下的交叉点
-   帮助在多个实现之间进行选择
-   加深对扩展规律的理解

#### 一个温和的证明（为何有效）

我们依赖于极限比较：

如果 $\lim_{n \to \infty} \frac{f(n)}{g(n)} = c$：

-   如果 $0 < c < \infty$，则 $f(n) = \Theta(g(n))$
-   如果 $c = 0$，则 $f(n) = o(g(n))$
-   如果 $c = \infty$，则 $f(n) = \omega(g(n))$

这源于渐近记号的正式定义，确保了比较的一致性。

#### 亲自尝试

1.  比较 $O(n^2)$ 与 $O(n^3)$
2.  比较 $O(n \log n)$ 与 $O(n^{1.5})$
3.  比较 $O(2^n)$ 与 $O(n!)$
4.  使用 Python 或 Excel 绘制它们的增长曲线

#### 测试用例

| $f(n)$     | $g(n)$     | $n \to \infty$ 时的比值 | 关系               |
| ---------- | ---------- | ----------------------- | ------------------ |
| $n$        | $n \log n$ | $0$                     | $n = o(n \log n)$  |
| $n \log n$ | $n^2$      | $0$                     | $n \log n = o(n^2)$ |
| $n^2$      | $n^2$      | $1$                     | $\Theta$           |
| $2^n$      | $n^3$      | $\infty$                | $2^n = \omega(n^3)$ |

#### 复杂度

| 操作               | 时间     | 空间     | 备注                         |
| ------------------ | -------- | -------- | ---------------------------- |
| 函数比值计算       | $O(1)$   | $O(1)$   | 常数时间比较                 |
| 经验表格生成       | $O(k)$   | $O(k)$   | 针对 $k$ 个采样点            |
| 绘图可视化         | $O(k)$   | $O(k)$   | 帮助理解交叉点               |

复杂度比较器是您获得渐近洞察力的透镜，它不仅显示哪个算法更快，还揭示其*为何*扩展性更好。

## 第 8 节 数据结构概述
### 71 栈模拟

栈模拟让我们能够一步步观察入栈（push）和出栈（pop）操作是如何展开的，从而揭示这种简单而强大的数据结构所具有的 LIFO（后进先出）特性。

#### 我们要解决什么问题？

栈无处不在：递归、表达式求值、回溯、函数调用中都有它的身影。
但对于初学者来说，其动态行为可能感觉很抽象。
模拟使其具体化，每一次入栈都添加一层，每一次出栈都移除一层。

目标：理解元素如何以及何时进入和离开栈，以及为什么顺序很重要。

#### 它是如何工作的（通俗解释）

1.  从一个空栈开始。
2.  Push(x)：将元素 `x` 添加到栈顶。
3.  Pop()：移除栈顶元素。
4.  Peek()（可选）：查看栈顶元素但不移除它。
5.  最近入栈的元素总是最先被移除。

想象一叠盘子：你只能从最上面拿取。

#### 逐步示例

操作序列：

```
Push(10)
Push(20)
Push(30)
Pop()
Push(40)
```

栈的演变：

| 步骤 | 操作      | 栈状态（顶 → 底） |
| ---- | --------- | ----------------- |
| 1    | Push(10)  | 10                 |
| 2    | Push(20)  | 20, 10             |
| 3    | Push(30)  | 30, 20, 10         |
| 4    | Pop()     | 20, 10             |
| 5    | Push(40)  | 40, 20, 10         |

#### 微型代码（Python）

```python
class Stack:
    def __init__(self):
        self.data = []

    def push(self, x):
        self.data.append(x)
        print(f"Pushed {x}: {self.data[::-1]}")

    def pop(self):
        if self.data:
            x = self.data.pop()
            print(f"Popped {x}: {self.data[::-1]}")
            return x

# 演示
s = Stack()
s.push(10)
s.push(20)
s.push(30)
s.pop()
s.push(40)
```

每个操作都会打印当前状态，模拟栈的行为。

#### 为什么这很重要

-   为函数调用和递归建模
-   是实现撤销操作和回溯的基础
-   支撑表达式解析和求值
-   培养对控制流和内存帧的直觉

#### 一个温和的证明（为什么它有效）

栈强制实施 LIFO 顺序：
如果你按顺序 $a_1, a_2, \ldots, a_n$ 入栈元素，
你必须以相反的顺序出栈它们：$a_n, \ldots, a_2, a_1$。

形式上，每次入栈使大小增加 1，每次出栈使大小减少 1，
确保 $|S| = \text{入栈次数} - \text{出栈次数}$，并且顺序自然地反转。

#### 亲自尝试

1.  模拟后缀表达式求值（`3 4 + 5 *`）
2.  跟踪递归函数调用（阶乘或斐波那契）
3.  使用栈实现浏览器后退功能
4.  入栈字符串并出栈以反转顺序

#### 测试用例

| 操作序列                          | 最终栈状态（顶 → 底） |
| --------------------------------- | --------------------- |
| Push(1), Push(2), Pop()           | 1                     |
| Push('A'), Push('B'), Push('C')   | C, B, A               |
| Push(5), Pop(), Pop()             | （空）                |
| Push(7), Push(9), Push(11), Pop() | 9, 7                  |

#### 复杂度

| 操作    | 时间复杂度 | 空间复杂度 | 说明             |
| ------- | ---------- | ---------- | ---------------- |
| Push(x) | O(1)       | O(n)       | 追加到列表末尾   |
| Pop()   | O(1)       | O(n)       | 移除最后一个元素 |
| Peek()  | O(1)       | O(n)       | 访问最后一个元素 |

栈模拟使抽象的次序变得有形，每一次入栈和出栈都讲述着关于控制、内存和流程的故事。
### 72 队列模拟

队列模拟展示了元素如何在先进先出的结构中移动，非常适合模拟排队等待、作业调度或数据流。

#### 我们解决什么问题？

队列体现了公平性和顺序性。
它们在任务调度、缓冲和资源管理中至关重要，但如果没有可视化，它们的行为可能显得不透明。

模拟操作揭示了入队和出队操作如何随时间塑造系统。

目标：理解 FIFO（先进先出）顺序以及它如何确保处理的公平性。

#### 工作原理（通俗解释）

1.  从一个空队列开始。
2.  Enqueue(x)：将元素 `x` 添加到队尾。
3.  Dequeue()：移除队首元素。
4.  Peek()（可选）：查看下一个待处理的元素。

就像售票窗口前的队伍，先到的人先离开。

#### 逐步示例

操作序列：

```
Enqueue(10)
Enqueue(20)
Enqueue(30)
Dequeue()
Enqueue(40)
```

队列演变过程：

| 步骤 | 操作         | 队列状态（队首 → 队尾） |
| ---- | ------------ | ----------------------- |
| 1    | Enqueue(10)  | 10                      |
| 2    | Enqueue(20)  | 10, 20                  |
| 3    | Enqueue(30)  | 10, 20, 30              |
| 4    | Dequeue()    | 20, 30                  |
| 5    | Enqueue(40)  | 20, 30, 40              |

#### 微型代码（Python）

```python
from collections import deque

class Queue:
    def __init__(self):
        self.data = deque()

    def enqueue(self, x):
        self.data.append(x)
        print(f"入队 {x}: {list(self.data)}")

    def dequeue(self):
        if self.data:
            x = self.data.popleft()
            print(f"出队 {x}: {list(self.data)}")
            return x

# 演示
q = Queue()
q.enqueue(10)
q.enqueue(20)
q.enqueue(30)
q.dequeue()
q.enqueue(40)
```

每一步都会打印队列的当前状态，帮助你追踪顺序的演变。

#### 为什么重要

-   模拟现实世界的排队等待
-   用于调度器、网络缓冲区和广度优先搜索遍历
-   确保对有限资源的公平访问
-   建立对流处理的直觉

#### 一个温和的证明（为什么有效）

队列保持到达顺序。
如果元素按顺序 $a_1, a_2, \ldots, a_n$ 到达，
它们将以相同的顺序 $a_1, a_2, \ldots, a_n$ 离开。

每次入队都在队尾追加，每次出队都从队首移除。
因此，插入和移除的序列相匹配，从而强制执行 FIFO。

#### 亲自尝试

1.  模拟一个打印队列，作业按顺序进入和完成。
2.  使用队列在小图上实现广度优先搜索。
3.  模拟售票队伍的人员到达和离开。
4.  跟踪数据包通过网络缓冲区的流动。

#### 测试用例

| 操作序列                                       | 最终队列（队首 → 队尾） |
| ---------------------------------------------- | ----------------------- |
| Enqueue(1), Enqueue(2), Dequeue()              | 2                       |
| Enqueue('A'), Enqueue('B'), Enqueue('C')       | A, B, C                 |
| Enqueue(5), Dequeue(), Dequeue()               | (空)                    |
| Enqueue(7), Enqueue(9), Enqueue(11), Dequeue() | 9, 11                   |

#### 复杂度

| 操作        | 时间复杂度 | 空间复杂度 | 备注               |
| ----------- | ---------- | ---------- | ------------------ |
| Enqueue(x)  | O(1)       | O(n)       | 追加到队尾         |
| Dequeue()   | O(1)       | O(n)       | 从队首移除         |
| Peek()      | O(1)       | O(n)       | 访问队首元素       |

队列模拟阐明了公平性的节奏，每个到达者都耐心等待轮到自己，没有人插队。
### 73 链表构建器

链表构建器展示了元素如何通过指针连接，这是动态内存结构的基础，其中数据可以根据需求增长或收缩。

#### 我们要解决什么问题？

数组具有固定大小并且需要连续的内存。
链表通过动态链接分散的节点来解决这个问题，一次一个指针。

通过模拟节点的创建和链接，我们建立对指针操作和遍历的直觉，这对于掌握列表、栈、队列和图至关重要。

目标：理解节点如何链接在一起，以及在插入或删除过程中如何维护引用。

#### 工作原理（通俗解释）

单链表是一个节点序列，每个节点包含：

- 一个值
- 一个指向下一个节点的指针

基本操作：

1. 创建节点(值) → 分配新节点。
2. 在之后插入 → 在现有节点之间链接新节点。
3. 删除 → 重定向指针以跳过某个节点。
4. 遍历 → 跟随下一个指针直到 `None`。

就像一条链子，每个链环只知道下一个链环。

#### 逐步示例

构建一个链表：

```
插入(10)
插入(20)
插入(30)
```

过程：

1. 创建节点(10)：头节点 → 10 → None
2. 创建节点(20)：头节点 → 10 → 20 → None
3. 创建节点(30)：头节点 → 10 → 20 → 30 → None

从 `head` 开始遍历打印：
`10 → 20 → 30 → None`

#### 微型代码（Python）

```python
class Node:
    def __init__(self, value):
        self.value = value
        self.next = None

class LinkedList:
    def __init__(self):
        self.head = None

    def insert(self, value):
        new_node = Node(value)
        if not self.head:
            self.head = new_node
        else:
            cur = self.head
            while cur.next:
                cur = cur.next
            cur.next = new_node
        self.display()

    def display(self):
        cur = self.head
        elems = []
        while cur:
            elems.append(str(cur.value))
            cur = cur.next
        print(" → ".join(elems) + " → None")

# 演示
ll = LinkedList()
ll.insert(10)
ll.insert(20)
ll.insert(30)
```

#### 为什么它重要

- 实现动态内存分配
- 不需要连续存储
- 为栈、队列、哈希链、邻接表提供动力
- 为高级基于指针的结构奠定基础

#### 一个温和的证明（为什么它有效）

设 $n$ 为节点数。
每个节点恰好有一个出向指针（指向 `next`）或为 `None`。
遍历一次恰好访问每个节点一次。

因此，插入或遍历需要 $O(n)$ 时间，存储空间为 $O(n)$（每个元素一个节点）。

#### 自己动手试试

1. 插入值 `{5, 15, 25, 35}`
2. 删除第二个节点并重新连接链接
3. 通过重新分配指针手动反转链表
4. 可视化反转过程中每个 `next` 如何变化

#### 测试用例

| 操作序列                          | 预期输出           |
| --------------------------------- | ------------------ |
| 插入(10), 插入(20)                | 10 → 20 → None     |
| 插入(5), 插入(15), 插入(25)       | 5 → 15 → 25 → None |
| 空链表                            | None               |
| 单节点                            | 42 → None          |

#### 复杂度

| 操作         | 时间   | 空间   | 备注                 |
| ------------ | ------ | ------ | -------------------- |
| 尾部插入     | O(n)   | O(n)   | 遍历到尾部           |
| 删除节点     | O(n)   | O(n)   | 查找前驱节点         |
| 搜索         | O(n)   | O(n)   | 顺序遍历             |
| 遍历         | O(n)   | O(n)   | 访问每个节点一次     |

链表构建器是你与指针的第一次共舞，结构从简单的连接中浮现，内存变得流动、灵活而自由。
### 74 数组索引可视化工具

数组索引可视化工具能帮助你理解数组如何在连续内存中组织数据，以及索引如何实现 $O(1)$ 的任意元素访问。

#### 我们要解决什么问题？

数组是最简单的数据结构，但初学者常常难以真正理解索引在底层是如何工作的。
通过可视化索引位置和内存偏移量，你可以明白为什么数组允许直接访问，却需要固定大小和连续空间。

目标：理解索引、地址和元素访问之间的关系。

#### 工作原理（通俗解释）

数组将 $n$ 个元素连续存储在内存中。
如果基地址是 $A_0$，每个元素占用 $s$ 字节，那么：

$$ A_i = A_0 + i \times s $$

因此，访问索引 $i$ 是常数时间操作：

- 计算地址
- 直接跳转到该地址
- 获取值

这种可视化将逻辑索引 (0, 1, 2, …) 与物理位置联系起来。

#### 逐步示例

假设我们有一个整数数组：

```
arr = [10, 20, 30, 40]
```

基地址：`1000`，元素大小：`4 字节`

| 索引 | 地址  | 值   |
| ---- | ----- | ---- |
| 0    | 1000  | 10   |
| 1    | 1004  | 20   |
| 2    | 1008  | 30   |
| 3    | 1012  | 40   |

访问 `arr[2]`：

- 计算 $A_0 + 2 \times 4 = 1008$
- 获取 `30`

#### 微型代码（Python）

```python
def visualize_array(arr, base=1000, size=4):
    print(f"{'索引':<8}{'地址':<10}{'值':<8}")
    for i, val in enumerate(arr):
        address = base + i * size
        print(f"{i:<8}{address:<10}{val:<8}")

arr = [10, 20, 30, 40]
visualize_array(arr)
```

输出：

```
索引    地址      值
0       1000      10
1       1004      20
2       1008      30
3       1012      40
```

#### 为什么这很重要

- 通过地址计算实现即时访问
- 连续性确保了缓存局部性
- 固定大小和类型一致性
- 是高级结构（字符串、矩阵、张量）的核心

#### 一个温和的证明（为什么它有效）

设 $A_0$ 为基地址。
每个元素占用 $s$ 字节。
要访问元素 $i$：

$$ A_i = A_0 + i \times s $$

这是一个简单的算术运算，因此访问是 $O(1)$ 的，与 $n$ 无关。

#### 亲自尝试

1.  用基地址 `5000` 和元素大小 `8` 可视化数组 `[5, 10, 15, 20, 25]`。
2.  使用公式手动访问 `arr[4]`。
3.  比较数组与链表的访问时间。
4.  修改元素大小并重新运行可视化。

#### 测试用例

| 数组           | 基地址 | 大小 | 访问     | 预期地址 | 值   |
| -------------- | ------ | ---- | -------- | -------- | ---- |
| [10, 20, 30]    | 1000   | 4    | arr[1]   | 1004     | 20   |
| [7, 14, 21, 28] | 500    | 2    | arr[3]   | 506      | 28   |

#### 复杂度

| 操作         | 时间复杂度 | 空间复杂度 | 备注                 |
| ------------ | ---------- | ---------- | -------------------- |
| 访问         | O(1)       | O(n)       | 通过公式直接访问     |
| 更新         | O(1)       | O(n)       | 单次写入             |
| 遍历         | O(n)       | O(n)       | 访问所有元素         |
| 插入/删除    | O(n)       | O(n)       | 需要移动后续元素     |

数组索引可视化工具揭示了逻辑如何与硬件结合：每个索引都是一个直接指针，每个元素都是距离基地址的一个可预测的步长。
### 75 哈希函数映射器

哈希函数映射器展示了如何将键转换为数组索引，从而将任意数据转换为快速访问的位置。

#### 我们要解决什么问题？

我们经常需要通过键（如 "Alice" 或 "user123"）来存储和检索数据，而不是通过数字索引。
但数组只理解数字。
哈希函数弥合了这一差距，它将键映射为整数索引，这样我们就可以利用类似数组的速度进行基于键的查找。

目标：理解键如何变成索引以及哈希冲突如何发生。

#### 工作原理（通俗解释）

哈希函数接收一个键并计算出一个索引：

$$ \text{索引} = h(\text{键}) \bmod m $$

其中：

- $h(\text{键})$ 是一个数值哈希值，
- $m$ 是表的大小。

例如：

```
键 = "cat"
h(键) = 493728
m = 10
索引 = 493728 % 10 = 8
```

现在 `"cat"` 被映射到了槽位 8。

如果另一个键映射到了相同的索引，就会发生冲突，可以通过链地址法或开放寻址法来处理。

#### 逐步示例

假设一个大小为 5 的表。

键：`"red"`、`"blue"`、`"green"`

| 键     | 哈希值 | 索引 (`% 5`) |
| ------ | ------ | ------------ |
| red    | 432    | 2            |
| blue   | 107    | 2 (冲突)     |
| green  | 205    | 0            |

我们看到 `"red"` 和 `"blue"` 在索引 2 处发生了冲突。

#### 微型代码（Python）

```python
def simple_hash(key):
    # 简单哈希：计算字符串中所有字符的 ASCII 码之和
    return sum(ord(c) for c in key)

def map_keys(keys, size=5):
    # 初始化哈希表，每个槽位是一个列表（用于链地址法）
    table = [[] for _ in range(size)]
    for k in keys:
        idx = simple_hash(k) % size
        table[idx].append(k)
        print(f"Key: {k:6} -> Index: {idx}")
    return table

keys = ["red", "blue", "green"]
table = map_keys(keys)
```

输出：

```
Key: red    -> Index: 2
Key: blue   -> Index: 2
Key: green  -> Index: 0
```

#### 为什么这很重要

- 实现了平均常数时间的查找和插入
- 构成了哈希表、字典、缓存的基础
- 展示了哈希质量与冲突处理之间的权衡

#### 一个温和的证明（为什么它有效）

如果一个哈希函数能均匀地分布键，
那么每个槽位预期的键数量是 $\frac{n}{m}$。

因此，预期的查找时间为：

$$ E[T] = O(1 + \frac{n}{m}) $$

对于精心选择的 $m$ 和良好的 $h$，$E[T] \approx O(1)$。

#### 自己动手试试

1.  将 `["cat", "dog", "bat", "rat"]` 映射到一个大小为 7 的表中。
2.  观察冲突并尝试使用更大的表。
3.  将 `sum(ord(c))` 替换为多项式哈希：
    $$ h(\text{键}) = \sum c_i \times 31^i $$
4.  比较分布质量。

#### 测试用例

| 键               | 表大小 | 结果（索引）          |
| ---------------- | ------ | --------------------- |
| ["a", "b", "c"]  | 3      | 1, 2, 0               |
| ["hi", "ih"]     | 5      | 冲突（和相同）        |

#### 复杂度

| 操作   | 时间（期望） | 空间   | 备注                     |
| ------ | ------------ | ------ | ------------------------ |
| 插入   | O(1)         | O(n)   | 平均值，良好哈希         |
| 查找   | O(1)         | O(n)   | 在均匀哈希条件下         |
| 删除   | O(1)         | O(n)   | 与查找成本相同           |

哈希函数映射器使哈希变得具体可感，你可以看到字符串变成槽位，冲突出现，而顺序则消融在概率和数学之中。
### 76 二叉树构建器

二叉树构建器展示了如何通过用左孩子和右孩子链接节点来构建层次化数据结构。

#### 我们正在解决什么问题？

像数组和列表这样的线性结构无法有效地表示层次关系。
当你需要排序、搜索和层次分组时，二叉树提供了基础。

目标：理解节点如何连接形成树，以及递归结构如何自然地出现。

#### 工作原理（通俗解释）

二叉树由节点组成。
每个节点包含：

- 一个值
- 一个左孩子
- 一个右孩子

构建一棵树：

1. 从一个根节点开始
2. 递归地插入新节点：

   * 如果值 < 当前节点值 → 向左走
   * 否则 → 向右走
3. 重复直到找到一个空链接

这样就产生了一棵二叉搜索树（BST），保持了顺序特性。

#### 逐步示例

插入值：`[10, 5, 15, 3, 7, 12, 18]`

过程：

```
10
├── 5
│   ├── 3
│   └── 7
└── 15
    ├── 12
    └── 18
```

遍历顺序：

- 中序：3, 5, 7, 10, 12, 15, 18
- 前序：10, 5, 3, 7, 15, 12, 18
- 后序：3, 7, 5, 12, 18, 15, 10

#### 微型代码（Python）

```python
class Node:
    def __init__(self, value):
        self.value = value
        self.left = None
        self.right = None

class BST:
    def __init__(self):
        self.root = None

    def insert(self, value):
        self.root = self._insert(self.root, value)

    def _insert(self, node, value):
        if node is None:
            return Node(value)
        if value < node.value:
            node.left = self._insert(node.left, value)
        else:
            node.right = self._insert(node.right, value)
        return node

    def inorder(self, node):
        if node:
            self.inorder(node.left)
            print(node.value, end=" ")
            self.inorder(node.right)

# 演示
tree = BST()
for val in [10, 5, 15, 3, 7, 12, 18]:
    tree.insert(val)
tree.inorder(tree.root)
```

输出：
`3 5 7 10 12 15 18`

#### 为什么它很重要

- 是搜索树、堆和表达式树的核心结构
- 构成了平衡树（AVL、红黑树）的基础
- 自然地支持分治递归

#### 一个温和的证明（为什么它有效）

二叉搜索树保持以下不变式：

$$ \forall \text{节点},\ v:
\begin{cases}
v_{\text{左}} < v_{\text{根}} < v_{\text{右}}
\end{cases} $$

插入操作通过递归放置来保持这个不变式。
每次插入都沿着高度为 $h$ 的单一路径进行，因此时间复杂度为 $O(h)$。
对于平衡树，$h = O(\log n)$。

#### 自己动手试试

1. 插入 `[8, 3, 10, 1, 6, 14, 4, 7, 13]`。
2. 画出树结构。
3. 执行中序遍历（应该输出排序后的顺序）。
4. 与不平衡的插入顺序进行比较。

#### 测试用例

| 输入序列          | 中序遍历         |
| ----------------- | ---------------- |
| [10, 5, 15, 3, 7] | 3, 5, 7, 10, 15  |
| [2, 1, 3]         | 1, 2, 3          |
| [5]               | 5                |

#### 复杂度

| 操作     | 时间（平均） | 时间（最坏） | 空间 |
| -------- | ------------ | ------------ | ---- |
| 插入     | O(log n)     | O(n)         | O(n) |
| 搜索     | O(log n)     | O(n)         | O(1) |
| 删除     | O(log n)     | O(n)         | O(1) |
| 遍历     | O(n)         | O(n)         | O(n) |

二叉树构建器揭示了层次结构中的顺序，每个节点都是一个决策，每个分支都是一个关于较小和较大的故事。
### 77 堆结构演示

堆结构演示帮助你可视化二叉堆如何组织数据，以始终保持最小或最大元素位于顶部，从而实现快速的优先级访问。

#### 我们正在解决什么问题？

我们经常需要一种能快速检索最小或最大元素的结构，例如在优先级队列或调度中。
每次都进行排序是低效的。
堆维护了部分有序性，使得根节点始终是极值，并且调整操作是局部进行的。

目标：理解插入和删除操作如何维持堆的性质。

#### 工作原理（通俗解释）

二叉堆是一个以数组形式存储的完全二叉树。
每个节点满足：

- 最小堆：父节点 ≤ 子节点
- 最大堆：父节点 ≥ 子节点

插入和删除操作通过*上浮*和*下沉*操作来处理。

#### 插入（向上堆化）

1.  在末尾添加新元素
2.  与父节点比较
3.  如果违反堆性质则交换
4.  重复直到堆性质成立

#### 删除根节点（向下堆化）

1.  用最后一个元素替换根节点
2.  与子节点比较
3.  与较小（最小堆）或较大（最大堆）的子节点交换
4.  重复直到性质恢复

#### 分步示例（最小堆）

插入 `[10, 4, 15, 2]`

1.  `[10]`
2.  `[10, 4]` → 交换(4, 10) → `[4, 10]`
3.  `[4, 10, 15]` （无需交换）
4.  `[4, 10, 15, 2]` → 交换(2, 10) → 交换(2, 4) → `[2, 4, 15, 10]`

最终堆（数组）：`[2, 4, 15, 10]`
树形视图：

```
    2
   / \
  4  15
 /
10
```

#### 微型代码（Python）

```python
import heapq

def heap_demo():
    heap = []
    for x in [10, 4, 15, 2]:
        heapq.heappush(heap, x)
        print("插入", x, "→", heap)
    while heap:
        print("弹出:", heapq.heappop(heap), "→", heap)

heap_demo()
```

输出：

```
插入 10 → [10]
插入 4 → [4, 10]
插入 15 → [4, 10, 15]
插入 2 → [2, 4, 15, 10]
弹出: 2 → [4, 10, 15]
弹出: 4 → [10, 15]
弹出: 10 → [15]
弹出: 15 → []
```

#### 为什么它很重要

-   支持优先级队列（任务调度器、Dijkstra算法）
-   支持 O(1) 访问最小/最大值
-   保持 O(log n) 的插入/删除成本
-   是堆排序的基础

#### 一个温和的证明（为什么它有效）

令 $h = \lfloor \log_2 n \rfloor$ 为堆的高度。
每次插入和删除操作沿着高度为 $h$ 的一条路径移动。
因此：

$$ T_{\text{插入}} = T_{\text{删除}} = O(\log n) $$
$$ T_{\text{查找最小值}} = O(1) $$

#### 亲自尝试

1.  将 `[7, 2, 9, 1, 5]` 插入到一个最小堆中
2.  在纸上追踪交换过程
3.  重复删除最小值并记录顺序（结果应为升序排列）
4.  为最大堆版本重复上述步骤

#### 测试用例

| 操作     | 输入         | 输出（堆）        |
| -------- | ------------ | ----------------- |
| 插入     | [5, 3, 8]    | [3, 5, 8]         |
| 弹出     | [3, 5, 8]    | 弹出 3 → [5, 8]   |
| 插入     | [10, 2, 4]   | [2, 10, 4]        |

#### 复杂度

| 操作         | 时间复杂度 | 空间复杂度 | 备注               |
| ------------ | ---------- | ---------- | ------------------ |
| 插入         | O(log n)   | O(n)       | 向上渗透           |
| 删除         | O(log n)   | O(n)       | 向下渗透           |
| 查找最小/最大 | O(1)       | O(1)       | 访问根节点         |
| 建堆         | O(n)       | O(n)       | 自底向上堆化       |

堆结构演示通过形状展示秩序，每个父节点都在其子节点之上，每次插入都是向平衡的一次攀登。
### 78 并查集概念

并查集概念（也称为 Disjoint Set Union，DSU）展示了如何高效地管理动态分组，判断元素是否属于同一集合，并在需要时合并集合。

#### 我们要解决什么问题？

在许多问题中，我们需要跟踪连通分量，例如在图、社交网络或 Kruskal 最小生成树算法中。
我们希望高效地回答两种操作：

- Find(x)：x 在哪个组中？
- Union(x, y)：合并 x 和 y 所在的组

朴素的方法（如扫描数组）成本太高。
并查集结构通过使用父指针和路径压缩，以*近乎常数的时间*解决这个问题。

#### 工作原理（通俗解释）

每个元素指向一个父节点。
根节点是其所在集合的代表。
如果两个元素共享同一个根节点，则它们在同一组中。

操作：

1. Find(x)：
   沿着父指针直到到达根节点
   （即满足 `parent[x] == x` 的节点）
   使用路径压缩来为下次操作展平路径

2. Union(x, y)：
   找到 x 和 y 的根节点
   如果不同，则将其中一个根节点连接到另一个根节点（合并集合）
   可选地，使用按秩合并/按大小合并来保持树结构较浅

#### 逐步示例

初始状态：`{1}, {2}, {3}, {4}`

执行：

```
Union(1, 2) → {1,2}, {3}, {4}
Union(3, 4) → {1,2}, {3,4}
Union(2, 3) → {1,2,3,4}
```

现在所有元素在一个根节点下连通。

如果执行 `Find(4)` → 返回 `1`（其所在集合的根节点）

#### 微型代码（Python）

```python
class UnionFind:
    def __init__(self, n):
        self.parent = [i for i in range(n)]
        self.rank = [0] * n

    def find(self, x):
        if self.parent[x] != x:
            self.parent[x] = self.find(self.parent[x])  # 路径压缩
        return self.parent[x]

    def union(self, x, y):
        rx, ry = self.find(x), self.find(y)
        if rx == ry:
            return
        if self.rank[rx] < self.rank[ry]:
            self.parent[rx] = ry
        elif self.rank[rx] > self.rank[ry]:
            self.parent[ry] = rx
        else:
            self.parent[ry] = rx
            self.rank[rx] += 1

# 演示
uf = UnionFind(5)
uf.union(0, 1)
uf.union(2, 3)
uf.union(1, 2)
print([uf.find(i) for i in range(5)])
```

输出：
`[0, 0, 0, 0, 4]`

#### 为什么它很重要

- Kruskal 最小生成树算法的基础
- 检测无向图中的环
- 高效处理动态图中的连通性查询
- 用于渗透、图像分割、聚类

#### 一个温和的证明（为什么它有效）

每次操作的摊还成本由反阿克曼函数 $\alpha(n)$ 给出，实际上是常数。

$$ T_{\text{find}}(n), T_{\text{union}}(n) = O(\alpha(n)) $$

因为路径压缩确保每次节点都指向更靠近根节点的位置，将结构展平到近乎恒定的深度。

#### 自己动手试试

1. 从 `{0}, {1}, {2}, {3}, {4}` 开始
2. 应用：`Union(0,1), Union(2,3), Union(1,2)`
3. 查询 `Find(3)` → 应该与 `0` 的根节点匹配
4. 在每次操作后打印父数组

#### 测试用例

| 操作序列                | 结果集合          |
| ----------------------- | ----------------- |
| Union(1, 2), Union(3, 4) | {1,2}, {3,4}, {0} |
| Union(2, 3)             | {0}, {1,2,3,4}    |
| Find(4)                 | 根节点 = 1 (或 0) |

#### 复杂度

| 操作          | 摊还时间         | 空间    | 备注               |
| ------------- | ---------------- | ------- | ------------------ |
| Find          | $O(\alpha(n))$   | $O(n)$  | 路径压缩           |
| Union         | $O(\alpha(n))$   | $O(n)$  | 使用按秩启发式合并 |
| Connected(x, y) | $O(\alpha(n))$ | $O(1)$  | 通过根节点比较     |

并查集概念将不相交的集合转变为一个动态的网络，连接被建立和展平，通过结构发现统一性。
### 79 图表示法演示

图表示法演示揭示了如何用数据结构对图进行编码，展示了邻接表、邻接矩阵和边列表之间的权衡。

#### 我们要解决什么问题？

图描述关系，例如城市之间的道路、网站之间的链接、网络中的友谊关系。
但在我们运行算法（如 BFS、Dijkstra 或 DFS）之前，我们需要一种与图的密度、大小和操作相匹配的表示方法。

目标：理解不同的表示法如何编码边，以及如何选择正确的表示法。

#### 工作原理（通俗解释）

图定义为：
$$ G = (V, E) $$
其中：

- $V$ = 顶点集
- $E$ = 边集（顶点对）

我们可以用三种主要方式表示 $G$：

1. 邻接矩阵

   * 大小为 $|V| \times |V|$ 的二维数组
   * 如果边 $(i, j)$ 存在，则条目 $(i, j) = 1$，否则为 0

2. 邻接表

   * 对于每个顶点，存储其邻居列表
   * 对于稀疏图很紧凑

3. 边列表

   * 所有边的简单列表
   * 易于遍历，但难以快速查找

#### 分步示例

考虑一个无向图：

```
顶点：{A, B, C, D}
边：{(A, B), (A, C), (B, D)}
```

邻接矩阵

|   | A | B | C | D |
| - | - | - | - | - |
| A | 0 | 1 | 1 | 0 |
| B | 1 | 0 | 0 | 1 |
| C | 1 | 0 | 0 | 0 |
| D | 0 | 1 | 0 | 0 |

邻接表

```
A: [B, C]
B: [A, D]
C: [A]
D: [B]
```

边列表

```
[(A, B), (A, C), (B, D)]
```

#### 微型代码（Python）

```python
from collections import defaultdict

# 邻接表
graph = defaultdict(list)
edges = [("A", "B"), ("A", "C"), ("B", "D")]

for u, v in edges:
    graph[u].append(v)
    graph[v].append(u)  # 无向图

print("邻接表:")
for node, neighbors in graph.items():
    print(f"{node}: {neighbors}")

# 邻接矩阵
vertices = ["A", "B", "C", "D"]
n = len(vertices)
matrix = [[0]*n for _ in range(n)]
index = {v: i for i, v in enumerate(vertices)}

for u, v in edges:
    i, j = index[u], index[v]
    matrix[i][j] = matrix[j][i] = 1

print("\n邻接矩阵:")
for row in matrix:
    print(row)
```

输出：

```
邻接表:
A: ['B', 'C']
B: ['A', 'D']
C: ['A']
D: ['B']

邻接矩阵:
[0, 1, 1, 0]
[1, 0, 0, 1]
[1, 0, 0, 0]
[0, 1, 0, 0]
```

#### 为什么重要

- 邻接矩阵 → 查找快 ($O(1)$)，空间占用高 ($O(V^2)$)
- 邻接表 → 对稀疏图高效 ($O(V+E)$)
- 边列表 → 易于遍历，非常适合像 Kruskal 这样的算法

明智地选择会影响图上每个算法的性能。

#### 一个温和的证明（为什么有效）

令 $V$ 为顶点数，$E$ 为边数。

| 表示法         | 存储空间   | 边检查       | 遍历        |
| -------------- | ---------- | ------------ | ----------- |
| 邻接矩阵       | $O(V^2)$   | $O(1)$       | $O(V^2)$    |
| 邻接表         | $O(V + E)$ | $O(\deg(v))$ | $O(V + E)$  |
| 边列表         | $O(E)$     | $O(E)$       | $O(E)$      |

稀疏图 ($E \ll V^2$) → 首选邻接表。
稠密图 ($E \approx V^2$) → 邻接矩阵可以接受。

#### 自己动手试试

1.  画一个有 5 个节点、6 条边的图
2.  写出所有三种表示法
3.  计算存储成本
4.  为 BFS 与 Kruskal 最小生成树算法选择最佳格式

#### 测试用例

| 图类型 | 表示法     | 优点             |
| ------ | ---------- | ---------------- |
| 稀疏   | 邻接表     | 空间效率高       |
| 稠密   | 邻接矩阵   | 常数时间查找     |
| 带权   | 边列表     | 易于排序         |

#### 复杂度

| 操作     | 邻接矩阵 | 邻接表       | 边列表 |
| -------- | -------- | ------------ | ------ |
| 空间     | $O(V^2)$ | $O(V+E)$     | $O(E)$ |
| 添加边   | $O(1)$   | $O(1)$       | $O(1)$ |
| 检查边   | $O(1)$   | $O(\deg(v))$ | $O(E)$ |
| 遍历     | $O(V^2)$ | $O(V+E)$     | $O(E)$ |

图表示法演示展示了连接的蓝图，同一个网络，三种不同的视角：矩阵、列表或边表。
### 80 字典树结构可视化工具

字典树结构可视化工具帮助您直观地看到字符串和前缀如何被高效存储，每个字符对应一条边，并为公共前缀构建共享路径。

#### 我们要解决什么问题？

当您需要存储和搜索大量字符串，尤其是按前缀搜索时，线性扫描或哈希表并不理想。
我们想要一种数据结构，能通过共享结构实现快速的前缀查询和高效的内存使用。

字典树（前缀树）正是为此而生，它将字符串存储为路径，并重用公共前缀。

目标：理解每个字符如何扩展路径，以及搜索和插入操作如何沿着边进行。

#### 工作原理（通俗解释）

字典树从一个空的根节点开始。
每条边代表一个字符。
每个节点可能有多个子节点，每个子节点对应一个可能的下一个字符。

插入单词的步骤：

1. 从根节点开始
2. 对于每个字符：

   * 如果对应的边不存在，则创建一个新的子节点
   * 移动到该子节点
3. 将最后一个节点标记为"单词结束"

搜索单词的步骤：

1. 从根节点开始
2. 按照每个字符沿着边移动
3. 如果路径存在且终点被标记，则找到单词

#### 逐步示例

插入 `cat`、`car`、`dog`

```
(根节点)
 ├── c
 │    └── a
 │         ├── t*
 │         └── r*
 └── d
      └── o
           └── g*
```

星号 `*` 标记单词结束。
公共前缀 `ca` 被共享。

搜索 `"car"`：

- `c` ✓
- `a` ✓
- `r` ✓
- 终点被标记 → 找到

搜索 `"cap"`：

- `c` ✓
- `a` ✓
- `p` ✗ → 未找到

#### 微型代码（Python）

```python
class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def insert(self, word):
        node = self.root
        for ch in word:
            if ch not in node.children:
                node.children[ch] = TrieNode()
            node = node.children[ch]
        node.is_end = True

    def search(self, word):
        node = self.root
        for ch in word:
            if ch not in node.children:
                return False
            node = node.children[ch]
        return node.is_end

# 演示
trie = Trie()
for w in ["cat", "car", "dog"]:
    trie.insert(w)
print(trie.search("car"))  # True
print(trie.search("cap"))  # False
```

#### 为什么它很重要

- 支持前缀搜索、自动补全、字典查找
- 避免重复计算前缀
- 对于字符串密集型应用非常高效
- 是压缩字典树、DAWG 和后缀树的基础

#### 一个温和的证明（为什么它有效）

单词 $w$ 中的每个字符在字典树中沿着一条路径前进。
插入成本 = $O(|w|)$，
搜索成本 = $O(|w|)$。

对于平均长度为 $L$ 的 $n$ 个单词，总节点数 ≤ $O(nL)$。

前缀查询成本 = $O(p)$，其中 $p$ = 前缀长度。

#### 亲自尝试

1. 插入 `["cat", "cap", "can", "dog"]`
2. 画出树路径
3. 查询前缀 `"ca"` 和 `"do"`
4. 计算创建的总节点数

#### 测试用例

| 操作       | 输入          | 输出               |
| ---------- | ------------- | ------------------ |
| 插入       | "cat", "car"  | 共享路径 "ca"      |
| 搜索       | "car"         | True               |
| 搜索       | "cap"         | False              |
| 前缀查询   | "ca"          | 存在               |

#### 复杂度

| 操作         | 时间复杂度 | 空间复杂度 | 备注                 |
| ------------ | ---------- | ---------- | -------------------- |
| 插入         | O(L)       | O(L)       | L = 单词长度         |
| 搜索         | O(L)       | O(1)       | 沿着路径             |
| 前缀查询     | O(p)       | O(1)       | 共享遍历             |

字典树结构可视化工具展示了源于语言的结构，每个单词是一条路径，每个前缀是一个交汇点，每个分支是一段共享的记忆。

## 第 9 节 图与树概述
### 81 图模型构造器

图模型构造器是我们正式构建图（由边连接的顶点集合）的方式，用以表示现实世界中的关系、网络或结构。

#### 我们要解决什么问题？

我们经常面临元素相互连接的问题：城市间的道路、网络中的友谊关系、项目中的依赖关系。
为了对这些进行推理，我们需要一种方法来建模实体（顶点）和连接（边）。

图模型构造器提供了将现实世界关系转化为我们可以分析的图数据结构的蓝图。

#### 工作原理（通俗解释）

图定义为：

$$
G = (V, E)
$$

其中

- $V$ = 顶点（节点）的集合
- $E$ = 顶点之间边（连接）的集合

每条边可以是：

- 无向边：$(u, v)$ 表示 $u$ 和 $v$ 双向连接
- 有向边：$(u, v)$ 表示从 $u$ 到 $v$ 的单向连接

你可以用多种方式构建图：

1. 边列表 – 成对的列表 $(u, v)$
2. 邻接表 – 节点 → 邻居列表的字典
3. 邻接矩阵 – 连接的二维表（1 表示有边，0 表示无边）

#### 示例

输入关系

```
A 连接到 B
A 连接到 C
B 连接到 C
C 连接到 D
```

顶点

```
V = {A, B, C, D}
```

边

```
E = {(A, B), (A, C), (B, C), (C, D)}
```

边列表

```
[(A, B), (A, C), (B, C), (C, D)]
```

邻接表

```
A: [B, C]
B: [A, C]
C: [A, B, D]
D: [C]
```

邻接矩阵

|   | A | B | C | D |
| - | - | - | - | - |
| A | 0 | 1 | 1 | 0 |
| B | 1 | 0 | 1 | 0 |
| C | 1 | 1 | 0 | 1 |
| D | 0 | 0 | 1 | 0 |

#### 微型代码（Python）

```python
def build_graph(edge_list):
    graph = {}
    for u, v in edge_list:
        graph.setdefault(u, []).append(v)
        graph.setdefault(v, []).append(u)  # 无向图
    return graph

edges = [("A","B"),("A","C"),("B","C"),("C","D")]
graph = build_graph(edges)
for node, neighbors in graph.items():
    print(node, ":", neighbors)
```

输出

```
A : ['B', 'C']
B : ['A', 'C']
C : ['A', 'B', 'D']
D : ['C']
```

#### 重要性

- 图让我们能在任何领域建模关系：道路、社交网络、依赖关系、知识。
- 一旦构建完成，你就可以应用图算法，如 BFS、DFS、最短路径、生成树、连通性，来解决实际问题。
- 构造阶段决定了后续算法运行的效率。

#### 一个温和的证明（为什么它有效）

给定 $n$ 个顶点和 $m$ 条边，我们通过连接 $u$ 和 $v$ 来表示每条边 $(u,v)$。
构造时间 = $O(n + m)$，因为每个顶点和每条边都被处理一次。

邻接表大小 = $O(n + m)$
邻接矩阵大小 = $O(n^2)$

因此，对于稀疏图，邻接表在空间上更高效；而对于稠密图，矩阵提供了常数时间的边查找。

#### 自己动手试试

1.  构建一个包含 5 个城市及其直飞航班的图。
2.  分别用边列表和邻接表表示它。
3.  计算边的数量和每个顶点的邻居数量。
4.  在纸上画出最终的图。

#### 测试用例

| 输入                     | 表示方式         | 关键属性               |
| ------------------------ | ---------------- | ---------------------- |
| `[(1,2), (2,3)]`        | 邻接表           | 3 个顶点，2 条边       |
| 有向边                   | 邻接表           | 仅单向链接             |
| 完全连接的 3 个节点      | 邻接矩阵         | 除对角线外全为 1       |

#### 复杂度

| 表示方式       | 空间      | 查找      | 遍历      |
| -------------- | --------- | --------- | --------- |
| 边列表         | O(m)      | O(m)      | O(m)      |
| 邻接表         | O(n + m)  | O(deg(v)) | O(m)      |
| 邻接矩阵       | O(n²)     | O(1)      | O(n²)     |

图模型构造器构建了连接的世界，从抽象关系到具体的数据结构，构成了后续所有图算法的支柱。
### 82 邻接矩阵构建器

邻接矩阵构建器以二维网格的形式表示图，展示顶点对之间是否相连。这是一种以紧凑的数学形式捕获所有边的简单而强大的方法。

#### 我们要解决什么问题？

我们需要一种快速、系统的方法来测试两个顶点是否相连。
虽然邻接表节省空间，但邻接矩阵使得边的查询时间复杂度为 $O(1)$，这在连接密集或需要频繁检查时非常理想。

邻接矩阵构建器为我们提供了一个类似表格的结构，可以清晰地存储边的信息。

#### 工作原理（通俗解释）

对于一个有 $n$ 个顶点的图，邻接矩阵是一个 $n \times n$ 的表格：

$$
A[i][j] =
\begin{cases}
1, & \text{如果存在从 } i \text{ 到 } j \text{ 的边},\\
0, & \text{否则}.
\end{cases}
$$

- 对于无向图，矩阵是对称的：$A[i][j] = A[j][i]$
- 对于有向图，对称性可能不成立
- 对于带权图，存储权重值而不是 1

#### 示例

顶点：$V = {A, B, C, D}$
边：${(A,B), (A,C), (B,C), (C,D)}$

邻接矩阵（无向）

|   | A | B | C | D |
| - | - | - | - | - |
| A | 0 | 1 | 1 | 0 |
| B | 1 | 0 | 1 | 0 |
| C | 1 | 1 | 0 | 1 |
| D | 0 | 0 | 1 | 0 |

要检查 A 和 C 是否相连，测试 $A[A][C] = 1$

#### 微型代码（Python）

```python
def adjacency_matrix(vertices, edges, directed=False):
    n = len(vertices)
    index = {v: i for i, v in enumerate(vertices)}
    A = [[0] * n for _ in range(n)]

    for u, v in edges:
        i, j = index[u], index[v]
        A[i][j] = 1
        if not directed:
            A[j][i] = 1
    return A

vertices = ["A", "B", "C", "D"]
edges = [("A", "B"), ("A", "C"), ("B", "C"), ("C", "D")]
A = adjacency_matrix(vertices, edges)
for row in A:
    print(row)
```

输出

```
[0, 1, 1, 0]
[1, 0, 1, 0]
[1, 1, 0, 1]
[0, 0, 1, 0]
```

#### 为何重要

- 边存在性的常数时间检查
- 为图算法和证明提供简单的数学表示
- 基于矩阵的图算法的基础，例如：
  * Floyd–Warshall（全源最短路径）
  * 邻接矩阵的幂（可达性）
  * 谱图理论（拉普拉斯矩阵，特征值）

#### 一个温和的证明（为何有效）

每个顶点对 $(u, v)$ 对应一个矩阵单元 $A[i][j]$。
我们访问每条边一次，以设置两个对称的条目（无向图）或一个条目（有向图）。
因此：

- 时间复杂度：初始化 $O(n^2)$，填充 $O(m)$
- 空间复杂度：$O(n^2)$

当 $m \approx n^2$（稠密图）时，这种权衡是值得的。

#### 亲自尝试

1.  为有向三角形（A→B, B→C, C→A）构建邻接矩阵
2.  修改它，在 B 上添加一个自环
3.  检查是否 $A[B][B] = 1$
4.  比较有向图与无向图的对称性

#### 测试用例

| 图类型     | 边         | 对称性       | 值           |
| ---------- | ---------- | ------------ | ------------ |
| 无向图     | (A,B)      | 对称         | A[B][A] = 1  |
| 有向图     | (A,B)      | 不对称       | A[B][A] = 0  |
| 带权图     | (A,B,w=5)  | 存储值       | A[A][B] = 5  |

#### 复杂度

| 操作             | 时间       | 空间       |
| ---------------- | ---------- | ---------- |
| 构建矩阵         | $O(n^2)$   | $O(n^2)$   |
| 边检查           | $O(1)$     | -          |
| 遍历邻居         | $O(n)$     | -          |

邻接矩阵构建器将图转化为表格，这是一种用于分析、高效查询和算法转换的通用结构。
### 83 邻接表构建器

邻接表构建器以一种灵活的方式表示图，将每个顶点的邻居存储在一个列表中。对于稀疏图而言，它具有内存效率，并且对于基于遍历的算法来说非常直观。

#### 我们要解决什么问题？

我们需要一种紧凑地表示图的方法，同时仍然支持对相连顶点的快速遍历。
当图是稀疏的（边数相对于 $n^2$ 较少）时，邻接矩阵会浪费空间。
邻接表只关注实际存在的边，使其既精简又直观。

#### 它是如何工作的？（通俗解释）

每个顶点都维护一个列表，记录它连接到的所有顶点。
在有向图中，边是单向的；在无向图中，每条边会出现两次。

对于一个具有顶点 $V$ 和边 $E$ 的图，邻接表定义为：

$$
\text{Adj}[u] = {v \mid (u, v) \in E}
$$

你可以把它想象成一个字典（或映射），其中每个键是一个顶点，其值是一个邻居列表。

#### 示例

顶点：$V = {A, B, C, D}$
边：${(A,B), (A,C), (B,C), (C,D)}$

邻接表（无向图）

```
A: [B, C]
B: [A, C]
C: [A, B, D]
D: [C]
```

#### 微型代码（Python）

```python
def adjacency_list(vertices, edges, directed=False):
    adj = {v: [] for v in vertices}
    for u, v in edges:
        adj[u].append(v)
        if not directed:
            adj[v].append(u)
    return adj

vertices = ["A", "B", "C", "D"]
edges = [("A", "B"), ("A", "C"), ("B", "C"), ("C", "D")]

graph = adjacency_list(vertices, edges)
for node, nbrs in graph.items():
    print(f"{node}: {nbrs}")
```

输出

```
A: ['B', 'C']
B: ['A', 'C']
C: ['A', 'B', 'D']
D: ['C']
```

#### 为什么它很重要？

- 对于稀疏图空间效率高（$O(n + m)$）
- 天然适用于 DFS、BFS 和路径查找算法
- 易于修改和扩展（加权边、标签）
- 构成了图遍历算法和网络模型的基础

#### 一个温和的证明（为什么它有效）

每条边恰好存储一次（有向图）或两次（无向图）。
如果 $n$ 是顶点数，$m$ 是边数：

- 初始化：$O(n)$
- 插入：$O(m)$
- 总空间：$O(n + m)$

不会为不存在的边浪费空间，每个列表只随实际的邻居增长。

#### 亲自尝试

1.  为具有边 (A→B, A→C, C→A) 的有向图构建一个邻接表。
2.  添加一个没有边的新顶点 E；确认它仍然显示为 `E: []`。
3.  计算总共有多少个邻居，它应该与边的数量相匹配。

#### 测试用例

| 图类型   | 输入边       | 表示               |
| -------- | ------------ | ------------------ |
| 无向图   | (A,B)        | A: [B], B: [A]     |
| 有向图   | (A,B)        | A: [B], B: []      |
| 加权图   | (A,B,5)      | A: [(B,5)]         |

#### 复杂度

| 操作           | 时间         | 空间        |
| -------------- | ------------ | ----------- |
| 构建列表       | $O(n + m)$   | $O(n + m)$  |
| 检查邻居       | $O(\deg(v))$ | -           |
| 添加边         | $O(1)$       | -           |
| 删除边         | $O(\deg(v))$ | -           |

邻接表构建器使你的图表示保持清晰且可扩展，非常适合那些在大规模网络中行走、探索和连接点的算法。
### 84 度计数器

度计数器计算图中每个顶点有多少条边与之相连。
对于无向图，度就是邻居的数量。
对于有向图，我们区分入度和出度。

#### 我们要解决什么问题？

我们想知道每个顶点的连接程度如何。
度计数有助于回答结构性问题：

- 图是否正则（所有顶点度数相同）？
- 是否存在源点（入度为零）或汇点（出度为零）？
- 网络中哪个节点是枢纽？

这些洞察是遍历、中心性和优化的基础。

#### 工作原理（通俗解释）

对于每条边 $(u, v)$：

- 无向图：递增 `degree[u]` 和 `degree[v]`
- 有向图：递增 `out_degree[u]` 和 `in_degree[v]`

完成后，每个顶点都有其连接计数。

#### 示例

无向图：
$$
V = {A, B, C, D}, \quad E = {(A,B), (A,C), (B,C), (C,D)}
$$

| 顶点 | 度 |
| ------ | ------ |
| A      | 2      |
| B      | 2      |
| C      | 3      |
| D      | 1      |

有向图版本：

- 入度(A)=1（来自 C），出度(A)=2（指向 B,C）

#### 微型代码（Python）

```python
def degree_counter(vertices, edges, directed=False):
    if directed:
        indeg = {v: 0 for v in vertices}
        outdeg = {v: 0 for v in vertices}
        for u, v in edges:
            outdeg[u] += 1
            indeg[v] += 1
        return indeg, outdeg
    else:
        deg = {v: 0 for v in vertices}
        for u, v in edges:
            deg[u] += 1
            deg[v] += 1
        return deg

vertices = ["A", "B", "C", "D"]
edges = [("A","B"), ("A","C"), ("B","C"), ("C","D")]
print(degree_counter(vertices, edges))
```

输出

```
{'A': 2, 'B': 2, 'C': 3, 'D': 1}
```

#### 为什么它很重要

- 揭示连接模式
- 识别孤立节点
- 支持图分类（正则图、稀疏图、稠密图）
- 对图算法至关重要（拓扑排序、PageRank、BFS剪枝）

#### 一个温和的证明（为什么它有效）

在任何无向图中，所有顶点的度数之和等于边数的两倍：

$$
\sum_{v \in V} \deg(v) = 2|E|
$$

在有向图中：

$$
\sum_{v \in V} \text{in}(v) = \sum_{v \in V} \text{out}(v) = |E|
$$

这些等式保证了正确性，每条边恰好贡献一次（如果是无向图则贡献两次）。

#### 自己动手试试

1.  创建一个具有边 (A,B), (B,C), (C,A) 的无向图
    *   验证所有顶点的度都是 2
2.  添加一个孤立顶点 D
    *   检查其度是否为 0
3.  转换为有向边并分别计算入度和出度

#### 测试用例

| 图类型       | 输入边          | 输出                                   |
| ------------- | ---------------- | ------------------------------------ |
| 无向图        | (A,B), (A,C)     | A:2, B:1, C:1                        |
| 有向图        | (A,B), (B,C)     | in(A)=0, out(A)=1; in(C)=1, out(C)=0 |
| 孤立节点      | (A,B), V={A,B,C} | C:0                                  |

#### 复杂度

| 操作          | 时间   | 空间  |
| ------------- | ------ | ------ |
| 计数度        | $O(m)$ | $O(n)$ |
| 查找度        | $O(1)$ | -      |

度计数器揭示了图的心跳，展示了哪些节点繁忙，哪些节点孤独，以及网络结构是如何展开的。
### 85 路径存在性检测器

路径存在性检测器用于检查图中两个顶点之间是否存在一条路径，即是否可以通过沿着边从源点到达目标点。

#### 我们要解决什么问题？

在许多场景中，如导航、依赖关系解析、通信，核心问题都是：
"我们能从 A 到达 B 吗？"

这里关注的不是找到*最短*路径，而仅仅是检查路径*是否存在*。

示例：

-   文件是否可以从根目录访问？
-   数据能否在网络中的两个节点之间流动？
-   依赖关系图中是否存在可达的边？

#### 工作原理（通俗解释）

我们使用图遍历从源节点开始探索。
如果到达了目标节点，则路径存在。

步骤：

1.  选择一种遍历方式（DFS 或 BFS）
2.  从源节点 `s` 开始
3.  标记已访问的节点
4.  递归地（DFS）或逐层地（BFS）遍历邻居节点
5.  如果目标节点 `t` 被访问到，则路径存在

#### 示例

图：
$$
V = {A, B, C, D}, \quad E = {(A, B), (B, C), (C, D)}
$$

查询：从 A 到 D 有路径吗？

遍历（DFS 或 BFS）：

-   从 A 开始 → B → C → D
-   到达 D → 路径存在 ✅

查询：从 D 到 A 有路径吗？

-   从 D 开始 → 没有出边 → 无路径 ❌

#### 微型代码（Python）

```python
from collections import deque

def path_exists(graph, source, target):
    visited = set()
    queue = deque([source])

    while queue:
        node = queue.popleft()
        if node == target:
            return True
        if node in visited:
            continue
        visited.add(node)
        queue.extend(graph.get(node, []))
    return False

graph = {
    "A": ["B"],
    "B": ["C"],
    "C": ["D"],
    "D": []
}
print(path_exists(graph, "A", "D"))  # True
print(path_exists(graph, "D", "A"))  # False
```

#### 为何重要

-   图连通性的核心
-   用于环检测、拓扑排序和可达性查询
-   是 AI 搜索、路由、编译器和网络分析的基础

#### 一个温和的证明（为何有效）

设图为 $G = (V, E)$，遍历方式为 BFS 或 DFS。
每条边 $(u, v)$ 只被探索一次。
如果存在路径，遍历最终将到达 `s` 所在连通分量中的所有节点。
因此，如果 `t` 位于该连通分量中，它将被发现。

遍历的完备性保证了正确性。

#### 动手试试

1.  构建一个有向图 $A \to B \to C$，并检查 $A \to C$ 和 $C \to A$。
2.  添加一条额外的边 $C \to A$。

    *   现在图是强连通的。
    *   每个节点都应该能到达其他所有节点。
3.  使用队列或递归跟踪来可视化遍历过程。

#### 测试用例

| 图结构       | 源点 | 目标点 | 结果  |
| ------------ | ---- | ------ | ----- |
| A→B→C        | A    | C      | True  |
| A→B→C        | C    | A      | False |
| A↔B          | A    | B      | True  |
| 不连通图     | A    | D      | False |

#### 复杂度

| 操作       | 时间复杂度 | 空间复杂度 |
| ---------- | ---------- | ---------- |
| BFS / DFS  | $O(n + m)$ | $O(n)$     |

$n$ = 顶点数，$m$ = 边数。

路径存在性检测器是图连通性最简单却最强大的诊断工具，它能揭示两个点是否属于同一个连通的世界。
### 86 树验证器

树验证器用于检查给定的图是否满足树的定义属性：连通且无环。

#### 我们要解决什么问题？

我们经常会遇到*看起来*像树的结构，但必须确认它们确实是树。
例如：

- 这个依赖图能表示为一棵树吗？
- 给定的父子关系是有效的层次结构吗？
- 这个无向图包含环或不连通的部分吗？

树验证器将这种检查形式化。

一棵树必须满足：

1.  连通性：从任一顶点出发可以到达其他所有顶点。
2.  无环性：不存在环。
3.  （对于无向图的等价条件）
    $$ |E| = |V| - 1 $$

#### 它是如何工作的？（通俗解释）

我们可以通过遍历和计数来验证：

方法一：深度优先搜索 + 父节点检查

1.  从任意节点开始深度优先搜索。
2.  跟踪已访问的节点。
3.  如果一个邻居已被访问*且不是父节点*，则存在环。
4.  遍历结束后，检查是否所有节点都被访问过（连通性）。

方法二：边-顶点属性

1.  检查图的边数是否恰好为 $|V| - 1$。
2.  运行深度优先搜索/广度优先搜索以确保图是连通的。

#### 示例

图 1：
$$
V = {A, B, C, D}, \quad E = {(A, B), (A, C), (B, D)}
$$

-   $|V| = 4$, $|E| = 3$
-   连通，无环 → ✅ 是树

图 2：
$$
V = {A, B, C}, \quad E = {(A, B), (B, C), (C, A)}
$$

-   $|V| = 3$, $|E| = 3$
-   存在环 → ❌ 不是树

#### 微型代码（Python）

```python
def is_tree(graph):
    n = len(graph)
    visited = set()
    parent = {}

    def dfs(node, par):
        visited.add(node)
        for nbr in graph[node]:
            if nbr == par:
                continue
            if nbr in visited:
                return False  # 检测到环
            if not dfs(nbr, node):
                return False
        return True

    # 从第一个节点开始
    start = next(iter(graph))
    if not dfs(start, None):
        return False

    # 检查连通性
    return len(visited) == n
```

示例：

```python
graph = {
    "A": ["B", "C"],
    "B": ["A", "D"],
    "C": ["A"],
    "D": ["B"]
}
print(is_tree(graph))  # True
```

#### 为什么它很重要

树验证确保了：

-   层次结构是无环的
-   数据结构（如抽象语法树、字典树）是良构的
-   网络拓扑避免了冗余链接
-   依赖树属性的算法（如深度优先搜索顺序、最近公共祖先、生成树）是安全的

#### 一个温和的证明（为什么它有效）

一个连通且无环的图就是一棵树。
归纳推理：

-   基础情况：单个节点，零条边，显然是一棵树。
-   归纳步骤：添加一条连接新节点的边可以保持无环性。
    如果形成了环，则违反了树的性质。

此外，对于无向图：
$$
\text{树} \iff \text{连通} \land |E| = |V| - 1
$$

#### 自己动手试试

1.  画一个有 4 个节点的小图。
2.  一条一条地添加边。

    *   每次添加后，测试图是否仍然是树。
3.  引入一个环，然后重新运行验证器。
4.  移除一条边，检查连通性是否被破坏。

#### 测试用例

| 图             | 连通性 | 环   | 是树 |
| -------------- | ------ | ---- | ---- |
| A–B–C          | ✅      | ❌    | ✅    |
| A–B, B–C, C–A  | ✅      | ✅    | ❌    |
| A–B, C         | ❌      | ❌    | ❌    |
| 单节点         | ✅      | ❌    | ✅    |

#### 复杂度

| 操作       | 时间复杂度 | 空间复杂度 |
| ---------- | ---------- | ---------- |
| 深度优先搜索 | $O(n + m)$ | $O(n)$     |

树验证器确保了结构、顺序和简洁性，这是每个层次结构背后静谧的几何学。
### 86 树验证器

树验证器用于检查给定的图是否满足树的定义属性：连通且无环。

#### 我们要解决什么问题？

我们经常会遇到一些*看起来*像树的结构，但必须确认它们确实是树。
例如：

- 这个依赖图能表示为一棵树吗？
- 给定的父子关系是有效的层次结构吗？
- 这个无向图是否包含环或不连通的部分？

树验证器将这种检查形式化。

一棵树必须满足：

1. 连通性：从任一顶点出发都可到达其他所有顶点。
2. 无环性：不存在环。
3. （对于无向图，等价条件）
   $$|E| = |V| - 1$$

#### 工作原理（通俗解释）

我们可以通过遍历和计数来验证。

方法 1：深度优先搜索 + 父节点检查

1. 从任意节点开始深度优先搜索。
2. 跟踪已访问的节点。
3. 如果一个邻居已被访问*且不是父节点*，则存在环。
4. 遍历结束后，检查是否所有节点都被访问过（连通性）。

方法 2：边-顶点属性

1. 检查图的边数是否恰好为 $|V| - 1$。
2. 运行深度优先搜索或广度优先搜索以确保图是连通的。

#### 示例

图 1：
$$
V = {A, B, C, D}, \quad E = {(A, B), (A, C), (B, D)}
$$

- $|V| = 4$, $|E| = 3$
- 连通，无环 → 是树

图 2：
$$
V = {A, B, C}, \quad E = {(A, B), (B, C), (C, A)}
$$

- $|V| = 3$, $|E| = 3$
- 存在环 → 不是树

#### 微型代码（Python）

```python
def is_tree(graph):
    n = len(graph)
    visited = set()
    parent = {}

    def dfs(node, par):
        visited.add(node)
        for nbr in graph[node]:
            if nbr == par:
                continue
            if nbr in visited:
                return False  # 检测到环
            if not dfs(nbr, node):
                return False
        return True

    # 从第一个节点开始
    start = next(iter(graph))
    if not dfs(start, None):
        return False

    # 检查连通性
    return len(visited) == n
```

示例：

```python
graph = {
    "A": ["B", "C"],
    "B": ["A", "D"],
    "C": ["A"],
    "D": ["B"]
}
print(is_tree(graph))  # True
```

#### 为什么重要

树验证确保：

- 层次结构是无环的
- 数据结构（如抽象语法树、字典树）是良构的
- 网络拓扑避免冗余连接
- 依赖于树属性的算法（DFS 顺序、最近公共祖先、生成树）是安全的

#### 一个温和的证明（为什么有效）

一个连通且无环的图就是树。
归纳推理：

- 基础情况：单个节点，零条边，显然是一棵树。
- 归纳步骤：添加一条连接新节点的边可以保持无环性。
  如果形成了环，则违反了树的性质。

此外，对于无向图：
$$
\text{树} \iff \text{连通} \land |E| = |V| - 1
$$

#### 动手试试

1.  画一个有 4 个节点的小图。
2.  逐一添加边。
    *   每次添加后，测试图是否仍然是树。
3.  引入一个环，然后重新运行验证器。
4.  移除一条边，检查连通性是否失效。

#### 测试用例

| 图             | 连通？ | 有环？ | 是树？ |
| -------------- | ------ | ------ | ------ |
| A–B–C          | 是     | 否     | 是     |
| A–B, B–C, C–A  | 是     | 是     | 否     |
| A–B, C         | 否     | 否     | 否     |
| 单节点         | 是     | 否     | 是     |

#### 复杂度

| 操作     | 时间复杂度   | 空间复杂度 |
| -------- | ------------ | ---------- |
| 深度优先搜索 | $O(n + m)$ | $O(n)$     |

树验证器确保了结构、顺序和简洁性，这是每个层次结构背后静默的几何学。
### 87 有根树构建器

有根树构建器根据给定的父节点数组或边列表构建一棵树，指定一个节点作为根节点，并相应地连接所有其他节点。

#### 我们要解决什么问题？

我们经常以*扁平*形式接收数据——例如父节点索引列表、数据库引用或父子节点对——我们需要重建实际的树结构。

例如：

- 一个父节点数组 `[ -1, 0, 0, 1, 1, 2 ]` 表示每个节点的父节点是哪个。
- 在文件系统中，每个目录都知道其父目录；我们需要重建层级结构。

有根树构建器将这种重建过程形式化。

#### 它是如何工作的（通俗解释）

父节点数组编码了每个节点的父节点：

- `parent[i] = j` 意味着节点 `j` 是节点 `i` 的父节点。
- 如果 `parent[i] = -1`，那么 `i` 是根节点。

步骤：

1.  找到根节点（父节点为 `-1` 的节点）。
2.  为每个节点初始化一个邻接列表 `children`。
3.  对于每个节点 `i`：
   * 如果 `parent[i] != -1`，则将 `i` 添加到 `children[parent[i]]` 中。
4.  输出邻接结构。

这样就得到了一棵具有父子关系的树。

#### 示例

父节点数组：

```
索引:  0  1  2  3  4  5
父节点: -1  0  0  1  1  2
```

解释：

- `0` 是根节点。
- `1` 和 `2` 是 `0` 的子节点。
- `3` 和 `4` 是 `1` 的子节点。
- `5` 是 `2` 的子节点。

树：

```
0
├── 1
│   ├── 3
│   └── 4
└── 2
    └── 5
```

#### 微型代码（Python）

```python
def build_tree(parent):
    n = len(parent)
    children = [[] for _ in range(n)]
    root = None

    for i in range(n):
        if parent[i] == -1:
            root = i
        else:
            children[parent[i]].append(i)

    return root, children
```

示例：

```python
parent = [-1, 0, 0, 1, 1, 2]
root, children = build_tree(parent)

print("根节点:", root)
for i, c in enumerate(children):
    print(f"{i}: {c}")
```

输出：

```
根节点: 0
0: [1, 2]
1: [3, 4]
2: [5]
3: []
4: []
5: []
```

#### 为什么它很重要

树重建在以下领域是基础性的：

- 编译器：抽象语法树（AST）重建
- 数据库：重建分层关系
- 操作系统：文件目录树
- 组织结构图：根据父子数据构建层级结构

它连接了线性存储和分层结构。

#### 一个温和的证明（为什么它有效）

如果父节点数组满足：

- 恰好有一个根节点：一个条目为 `-1`
- 所有其他节点恰好有一个父节点
- 生成的结构是连通且无环的

那么输出就是一棵有效的有根树：
$$
|E| = |V| - 1, \text{ 并且恰好有一个节点没有父节点。}
$$

每个子节点被链接一次，形成一棵以唯一具有 `-1` 的节点为根的树。

#### 自己动手试试

1.  编写你自己的父节点数组（例如，`[ -1, 0, 0, 1, 2 ]`）。
2.  将其转换成一棵树。
3.  手动绘制层级结构。
4.  验证连通性和无环性。

#### 测试用例

| 父节点数组          | 根节点 | 子节点结构                |
| ------------------- | ------ | ------------------------- |
| [-1, 0, 0, 1, 1, 2] | 0      | 0:[1,2], 1:[3,4], 2:[5]   |
| [-1, 0, 1, 2]       | 0      | 0:[1], 1:[2], 2:[3]       |
| [-1]                | 0      | 0:[]                      |

#### 复杂度

| 操作     | 时间复杂度 | 空间复杂度 |
| -------- | ---------- | ---------- |
| 构建     | $O(n)$     | $O(n)$     |

有根树构建器弥合了扁平数据与分层形式之间的鸿沟，将数组转化为活生生的结构。
### 88 遍历顺序可视化工具

遍历顺序可视化工具展示了不同的树遍历方式（前序、中序、后序、层序）如何探索节点，揭示了递归和迭代访问背后的逻辑。

#### 我们要解决什么问题？

在处理树结构时，访问节点的顺序至关重要。不同的遍历方式服务于不同的目标：

- 前序：先处理父节点，再处理子节点
- 中序：先处理左子节点，再处理父节点，最后处理右子节点
- 后序：先处理子节点，再处理父节点
- 层序：广度优先访问节点

理解这些遍历方式有助于：

- 表达式解析
- 文件系统导航
- 树的打印和求值

可视化工具阐明了每个节点*何时*以及*为何*被访问。

#### 工作原理（通俗解释）

考虑一棵二叉树：

```
      A
     / \
    B   C
   / \
  D   E
```

每种遍历方式以不同的顺序排列节点：

| 遍历方式   | 顺序           |
| ----------- | ------------- |
| 前序        | A, B, D, E, C |
| 中序        | D, B, E, A, C |
| 后序        | D, E, B, C, A |
| 层序        | A, B, C, D, E |

可视化策略：

- 从根节点开始。
- 使用递归（深度优先）或队列（广度优先）。
- 记录每次访问步骤。
- 按访问顺序输出序列。

#### 逐步示例

树：

```
A
├── B
│   ├── D
│   └── E
└── C
```

前序

1. 访问 A
2. 访问 B
3. 访问 D
4. 访问 E
5. 访问 C

序列：A B D E C

中序

1. 遍历 A 的左子树 (B)
2. 遍历 B 的左子树 (D) → 访问 D
3. 访问 B
4. 遍历 B 的右子树 (E) → 访问 E
5. 访问 A
6. 访问右子树 (C)

序列：D B E A C

#### 微型代码（Python）

```python
class Node:
    def __init__(self, val):
        self.val = val
        self.left = None
        self.right = None

def preorder(root):
    if not root:
        return []
    return [root.val] + preorder(root.left) + preorder(root.right)

def inorder(root):
    if not root:
        return []
    return inorder(root.left) + [root.val] + inorder(root.right)

def postorder(root):
    if not root:
        return []
    return postorder(root.left) + postorder(root.right) + [root.val]

def level_order(root):
    if not root:
        return []
    queue = [root]
    result = []
    while queue:
        node = queue.pop(0)
        result.append(node.val)
        if node.left:
            queue.append(node.left)
        if node.right:
            queue.append(node.right)
    return result
```

#### 为何重要

遍历顺序决定了：

- 计算序列（求值、删除、打印）
- 表达式树求值（后序）
- 序列化/反序列化（前序 + 中序）
- 广度优先探索（层序）

理解遍历 = 理解算法如何在结构中移动。

#### 一个温和的证明（为何有效）

每种遍历都是一次系统性的遍历：

- 前序确保根节点优先访问。
- 中序确保在二叉搜索树中得到排序顺序。
- 后序确保子节点在父节点之前被处理。
- 层序确保按最小深度优先分层。

由于每个节点恰好被访问一次，其正确性源于递归和归纳。

#### 亲自尝试

1.  构建一棵包含 5 个节点的二叉树。
2.  手动写出所有四种遍历顺序。
3.  逐步跟踪递归调用。
4.  观察每种遍历方式下顺序如何变化。

#### 测试用例

| 遍历方式   | 示例树       | 期望顺序       |
| ----------- | ------------ | -------------- |
| 前序        | A-B-C        | A B C          |
| 中序        | A-B-C        | B A C          |
| 后序        | A-B-C        | B C A          |
| 层序        | A-B-C        | A B C          |

#### 复杂度

| 操作              | 时间复杂度 | 空间复杂度         |
| ----------------- | ------ | -------------- |
| DFS (前/中/后序) | $O(n)$ | $O(h)$ (栈)    |
| BFS (层序)        | $O(n)$ | $O(n)$ (队列)  |

遍历顺序可视化工具将抽象的定义转化为动态过程，展示了结构如何引导计算。
### 89 边分类器

边分类器用于确定图遍历过程中遇到的每条边的类型，无论它是树边、后向边、前向边还是交叉边。这种分类有助于我们理解有向图或无向图的结构和流向。

#### 我们要解决什么问题？

在图算法中，并非所有边都扮演相同的角色。当我们使用深度优先搜索（DFS）进行遍历时，可以根据发现时间来解释顶点之间的关系。

边分类有助于回答以下问题：

- 图中是否存在环？（寻找后向边）
- 图的结构是怎样的？（树边与前向边）
- 这是有向无环图（DAG）吗？（没有后向边）
- 节点之间的层次关系是什么？

通过对边进行标记，我们可以深入理解遍历行为所揭示的结构信息。

#### 工作原理（通俗解释）

在深度优先搜索（DFS）过程中，我们为每个顶点分配：

- **发现时间**：首次访问该顶点的时间。
- **完成时间**：探索完成该顶点的时间。

然后，每条边 $(u, v)$ 根据以下条件被分类：

| 类型         | 条件                                       |
| ------------ | ------------------------------------------ |
| 树边         | $v$ 是通过边 $(u, v)$ 首次被发现的         |
| 后向边       | $v$ 是 $u$ 的祖先（环的指示器）            |
| 前向边       | $v$ 是 $u$ 的后代，但已经被访问过          |
| 交叉边       | $v$ 既不是 $u$ 的祖先也不是 $u$ 的后代     |

在无向图中，只会出现树边和后向边。

#### 示例

有向图：

```
1 → 2 → 3
↑   ↓
4 ← 5
```

从顶点 1 开始进行深度优先搜索（DFS）：

- (1,2): 树边
- (2,3): 树边
- (3,4): 后向边（环 1–2–3–4–1）
- (2,5): 树边
- (5,4): 树边
- (4,1): 后向边

因此，我们通过后向边检测到了环。

#### 微型代码（Python）

```python
def classify_edges(graph):
    time = 0
    discovered = {}
    finished = {}
    classification = []

    def dfs(u):
        nonlocal time
        time += 1
        discovered[u] = time
        for v in graph[u]:
            if v not in discovered:
                classification.append(((u, v), "Tree"))
                dfs(v)
            elif v not in finished:
                classification.append(((u, v), "Back"))
            elif discovered[u] < discovered[v]:
                classification.append(((u, v), "Forward"))
            else:
                classification.append(((u, v), "Cross"))
        time += 1
        finished[u] = time

    for node in graph:
        if node not in discovered:
            dfs(node)
    return classification
```

#### 为什么它很重要？

边分类是以下算法的基础：

- 环检测（寻找后向边）
- 拓扑排序（有向无环图没有后向边）
- 深度优先搜索树结构分析
- 强连通分量检测

它将遍历过程转化为对结构的洞察。

#### 一个温和的证明（为什么它有效）

深度优先搜索（DFS）对发现时间和完成时间施加了时间顺序。
一条边 $(u, v)$ 只能落入四种类型中的一种，因为：

$$
\text{每个顶点都有一个不同的发现时间和完成时间区间。}
$$

通过比较区间 $(d[u], f[u])$ 和 $(d[v], f[v])$，
我们可以推断出 $v$ 是位于 $u$ 的遍历窗口之内、之前还是之后。

#### 自己动手试试

1.  绘制一个小型有向图。
2.  使用深度优先搜索（DFS）分配发现/完成时间。
3.  比较每条边的区间。
4.  将每条边标记为树边、后向边、前向边或交叉边。
5.  验证有向无环图没有后向边。

#### 测试用例

| 边     | 类型   |
| ------ | ------ |
| (A, B) | 树边   |
| (B, C) | 树边   |
| (C, A) | 后向边 |
| (B, D) | 树边   |
| (D, E) | 树边   |
| (E, B) | 后向边 |

#### 复杂度

| 操作           | 时间复杂度 | 空间复杂度 |
| -------------- | ---------- | ---------- |
| DFS 遍历       | $O(n + m)$ | $O(n)$     |
| 边分类         | $O(m)$     | $O(m)$     |

边分类器将遍历过程转化为拓扑结构，使得环、层次结构和交叉链接等不可见的结构变得显式可见。
### 90 连通性检查器

连通性检查器用于判断一个图是否连通，即是否可以从任意一个顶点到达其他所有顶点。它是图论和网络分析中的一个基本诊断工具。

#### 我们要解决什么问题？

连通性告诉我们这个图是形成一个整体还是多个孤立的部分。

我们经常问：

- 在这个网络中，所有节点都能通信吗？
- 这个迷宫从起点到终点是可解的吗？
- 这个无向图是形成一个连通分量还是多个？
- 对于有向图：我们能从任意顶点到达其他所有顶点吗？

连通性检查器给出是/否的答案，并且可以枚举连通分量。

#### 工作原理（通俗解释）

**无向图：**

1.  选择一个起始节点。
2.  执行 DFS 或 BFS，标记所有可达节点。
3.  遍历结束后，如果所有节点都被标记，则该图是连通的。

**有向图：**

- 使用两次遍历：
  1.  从任意节点运行 DFS。如果没有访问到所有节点，则不是强连通的。
  2.  反转所有边，再次运行 DFS。
     如果仍然没有访问到所有节点，则不是强连通的。

或者，通过 Kosaraju 算法或 Tarjan 算法检测强连通分量。

#### 示例（无向图）

图 1：

```
1, 2, 3
|       |
4, 5, 6
```

所有节点可达 → 连通。

图 2：

```
1, 2    3, 4
```

两个独立部分 → 不连通。

#### 示例（有向图）

图：

```
1 → 2 → 3
↑       ↓
└───────┘
```

每个节点都可以从其他任意节点到达 → 强连通

图：

```
1 → 2 → 3
```

没有从 3 到 1 的路径 → 不是强连通

#### 微型代码（Python）

```python
from collections import deque

def is_connected(graph):
    n = len(graph)
    visited = set()

    # 从第一个节点开始 BFS
    start = next(iter(graph))
    queue = deque([start])
    while queue:
        u = queue.popleft()
        if u in visited:
            continue
        visited.add(u)
        for v in graph[u]:
            if v not in visited:
                queue.append(v)
    
    return len(visited) == n
```

示例：

```python
graph = {
    1: [2, 4],
    2: [1, 3],
    3: [2, 6],
    4: [1, 5],
    5: [4, 6],
    6: [3, 5]
}
print(is_connected(graph))  # True
```

#### 为什么它很重要

连通性在以下方面至关重要：

- 网络可靠性，确保所有节点都能通信
- 图算法，许多算法假设图是连通的
- 聚类，寻找连通分量
- 路径查找，不可达节点意味着存在障碍

在进行更深入的分析之前，它通常是*第一个诊断检查*。

#### 一个温和的证明（为什么它有效）

对于无向图，连通性是一种等价关系：

- 自反性：节点与自身连通
- 对称性：如果 A 连通到 B，则 B 连通到 A
- 传递性：如果 A 连通到 B 且 B 连通到 C，则 A 连通到 C

因此，DFS/BFS 可达性划分唯一定义了连通分量。

#### 自己动手试试

1.  画一个有 6 个节点的图。
2.  从节点 1 运行 BFS 或 DFS。
3.  标记所有可达节点。
4.  如果有些节点未被访问，你就找到了多个分量。
5.  对于有向图，尝试反转边并重新测试。

#### 测试用例

| 图           | 类型       | 结果                   |
| ------------ | ---------- | ---------------------- |
| 1–2–3        | 无向图     | 连通                   |
| 1–2, 3–4     | 无向图     | 不连通                 |
| 1→2→3, 3→1   | 有向图     | 强连通                 |
| 1→2→3        | 有向图     | 不是强连通             |

#### 复杂度

| 操作      | 时间复杂度 | 空间复杂度 |
| --------- | ---------- | ---------- |
| DFS/BFS   | $O(n + m)$ | $O(n)$     |

连通性检查器确保你的图是一个完整的故事，而不是一系列孤立的片段，这是在图中进行每一次探索之前的基础。

## 第 10 节 算法设计模式
### 91 暴力破解模式

暴力破解模式是解决问题最简单且最通用的方法：尝试所有可能的选项，逐一评估，然后选出最佳方案。它以计算效率换取概念的清晰性和正确性。

#### 我们要解决什么问题？

有时，在采用巧妙的优化或启发式方法之前，我们需要一个基准解决方案，一种确保正确性的方法。暴力破解方法通过探索所有可能的配置来保证找到正确答案，即使速度很慢。

常见用例：
- 穷举搜索（例如，生成所有排列或子集）
- 实现启发式方法前的基准测试
- 通过比较证明最优性

#### 工作原理（通俗解释）

暴力破解算法通常遵循以下结构：
1. 枚举所有候选解。
2. 评估每个候选解的有效性或成本。
3. 选择最佳（或第一个有效）解。

这在概念上很简单，但通常在时间上代价高昂。

#### 示例：旅行商问题（TSP）

给定 $n$ 个城市及它们之间的距离，找到访问所有城市的最短路线。

暴力破解解决方案：
1. 生成所有 $n!$ 条可能的路线。
2. 计算每条路线的总距离。
3. 返回最短路线。

这确保了正确性，但复杂度呈阶乘级增长。

#### 微型代码（Python）

```python
from itertools import permutations

def tsp_bruteforce(dist):
    n = len(dist)
    cities = list(range(n))
    best = float('inf')
    best_path = None
    
    for perm in permutations(cities[1:]):  # 固定城市 0 为起点
        path = [0] + list(perm) + [0]
        cost = sum(dist[path[i]][path[i+1]] for i in range(n))
        if cost < best:
            best = cost
            best_path = path
    return best, best_path

# 示例距离矩阵
dist = [
    [0, 10, 15, 20],
    [10, 0, 35, 25],
    [15, 35, 0, 30],
    [20, 25, 30, 0]
]

print(tsp_bruteforce(dist))  # (80, [0, 1, 3, 2, 0])
```

#### 为何重要

暴力破解的价值在于：
- **正确性**：保证得到正确答案。
- **基准测试**：为优化提供基准真相。
- **小规模输入**：当 $n$ 较小时通常可行。
- **教学**：阐明搜索和评估的结构。

它是更精炼算法（如动态规划、回溯和启发式算法）发展的种子。

#### 一个温和的证明（为何有效）

令 $S$ 为所有可能解的有限集合。
如果算法评估了每个 $s \in S$ 并正确计算其质量，并选择最小值（或最大值），那么所选 $s^*$ 可证明是最优的：
$$
s^* = \arg\min_{s \in S} f(s)
$$
完备性和正确性是固有的，但效率不是。

#### 动手尝试

1. 枚举 ${1, 2, 3}$ 的所有子集。
2. 检查哪些子集的和为 4。
3. 确认考虑了所有可能性。
4. 思考时间成本：$n$ 个元素有 $2^n$ 个子集。

#### 测试用例

| 问题       | 输入大小 | 是否可行？ | 备注                          |
| ---------- | -------- | ---------- | ----------------------------- |
| TSP        | n = 4    | ✅          | $4! = 24$ 条路径              |
| TSP        | n = 10   | ❌          | $10! \approx 3.6 \times 10^6$ |
| 子集和问题 | n = 10   | ✅          | $2^{10} = 1024$ 个子集       |
| 子集和问题 | n = 30   | ❌          | $2^{30} \approx 10^9$ 个子集 |

#### 复杂度

| 操作       | 时间              | 空间   |
| ---------- | ----------------- | ------ |
| 枚举       | $O(k^n)$ （可变） | $O(n)$ |

暴力破解模式是算法设计的空白画布：简单、穷尽且纯粹，是在寻求优雅之前保证真相的一种方式。
### 92 贪心模式

贪心模式通过逐步构建解决方案来工作，在每个阶段选择局部最优的移动，即当前看起来最好的选择，并希望（且通常可以证明）这条路径能导向全局最优结果。

#### 我们要解决什么问题？

当问题展现出两个关键性质时，会使用贪心算法：

1. **贪心选择性质** —— 可以通过选择局部最优来达到全局最优。
2. **最优子结构** —— 一个最优解包含其子问题的最优解。

你会在各处遇到贪心推理：调度、寻路、压缩和资源分配。

#### 它是如何工作的（通俗解释）

贪心思维就是"每次都拿最好的那一口"。
没有回头路，不探索替代方案，只是一系列决定性的步骤。

一般形式：

1. 从一个空的或初始的解决方案开始。
2. 重复地根据某个规则选择最佳的局部移动。
3. 当没有更多可能的或期望的移动时停止。

#### 示例：硬币找零（标准硬币体系）

给定硬币面值 ${25, 10, 5, 1}$，为 63 美分找零。

贪心方法：

- 取小于等于剩余值的最大硬币。
- 减去并重复。
  结果：$25 + 25 + 10 + 1 + 1 + 1 = 63$（总共 6 枚硬币）

这适用于标准硬币体系，但并非所有体系都适用，这是一个很好的教学点。

#### 微型代码（Python）

```python
def greedy_coin_change(coins, amount):
    result = []
    for c in sorted(coins, reverse=True):
        while amount >= c:
            amount -= c
            result.append(c)
    return result

print(greedy_coin_change([25, 10, 5, 1], 63))
# [25, 25, 10, 1, 1, 1]
```

#### 为什么它很重要

贪心模式是一个核心设计范式：

- **简单快速** —— 通常是线性或 $O(n \log n)$ 复杂度。
- **在条件满足时可证明最优**。
- **直观** —— 有助于洞察问题的结构。
- **基础** —— 许多近似算法和启发式算法"本质上都是贪心的"。

#### 一个温和的证明（为什么它有效）

对于具有最优子结构的问题，我们通常可以用归纳法证明：

如果一个贪心选择 $g$ 留下一个子问题 $P'$，并且
$$\text{OPT}(P) = g + \text{OPT}(P')$$
那么最优地解决 $P'$ 就能确保全局最优性。

对于标准硬币体系的找零问题，这成立，因为选择较大面值的硬币永远不会妨碍达到最优总数。

#### 自己动手试试

1. 将贪心方法应用于活动选择问题：
   按结束时间对活动排序，选择最早结束的活动，并跳过重叠的活动。
2. 与暴力枚举法进行比较。
3. 检查贪心结果是否最优，为什么是或为什么不是？

#### 测试用例

| 问题                     | 贪心有效？ | 备注                                      |
| ------------------------ | ---------- | ----------------------------------------- |
| 活动选择                 | ✅          | 局部最早结束导向全局最大值                |
| 硬币找零 (1, 3, 4) 凑 6  | ❌          | 3+3 比 4+1+1 更优                         |
| 霍夫曼编码               | ✅          | 贪心合并产生最优树                        |
| Kruskal 最小生成树算法   | ✅          | 贪心选择边构建最小生成树                  |

#### 复杂度

| 操作       | 时间                    | 空间  |
| ---------- | ----------------------- | ----- |
| 选择       | $O(n \log n)$（排序）   | $O(1)$ |
| 步骤选择   | $O(n)$                  | $O(1)$ |

贪心模式是果断推理的艺术，选择当前看起来最好的，并相信问题的结构会回报这份信心。
### 93 分治模式

分治模式将一个大问题分解为更小、相似的子问题，分别解决每个子问题（通常是递归地），然后将它们的结果组合成最终答案。

它是归并排序、快速排序、二分查找以及数学和计算领域中众多快速算法背后的模式。

#### 我们要解决什么问题？

在以下情况下我们使用分治法：

1. 问题可以分解为相同类型的更小子问题。
2. 这些子问题是独立的且更容易解决。
3. 它们的解可以被高效地合并。

它是数学归纳法、分解、解决、合并的算法镜像。

#### 工作原理（通俗解释）

将分治法视为一个递归的三步舞：

1. **分解** – 将问题分割成更小的部分。
2. **解决** – 递归地解决每个部分。
3. **合并** – 将子结果合并成最终答案。

每次递归调用处理一部分工作，直到达到基本情况。

#### 示例：归并排序

对数组 $A[1..n]$ 进行排序。

1. **分解**：将 $A$ 分成两半。
2. **解决**：递归地对每一半进行排序。
3. **合并**：合并两个已排序的半部分。

递归关系：
$$T(n) = 2T\left(\frac{n}{2}\right) + O(n)$$
解：
$$T(n) = O(n \log n)$$

#### 微型代码（Python）

```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])
    return merge(left, right)

def merge(left, right):
    result = []
    i = j = 0
    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            result.append(left[i]); i += 1
        else:
            result.append(right[j]); j += 1
    result.extend(left[i:]); result.extend(right[j:])
    return result
```

#### 为何重要

分治法将递归转化为效率。它是以下领域的框架：

- 排序（归并排序、快速排序）
- 搜索（二分查找）
- 矩阵乘法（Strassen 算法）
- 快速傅里叶变换（FFT）
- 几何（最近点对、凸包）
- 数据科学（分治回归、决策树）

它体现了这个原则：*通过缩小问题来解决大问题。*

#### 一个温和的证明（为何有效）

假设每个大小为 $\frac{n}{2}$ 的子问题都得到了最优解。

如果我们以成本 $f(n)$ 合并 $k$ 个子结果，总成本遵循递归关系
$$T(n) = aT\left(\frac{n}{b}\right) + f(n)$$

使用主定理，我们比较 $f(n)$ 和 $n^{\log_b a}$ 来求解 $T(n)$。

对于归并排序：
$a = 2, b = 2, f(n) = n$ ⇒ $T(n) = O(n \log n)$。

#### 亲自尝试

1. 将分治法应用于最大子数组和问题（Kadane 算法的替代方案）。
2. 编写一个具有清晰分解/解决步骤的二分查找。
3. 可视化递归树以及每一层的总成本。

#### 测试用例

| 问题         | 分解           | 合并           | 效果良好？ |
| ------------ | -------------- | -------------- | ---------- |
| 归并排序     | 分割数组       | 合并两半       | ✅          |
| 快速排序     | 划分数组       | 连接           | ✅（平均）  |
| 二分查找     | 分割范围       | 返回匹配项     | ✅          |
| 最近点对问题 | 分割平面       | 比较边界       | ✅          |

#### 复杂度

| 步骤     | 成本               |
| -------- | ------------------ |
| 分解     | $O(1)$ 或 $O(n)$   |
| 解决     | $aT(n/b)$          |
| 合并     | $O(n)$（典型情况） |

总体而言：在许多经典情况下为 $O(n \log n)$。

分治法是递归分解的精髓，通过掌握部分来洞察整体。
### 94 动态规划模式

动态规划（Dynamic Programming，DP）模式通过将复杂问题分解为重叠的子问题，每个子问题只求解一次，并存储结果以避免重复计算，从而解决问题。

它通过记忆化（memoization）或制表法（tabulation），将指数级的递归解决方案转化为高效的多项式级解决方案。

#### 我们正在解决什么问题？

当一个问题是：

1.  **重叠子问题** – 相同的子任务多次出现。
2.  **最优子结构** – 最优解可以由其子问题的最优解构建。

朴素的递归会重复工作。动态规划确保每个子问题只被求解一次。

#### 工作原理（通俗解释）

将动态规划视为智能递归：

- 定义一个状态，用以捕捉进度。
- 定义一个递推关系，将较大的状态与较小的状态联系起来。
- 存储结果以便后续复用。

两种主要实现方式：

1.  **自顶向下（记忆化）** – 带缓存的递归。
2.  **自底向上（制表法）** – 迭代地填充表格。

#### 示例：斐波那契数列

朴素递归：
$$F(n) = F(n-1) + F(n-2)$$
这会重复计算许多值。

动态规划解决方案：

1.  基础情况：$F(0)=0, F(1)=1$
2.  自底向上构建表格：
    $$F[i] = F[i-1] + F[i-2]$$

结果：时间复杂度 $O(n)$，空间复杂度 $O(n)$（或优化后为 $O(1)$）。

#### 微型代码（Python）

```python
def fib(n):
    dp = [0, 1] + [0]*(n-1)
    for i in range(2, n+1):
        dp[i] = dp[i-1] + dp[i-2]
    return dp[n]
```

或者记忆化递归：

```python
from functools import lru_cache

@lru_cache(None)
def fib(n):
    if n < 2:
        return n
    return fib(n-1) + fib(n-2)
```

#### 为何重要

动态规划是算法问题求解的核心：

-   **优化问题**：最短路径、背包问题、编辑距离
-   **计数问题**：爬楼梯方式数、划分方式数
-   **序列分析**：最长递增子序列（LIS）、最长公共子序列（LCS）
-   **资源分配**：调度问题、投资问题

它是我们为递归赋予结构的方式。

#### 一个温和的证明（为何有效）

令 $T(n)$ 为解决所有不同子问题的成本。
由于每个子问题只被求解一次，并以常数时间组合：
$$T(n) = O(\text{状态数量}) \times O(\text{状态转移成本})$$

对于斐波那契数列：

-   状态数量 = $n$
-   状态转移成本 = $O(1)$
    ⇒ $T(n) = O(n)$

记忆化确保每个子问题最多被访问一次。

#### 亲自尝试

1.  为硬币找零问题（组成某个金额的方式数）编写动态规划代码。
2.  追踪最长公共子序列（LCS）的表格构建过程。
3.  比较自顶向下与自底向上方法的性能。

#### 测试用例

| 问题           | 状态      | 状态转移                               | 时间复杂度 |
| -------------- | --------- | -------------------------------------- | ---------- |
| 斐波那契数列   | $n$       | $dp[n]=dp[n-1]+dp[n-2]$                | $O(n)$     |
| 背包问题       | $(i,w)$   | $\max(\text{取}, \text{不取})$         | $O(nW)$    |
| 编辑距离       | $(i,j)$   | 比较字符                               | $O(nm)$    |

#### 复杂度

| 类型                 | 时间复杂度                | 空间复杂度                |
| -------------------- | ------------------------- | ------------------------- |
| 自顶向下记忆化       | $O(\text{\#states})$      | $O(\text{\#states})$      |
| 自底向上制表法       | $O(\text{\#states})$      | $O(\text{\#states})$      |

动态规划是带有记忆的分治法，递归地思考，计算一次，永久复用。
### 95 回溯模式

回溯模式通过逐步构建解决方案，并在路径变得无效时立即放弃，来探索所有可能的解。

对于需要生成组合、排列或子集，并尽早剪除不可能或非最优分支的问题，它是一种系统化的搜索策略。

#### 我们要解决什么问题？

我们面临的问题具有以下特点：
- 解空间很大，但结构清晰。
- 我们可以尽早检测到无效的部分解。

示例：
- N皇后问题（安全放置皇后）
- 数独（在约束条件下填充网格）
- 子集和问题（选择元素使其和等于目标值）

暴力搜索盲目地探索所有可能性。
回溯则在死路一出现时就将其剪除。

#### 工作原理（通俗解释）

想象探索一个迷宫：
1. 迈出一步（做出一个选择）。
2. 如果它导向一个有效的部分解，则继续前进。
3. 如果失败，则回溯，撤销并尝试另一条路径。

递归的每一层都对应一个决策点。

#### 示例：N皇后问题

我们需要在 $n \times n$ 的棋盘上放置 $n$ 个皇后，使得任意两个皇后都不能互相攻击。

在每一行，选择一个安全的列。
如果没有可行的列，则回溯到上一行。

#### 微型代码（Python）

```python
def solve_n_queens(n):
    res, board = [], [-1]*n

    def is_safe(row, col):
        for r in range(row):
            c = board[r]
            if c == col or abs(c - col) == abs(r - row):
                return False
        return True

    def backtrack(row=0):
        if row == n:
            res.append(board[:])
            return
        for col in range(n):
            if is_safe(row, col):
                board[row] = col
                backtrack(row + 1)
                board[row] = -1  # 撤销

    backtrack()
    return res
```

#### 为什么它很重要

回溯是一种通用的求解方法，适用于：
- 组合搜索：子集、排列、划分
- 约束满足：数独、图着色、N皇后问题
- 带剪枝的优化（分支定界法基于此构建）

它不仅仅是暴力搜索，而是有指导的探索。

#### 一个温和的证明（为什么它有效）

设 $S$ 为可能状态的总数。
回溯尽早剪除了所有无效路径，
因此实际访问的节点数 $\le S$。

如果检查每个状态并递归需要 $O(1)$ 时间，
则总复杂度与有效部分状态的数量成正比，
通常远小于完全枚举。

#### 亲自尝试

1. 使用回溯解决子集和问题。
2. 生成 `[1,2,3]` 的所有排列。
3. 实现数独求解器（9×9 约束满足）。

跟踪调用过程，每个递归调用代表一个部分决策。

#### 测试用例

| 问题       | 决策           | 约束条件               | 输出           |
| ---------- | -------------- | ---------------------- | -------------- |
| N皇后问题  | 选择列         | 皇后互不攻击           | 放置方案       |
| 子集和问题 | 包含/排除      | 和 ≤ 目标值            | 有效子集       |
| 数独       | 填充单元格     | 行/列/子网格内数字唯一 | 已完成的网格   |

#### 复杂度

| 问题       | 时间复杂度     | 空间复杂度 |
| ---------- | -------------- | ---------- |
| N皇后问题  | $O(n!)$ 最坏情况 | $O(n)$     |
| 子集和问题 | $O(2^n)$       | $O(n)$     |
| 数独       | 指数级         | 网格大小   |

回溯是搜索的艺术：撤销、尝试、测试、后退，直到找到一条有效路径。
### 96 分支限界法

分支限界模式是一种优化框架，它系统地探索搜索空间，同时剪枝那些无法产生比当前已知最优解更好的解的分支。

它通过引入界限来扩展回溯法，使我们能够尽早跳过没有希望的分支。

#### 我们要解决什么问题？

我们希望解决以下优化问题：
- 搜索空间是组合性的（例如，排列、子集）。
- 每个部分解都可以被评估或界定。
- 我们在某个成本函数下寻求最优解。

例子：
- 背包问题：在容量限制下最大化价值。
- 旅行商问题（TSP）：找到最短的环游路线。
- 作业调度：最小化总完成时间。

暴力搜索是指数级的。
分支限界法剪枝那些无法改进已知最佳答案的分支。

#### 工作原理（通俗解释）

想象探索一棵树：
1. 分支：扩展可能的选择。
2. 限界：计算从该分支可达到的值的界限。
3. 如果界限 ≤ 当前找到的最佳值，则剪枝（停止探索）。
4. 否则，继续深入探索。

我们使用：
- 上界：从该路径可能获得的最佳值。
- 下界：目前找到的最佳值。

当上界 ≤ 下界时进行剪枝。

#### 示例：0/1背包问题

给定物品的重量和价值，选择总价值最大且总重量不超过容量的子集。

我们递归地包含/排除每个物品，
但剪枝那些无法超越当前最佳值的分支（例如，超重或潜在价值太低）。

#### 微型代码（Python）

```python
def knapsack_branch_bound(items, capacity):
    best_value = 0

    def bound(i, curr_w, curr_v):
        # 简单界限：贪心地添加剩余物品
        if i >= len(items):
            return curr_v
        w, v = curr_w, curr_v
        for j in range(i, len(items)):
            if w + items[j][0] <= capacity:
                w += items[j][0]
                v += items[j][1]
        return v

    def dfs(i, curr_w, curr_v):
        nonlocal best_value
        if curr_w > capacity:
            return
        if curr_v > best_value:
            best_value = curr_v
        if i == len(items):
            return
        if bound(i, curr_w, curr_v) <= best_value:
            return
        # 包含物品
        dfs(i+1, curr_w + items[i][0], curr_v + items[i][1])
        # 排除物品
        dfs(i+1, curr_w, curr_v)

    dfs(0, 0, 0)
    return best_value
```

#### 为何重要

分支限界法：
- 通过数学剪枝推广了回溯法。
- 将指数级搜索转变为实用的算法。
- 在启发式方法可能失败时提供精确解。

应用于：
- 整数规划
- 路径优化
- 调度和分配问题

#### 一个温和的证明（为何有效）

令 $U(n)$ 为一个子树的上界。
如果 $U(n) \le V^*$（已知最佳值），那么其下的任何解都不可能超过 $V^*$。

通过单调界限，剪枝保持了正确性——
最优解永远不会被丢弃。

该算法是完备的（探索所有有希望的分支）
且是最优的（找到全局最优解）。

#### 动手尝试

1. 使用分支限界法解决 0/1 背包问题。
2. 实现带有成本矩阵的 TSP 问题，并通过下界进行剪枝。
3. 比较探索的节点数与暴力枚举法。

#### 测试用例

| 物品 (重量,价值)         | 容量 | 最佳价值 | 探索的分支数 |
| ----------------------- | ---- | -------- | ------------ |
| [(2,3),(3,4),(4,5)]     | 5    | 7        | 减少         |
| [(1,1),(2,2),(3,5),(4,6)] | 6  | 8        | 减少         |

#### 复杂度

| 问题     | 时间（最坏） | 时间（典型）       | 空间   |
| -------- | ------------ | ------------------ | ------ |
| 背包问题 | $O(2^n)$     | 少得多（剪枝）     | $O(n)$ |
| TSP      | $O(n!)$      | 显著减少（剪枝）   | $O(n)$ |

分支限界法是带有洞察力的搜索，它修剪掉不可能的部分，只专注于最优解可能隐藏的地方。
### 97 随机化模式

随机化模式将偶然性引入算法设计。算法不再遵循固定路径，而是做出随机选择，这些选择在平均情况下能带来高效的性能或简洁性。

随机化有助于打破对称性、避免最坏情况的陷阱，并简化复杂逻辑。

#### 我们要解决什么问题？

我们希望算法能够：
- 避免病态的最坏情况输入。
- 简化那些难以确定性做出的决策。
- 获得良好的期望性能。

常见例子：
- 随机化快速排序：随机选择枢轴。
- 随机化搜索/采样：通过随机试验估计数量。
- 蒙特卡洛和拉斯维加斯算法：用准确性换取速度，反之亦然。

#### 工作原理（通俗解释）

随机化可以表现为两种形式：

1. 拉斯维加斯算法
   * 总是产生正确结果。
   * 运行时间是随机的（例如，随机化快速排序）。

2. 蒙特卡洛算法
   * 在固定时间内运行。
   * 可能有一个小的错误概率（例如，素数性测试）。

通过选择随机路径或样本，我们可以平滑掉坏的情况，并常常简化逻辑。

#### 示例：随机化快速排序

随机选择枢轴以避免最坏情况的分割。

每一步：
1. 从数组中随机选择一个枢轴 $p$。
2. 将数组分割成小于 $p$ 和大于 $p$ 的部分。
3. 递归地对两部分进行排序。

即使输入是恶意的，期望运行时间也是 $O(n \log n)$。

#### 微型代码（Python）

```python
import random

def randomized_quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = random.choice(arr)
    left = [x for x in arr if x < pivot]
    mid = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return randomized_quicksort(left) + mid + randomized_quicksort(right)
```

#### 为什么重要

随机化算法具有以下特点：
- 简单：随机化取代了复杂逻辑。
- 高效：通常在期望上更快。
- 鲁棒：能抵抗恶意输入。

它们出现在：
- 排序、搜索和哈希。
- 近似算法。
- 密码学和采样。
- 机器学习（例如，SGD、装袋法）。

#### 一个温和的证明（为什么有效）

令 $T(n)$ 为随机化快速排序的期望时间：
$$T(n) = n - 1 + \frac{2}{n} \sum_{k=0}^{n-1} T(k)$$

求解可得 $T(n) = O(n \log n)$。
随机枢轴确保每个元素有相等的概率分割数组，
使得平均情况下很可能得到平衡的分割。

期望成本避免了固定枢轴快速排序的 $O(n^2)$ 最坏情况。

#### 自己动手试试

1. 实现随机化快速排序，在已排序输入上运行。
2. 与标准快速排序的平均时间进行比较。
3. 尝试一个随机素数性测试（例如，米勒-拉宾测试）。
4. 使用随机采样通过蒙特卡洛方法近似计算 $\pi$。

#### 测试用例

| 输入          | 预期结果        | 备注                               |
| ------------- | --------------- | ---------------------------------- |
| [1,2,3,4,5]   | [1,2,3,4,5]     | 随机枢轴避免了最坏情况             |
| [5,4,3,2,1]   | [1,2,3,4,5]     | 由于随机分割，仍然很快             |

#### 复杂度

| 算法                 | 期望时间        | 最坏时间          | 空间复杂度   |
| -------------------- | --------------- | ----------------- | ------------ |
| 随机化快速排序       | $O(n \log n)$   | $O(n^2)$（罕见）  | $O(\log n)$  |
| 随机化搜索           | $O(1)$ 期望     | $O(n)$ 最坏       | $O(1)$       |

随机化将僵化的逻辑转变为灵活的、平均情况下的卓越性能，是在不确定或敌对世界中的一个实用盟友。
### 98 近似模式

当寻找精确解过于昂贵或不可能时，会使用近似模式。与其追求完美，我们设计出能产生*足够接近*最优解的算法，这些算法快速、可预测，并且通常保证在一个因子范围内。

这种模式在 NP 难问题中表现出色，因为精确方法在这些问题上扩展性很差。

#### 我们要解决什么问题？

一些问题，如旅行商问题、顶点覆盖问题或背包问题，没有已知的多项式时间精确解。
我们需要能快速给出足够好答案的算法，特别是对于大规模输入。

近似算法确保：

- 可预测的性能。
- 可衡量的精度。
- 多项式运行时间。

#### 工作原理（通俗解释）

近似算法输出的解在一个已知的比率范围内接近最优值：

如果最优成本是 $\text{OPT}$，
而我们的算法返回 $\text{ALG}$，
那么对于一个最小化问题：

$$\frac{\text{ALG}}{\text{OPT}} \le \alpha$$

其中 $\alpha$ 是近似因子（例如，2、1.5 或 $(1 + \epsilon)$）。

#### 示例：顶点覆盖（2-近似）

问题：找到覆盖所有边的最小顶点集合。

算法：

1. 从一个空集合 $C$ 开始。
2. 当还有边剩余时：

   * 选取任意一条未被覆盖的边 $(u, v)$。
   * 将 $u$ 和 $v$ 都加入 $C$。
   * 移除所有与 $u$ 或 $v$ 关联的边。
3. 返回 $C$。

这保证了 $|C| \le 2 \cdot |C^*|$，
其中 $C^*$ 是最优顶点覆盖。

#### 微型代码（Python）

```python
def vertex_cover(edges):
    cover = set()
    while edges:
        (u, v) = edges.pop()
        cover.add(u)
        cover.add(v)
        edges = [(x, y) for (x, y) in edges if x not in (u, v) and y not in (u, v)]
    return cover
```

#### 为什么它很重要

近似算法：

- 提供可证明的保证。
- 可扩展到大规模问题。
- 在时间和精度之间提供可预测的权衡。

广泛应用于：

- 组合优化。
- 调度、路由、资源分配。
- AI 规划、聚类和压缩。

#### 一个温和的证明（为什么它有效）

设 $C^*$ 为最优覆盖。
每条边都必须被 $C^*$ 覆盖。
我们为每条边选择 2 个顶点，所以：

$$|C| = 2 \cdot \text{(选择的边数)} \le 2 \cdot |C^*|$$

因此，近似因子为 2。

#### 亲自尝试

1. 实现 2-近似的顶点覆盖算法。
2. 对于小图，将结果大小与暴力解法进行比较。
3. 使用贪心选择探索 $(1 + \epsilon)$-近似。
4. 将相同思想应用于集合覆盖或背包问题。

#### 测试用例

| 图       | 最优解 | 算法结果 | 比率 |
| -------- | ------ | -------- | ---- |
| 三角形   | 2      | 2        | 1.0  |
| 正方形   | 2      | 4        | 2.0  |

#### 复杂度

| 算法                 | 时间                | 空间     | 保证           |
| -------------------- | ------------------- | -------- | -------------- |
| 顶点覆盖（贪心）     | $O(E)$              | $O(V)$   | 2-近似         |
| 背包问题（FPTAS）    | $O(n^3 / \epsilon)$ | $O(n^2)$ | $(1+\epsilon)$ |

近似是*快速接近完美*的艺术，是理论与实践之间一座务实的桥梁。
### 99 在线算法模式

在线算法模式用于处理按顺序到达的输入，并且必须在不知道未来数据的情况下立即做出决策。没有回头路，也不能在以后重新优化，你必须边走边承诺。

这种模式模拟了实时决策，从缓存到任务调度和资源分配。

#### 我们要解决什么问题？

在许多系统中，数据并非一次性全部到达。你必须现在做决定，而不是在看清全局之后。

典型场景：
- 缓存替换（决定接下来驱逐哪个页面）。
- 任务分配（作业实时到达）。
- 动态路由（数据包持续到达）。

离线算法预先知道一切；在线算法则不然，但必须保持有竞争力的性能。

#### 工作原理（通俗解释）

在线算法逐个处理输入。
每一步：
1.  在时间 $t$ 接收输入项 $x_t$。
2.  仅使用当前状态做出决策 $d_t$。
3.  之后不能更改 $d_t$。

性能通过竞争比来衡量：

$$
\text{竞争比} = \max_{\text{输入序列}} \frac{\text{算法成本}}{\text{最优离线算法成本}}
$$

如果算法 $\text{ALG}$ 的成本最多是最优成本的 $k$ 倍，则该算法是 $k$-竞争的。

#### 示例：分页 / 缓存替换

你有一个大小为 $k$ 的缓存。
页面请求序列到达。
如果请求的页面不在缓存中 → 缺页 → 加载它（如果缓存已满则驱逐一个页面）。

算法：
- FIFO（先进先出）：驱逐最旧的。
- LRU（最近最少使用）：驱逐最近最少访问的。
- Random（随机）：随机驱逐。

LRU 是 $k$-竞争的，意味着其性能在最优算法的 $k$ 倍之内。

#### 微型代码（Python）

```python
def lru_cache(pages, capacity):
    cache = []
    faults = 0
    for p in pages:
        if p not in cache:
            faults += 1
            if len(cache) == capacity:
                cache.pop(0)
            cache.append(p)
        else:
            cache.remove(p)
            cache.append(p)
    return faults
```

#### 为什么它很重要

在线算法：
- 反映了现实世界的约束（没有预见性）。
- 在流处理、缓存和调度中实现自适应系统。
- 即使在最坏情况的输入下也能提供有竞争力的性能保证。

应用于：
- 操作系统（页面替换）。
- 网络（数据包路由）。
- 金融（在线定价、竞价）。
- 机器学习（在线梯度下降）。

#### 一个温和的证明（为什么它有效）

对于 LRU 缓存：
每次缓存未命中都意味着一个在过去 $k$ 次请求中未出现的唯一页面。
最优离线算法（OPT）可以避免一些缺页，但最多只能减少 $k$ 倍。
因此：

$$
\text{LRU 缺页数} \le k \cdot \text{OPT 缺页数}
$$

所以 LRU 是 $k$-竞争的。

#### 自己动手试试

1.  在同一请求序列上模拟 LRU、FIFO、Random 缓存。
2.  统计缺页次数。
3.  与离线 OPT（Belady 算法）进行比较。
4.  用 $k=2,3,4$ 进行实验。

#### 测试用例

| 页面请求序列      | 缓存大小 | 算法    | 缺页数 | 比率（对比 OPT） |
| ----------------- | -------- | ------- | ------ | ---------------- |
| [1,2,3,1,2,3]     | 2        | LRU     | 6      | 3.0              |
| [1,2,3,4,1,2,3,4] | 3        | LRU     | 8      | 2.7              |

#### 复杂度

| 算法          | 时间复杂度 | 空间复杂度 | 竞争比 |
| ------------- | ---------- | ---------- | ------ |
| FIFO          | $O(nk)$    | $O(k)$     | $k$    |
| LRU           | $O(nk)$    | $O(k)$     | $k$    |
| OPT（离线）   | $O(nk)$    | $O(k)$     | 1      |

在线算法拥抱不确定性，它们明智地行动于*此刻*，并相信分析会证明它们以后不会后悔。
### 100 混合策略模式

混合策略模式结合了多种算法范式，例如分治法、贪心算法和动态规划，以平衡它们的优势并克服各自的弱点。混合算法不固守单一的设计理念，而是根据问题的结构和输入规模进行调整。

#### 我们要解决什么问题？

没有一种范式能适用于所有问题。
有些输入规模小，适合使用暴力法；有些需要递归结构；还有些则需要启发式方法。

我们需要一种元策略，能够融合多种范式，并根据以下条件在它们之间切换：

- 输入规模（例如，小规模 vs 大规模）
- 结构（例如，已排序 vs 未排序）
- 精度要求（例如，精确解 vs 近似解）

混合策略提供了超越理论渐近复杂度的实际性能。

#### 工作原理（通俗解释）

混合算法使用*决策逻辑*来为每种情况选择最佳方法。

常见模式：

1.  **小规模基础切换**：
    当 $n$ 较小时使用暴力法（例如，在快速排序内部使用插入排序）。
2.  **阶段组合**：
    使用一种算法进行初始化，另一种算法进行优化（例如，使用贪心算法获得初始解，再使用动态规划进行优化）。
3.  **条件策略**：
    根据数据分布选择算法（例如，快速排序 vs 堆排序）。

#### 示例：内省排序

内省排序开始时像快速排序以获得平均速度，但如果递归深度变得过大（枢轴分割不佳），它会切换到堆排序以保证 $O(n \log n)$ 的最坏情况时间复杂度。

步骤：

1.  使用快速排序进行分区。
2.  跟踪递归深度。
3.  如果深度 > 阈值（$2 \log n$），则切换到堆排序。

这确保了兼得两者之长：平均速度 + 最坏情况安全性。

#### 微型代码（Python）

```python
def introsort(arr, depth_limit):
    if len(arr) <= 1:
        return arr
    if depth_limit == 0:
        return heapsort(arr)
    pivot = arr[len(arr)//2]
    left = [x for x in arr if x < pivot]
    mid = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return introsort(left, depth_limit - 1) + mid + introsort(right, depth_limit - 1)
```

*(当达到深度限制时使用堆排序)*

#### 为何重要

混合策略提供了实际效率、可预测的性能和稳健的回退行为。
它们反映了专家开发者构建系统的方式：不是一刀切，而是分层的、有条件的。

常见的混合算法：

- Timsort = 归并排序 + 插入排序
- 内省排序 = 快速排序 + 堆排序
- 分支定界 + 贪心算法 = 带剪枝和启发式的搜索
- 神经网络 + 符号系统 = 学习 + 逻辑推理

#### 一个温和的证明（为何有效）

设 $A_1, A_2, \ldots, A_k$ 为候选算法，其成本函数为 $T_i(n)$。
混合策略 $H$ 在条件 $C_i(n)$ 成立时选择 $A_i$。

如果决策逻辑确保
$$T_H(n) = \min_i { T_i(n) \mid C_i(n) }$$
那么 $H$ 的表现至少与最佳适用算法一样好。

因此 $T_H(n) = O(\min_i T_i(n))$。

#### 动手尝试

1.  实现快速排序 + 插入排序的混合算法。
2.  设置切换阈值 $n_0 = 10$。
3.  与纯快速排序比较性能。
4.  尝试不同的阈值。

#### 测试用例

| 输入规模 | 算法         | 时间     | 备注                 |
| -------- | ------------ | -------- | -------------------- |
| 10       | 插入排序     | 最快     | 简单性胜出           |
| 1000     | 快速排序     | 最优     | 开销低               |
| 1e6      | 内省排序     | 稳定     | 无最坏情况爆炸       |

#### 复杂度

| 组件       | 最佳          | 平均          | 最坏          | 空间       |
| ---------- | ------------- | ------------- | ------------- | ---------- |
| 快速排序   | $O(n \log n)$ | $O(n \log n)$ | $O(n^2)$      | $O(\log n)$ |
| 堆排序     | $O(n \log n)$ | $O(n \log n)$ | $O(n \log n)$ | $O(1)$      |
| 内省排序   | $O(n \log n)$ | $O(n \log n)$ | $O(n \log n)$ | $O(\log n)$ |

混合策略不仅仅是一种算法技巧，更是一种思维方式：
结合精确性、适应性和实用性，构建能在实际环境中表现出色的算法。
